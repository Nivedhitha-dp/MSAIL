<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Translation | MSAIL</title>
    <link>https://MSAIL.github.io/tag/machine-translation/</link>
      <atom:link href="https://MSAIL.github.io/tag/machine-translation/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Translation</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Tue, 15 Mar 2022 18:00:00 -0400</lastBuildDate>
    <image>
      <url>https://MSAIL.github.io/media/logo.png</url>
      <title>Machine Translation</title>
      <link>https://MSAIL.github.io/tag/machine-translation/</link>
    </image>
    
    <item>
      <title>An Overview of Attention and Transformer Mechanisms for NLP</title>
      <link>https://MSAIL.github.io/talk/attention_031522/</link>
      <pubDate>Tue, 15 Mar 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/attention_031522/</guid>
      <description>&lt;p&gt;Nisreen explained the technical aspects of attention and self attention mechanisms, as well as explored how attention is used in the transformer architecture in order to aid in machine translation tasks.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Machine Translation by Jointly Learning to Align and Translate, Bahdanau et al.&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention is all you Need, Vaswani et al.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
