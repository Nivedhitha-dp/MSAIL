<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | MSAIL</title>
    <link>https://MSAIL.github.io/tag/deep-learning/</link>
      <atom:link href="https://MSAIL.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 11 Apr 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://MSAIL.github.io/media/logo.png</url>
      <title>Deep Learning</title>
      <link>https://MSAIL.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Do convolutional neural networks mimic the human visual system?</title>
      <link>https://MSAIL.github.io/post/cnn_human_visual/</link>
      <pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/post/cnn_human_visual/</guid>
      <description>&lt;p&gt;Richard Feynman once said “What I cannot create I do not understand.” Therefore, to truly understand the human visual system, we must learn to create it. One of the most effective forms of such creation is the Convolutional Neural Network (CNN) system to mimic the human visual system. Computer Vision models that use the CNN system have achieved near human-level performances on tasks such as image classification and object detection. There is no question the CNNs have shown us mind-blowing performance, but the question is: do they actually resemble the biological visual system? Are we really creating it? As a quest to answer this question, in this article we will explore the similarities and differences between the CNNs and the biological visual system.&lt;/p&gt;
&lt;p&gt;To start off, let’s explore the roots of the CNN - how does a CNN function and what are its capabilities? In simple terms, the CNN is able to learn features from images, for example - it is able to deduce if an image has a dog; it is able to deduce if the image is a desert, it can tell if an image is a painting. Let’s take a look at some example features that CNNs can learn to extract from an image. In the image underneath, we can see that for a baseball, the CNN meshes the unique features of a baseball such as rounded shape, stripes, into a filter image. This filter image is then used to cross check against input images to determine whether each input image contains a baseball. The same is done for dogs, clouds and buildings, etc, with their own respective filter images. This way, we can use CNNs for tasks such as classification (labeling an image to a group/class), object detection (detecting the presence of certain objects in an image), and image generation (imitating certain styles and patterns to generate unique images), among many other tasks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig1.png&#34; alt=&#34;Filters for detecting image features&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Use of filters for detecting image features [1]
&lt;/p&gt;
&lt;p&gt;So now that we have an idea of how CNNs function, we can move on to discussing how its design compares to the biological visual system. The CNN is composed of image processing layers that deduce and pass down information from one layer to the next. At each layer, information of different abstractions is deduced. Generally, in the earlier layers, simpler and more basic ideas will be deduced while later layers will use the gathered information from the previous layers to deduce more complex ideas. The following figure lays down an example CNN - in the figure, a boat image is passed from the left end layer to the next until it reaches the right-most layer, where it classifies the class of the image; in this case, it should be “Boat”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig2.png&#34; alt=&#34;Layer structure&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Layer Structure of a CNN [6]
&lt;/p&gt;
&lt;p&gt;Due to the way the layers learn more complex features the deeper into the network, we can call this a hierarchical learning structure. To see clearly what we mean by the complexity of features, we can observe the following figure. The first group shows the detection of edges - which are simpler features compared to the textures in the second group. The CNN layers detect such “textures” through the combination of “edges” detected in the previous layer. Likewise, the following layers learn patterns through combining textures, and so on and so forth, until the last layers learn the presence of certain objects. In this way, the complexity of features increases as we go deeper down the network, demonstrating a hierarchical learning mechanism.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig3.png&#34; alt=&#34;Complexity of learned features&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Different complexity of learned features [1]
&lt;/p&gt;
&lt;p&gt;This hierarchical layer structure is actually also utilized in the visual cortex ventral pathway, which is a layerlike pathway consisting of the sequence LGN-V1-V2-V4-IT where each of them represents a certain information processing layer (Figure 4). As we proceed through the visual pathway,  the features learned become more complex, just as in the CNN. The receptive visual field size increases as well, as larger receptive field suggests a more holistic and general feature in the image. In a way, this makes sense - to recognize a baseball, the network has to learn stripes and some circular shapes. Likewise, for a dog, the features might be the dog snouts, black and white eyes, furry texture, etc. Such features cannot be instantly detected in just a single step, but rather gathered throughout different layers’ learning, which forms the basis of hierarchical learning.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig4.png&#34; alt=&#34;Visual Cortex Ventral Pathway&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Human Visual Cortex Ventral Pathway [3]
&lt;/p&gt;
&lt;p&gt;In addition to the concept of hierarchical information processing in CNNs, another fundamental concept called pooling is utilized. Pooling is basically the idea of generalizing or approximating a set of values in an area into a smaller set of values. This concept is explained in the following figure. The input image is a grid of 16 values and pooling is applied on the image to result in a grid of 4 values; each of the 4 values are the maximum values taken from their respective areas represented by the color. By taking the maximum, we reduce the size of the information we are looking at and select the most important values that need to be paid attention to.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig5.png&#34; alt=&#34;Pooling Mechanism&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Pooling Mechanism [8]
&lt;/p&gt;
&lt;p&gt;To understand at the higher level, the pooling is used to aggregate information gathered into summarized information. This idea of aggregation allows for the hierarchical information processing - the basic features learned are aggregated and then the details are gotten rid of to learn high level features. Pooling reduces the dimension of the representation and “creates an invariance to small shifts and distortions”. Basically this means that switching an image around by slight pixel changes will not affect the information being deduced from the image. Through pooling, we eliminate repeated learning of similar features that are right next to each other in the image feature representation. Interestingly, this idea behind the pooling layer is found in the relation between simple and complex cells in the biological neural system, where simple cells simply evoke a response on each of their particular spatial locations, while complex cells seem to be pooling over responses from the simple cells and thus showing more spatial invariance in their responses.&lt;/p&gt;
&lt;p&gt;No matter how neat CNN is in capturing visual information like the biological system, there are some outright flaws in it. One is the possibility of adversarial attacks, which involve hacking the CNN by slightly changing the pixel values of images in a way that is undetectable to a human eye but enough to fool the CNN to make faulty conclusions. An example is shown below, where the panda image is altered to be recognized as a gibbon by a CNN although there seems to be no difference to the human eye (Figure 6). This example shows how the CNN is perceiving ideas through meticulous attention to every single pixel in an image, which might not be the case with human visuals; for humans, perception likely happens through directly seeing the patterns and lines rather than individual pixels.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig6.png&#34; alt=&#34;Adversarial Attack on CNN&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Adversarial Attack on CNN (&lt;a href=&#34;https://openai.com/blog/adversarial-example-research/&#34;&gt;OpenAI&lt;/a&gt;)
&lt;/p&gt;
&lt;p&gt;On the other hand, this makes us wonder, “Can the human visual system be hacked as well? Are there ways to fool our eyes although maybe to another species there isn’t noticeable change?” It turns out that there are ways to fool our visual perception as well through small image change. Researchers have found a way to generate images that are designed to tip the perception towards a different idea although there isn’t much change in the image composition. Look at the example below - the left image looks like a cat, but when altered slightly to form the right image, it starts to look more like a dog. Such a hack is akin to the idea of subliminal stimuli - visual or auditory stimuli that the conscious mind cannot detect but that the brain subconsciously processes - maybe adversarial attacks are subliminal messages for the CNN.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig7.png&#34; alt=&#34;Adversarial Attack on Human Visual System&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Adversarial Attack on Human Visual System [9]
&lt;/p&gt;
&lt;p&gt;While it’s quite interesting to ponder such ideas and even question our sensual perception, the conclusion is that there are evidently parallels between the way CNN works and the way the human visual system works. However, there are also some fundamental differences between them - although these differences could possibly be reduced through more complex layers and architectural changes in the CNN design.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] 
&lt;a href=&#34;https://distill.pub/2017/feature-visualization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Feature Visualization - What are CNNs learning?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] 
&lt;a href=&#34;https://distill.pub/2018/building-blocks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interpretation with building blocks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] 
&lt;a href=&#34;https://neurdiness.wordpress.com/2018/05/17/deep-convolutional-neural-networks-as-models-of-the-visual-system-qa/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural networks as models of the visual system&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] 
&lt;a href=&#34;https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LeCun - Nature Deep Learning Review&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] 
&lt;a href=&#34;https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How Conv layers work&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] 
&lt;a href=&#34;https://www.vision-systems.com/boards-software/article/14037858/fundamentals-of-deep-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fundamentals of Deep Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[7] 
&lt;a href=&#34;http://fourier.eng.hmc.edu/e180/lectures/v1/node7.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simple and Complex cells in the Human visual system&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8] 
&lt;a href=&#34;https://towardsdatascience.com/understanding-convolutions-and-pooling-in-neural-networks-a-simple-explanation-885a2d78f211&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Convolutions and Pooling in Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9] 
&lt;a href=&#34;https://spectrum.ieee.org/the-human-os/artificial-intelligence/machine-learning/hacking-the-brain-with-adversarial-images&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hacking the Brain with Adversarial Images&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Transformers for Vision</title>
      <link>https://MSAIL.github.io/talk/image-worth-16x16-words/</link>
      <pubDate>Tue, 09 Mar 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/image-worth-16x16-words/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Andrew Awad and Drake Svoboda&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Using Transformers for Computer Vision&lt;/p&gt;
&lt;p&gt;In recent years we&amp;rsquo;ve seen the rise of transformers in natural language processing research, burgeoning the field to incredible heights. However, these very same transformers were seldom applied to computer vision tasks until recently. Andrew and Drake discussed how transformers have been used in vision tasks in recent years in a presentation covering two papers. The first, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (via Google Brain), is the &amp;ldquo;Attention is All You Need&amp;rdquo; of vision. Namely, this paper covers how one can construct a vision architecture devoid of the commonly applied CNN and still achieve comparable or better performance results while possibly cutting down computing resources. The second paper, End-to-End Object Detection with Transformers (via FAIR), formalizes the object detection task in a unique way that affords the usage of transformers.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2010.11929.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2005.12872.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;End-to-End Object Detection with Transformers&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Proving Theorems with Generative Language Models</title>
      <link>https://MSAIL.github.io/talk/generative_language_modeling/</link>
      <pubDate>Mon, 01 Mar 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/generative_language_modeling/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Ashwin Sreevatsa&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Generative Language Modeling for Automated Theorem Proving Presentation&lt;/p&gt;
&lt;p&gt;In the past decade, deep learning and artificial neural networks have been incredibly successful at a variety of tasks such as computer vision, translation, game playing, and robotics among others. However, there have been less examples of deep learning making progress with reasoning related tasks- such as automated theorem proving, the task of proving mathematical theorems using computer programs. This paper explores the use of transformer-based models to automated theorem proving and presents GPT-f, a deep learning-based automated prover and proof assistant.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2009.03393.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generative Language Modeling for Automated Theorem Proving Presentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lightning Round -- Assorted AI Topics</title>
      <link>https://MSAIL.github.io/previous_material/lightning/</link>
      <pubDate>Sat, 27 Feb 2021 15:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/lightning/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Lightning Round &amp;ndash; Assorted AI Topics&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;Here, we talk about a wide range of topics in AI that haven&amp;rsquo;t received their own slide decks &amp;ndash; the list includes reinforcement learning, optimization, adversarial machine learning, meta learning, active learning, multi-agent systems, and more. We hope that showcasing the breadth of AI research inspires you to dig deeper on your own and find what interests you!&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/169IpCxkST0Fjp7LjQgpdfiDz-Ccs1K-v/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1uQzkFpr4LyslagkloUHs5lnCQ3wNmVLfuaa7UsbTaGA/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ethics</title>
      <link>https://MSAIL.github.io/previous_material/ethics/</link>
      <pubDate>Fri, 26 Feb 2021 17:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/ethics/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Ethics in AI Research&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;We discuss the various ethical problems AI research presents, including well-known problems like bias and weaponized AI and less publicized problems like interpretability and environmental impact of large machine learning models. We also talk about some of the solutions that researchers are attempting to implement and what we can do to contribute.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1C-bWWrhh_hK6ZwNmLEYK95uLJ6eiCbi1/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1KUUqzdz-Te1oNS4AMnxxPqO_mFUpmkDNokr0As9rCHQ/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised Learning</title>
      <link>https://MSAIL.github.io/previous_material/unsupervised/</link>
      <pubDate>Sat, 20 Feb 2021 15:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/unsupervised/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Unsupervised Learning&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;This lesson went over the unsupervised side of AI, where labels don&amp;rsquo;t exist and models are left on their own to learn useful information. We presented machine learning approaches with and without deep learning that tackle unsupervised problems.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1WXBrrbNDryufUYkzQS1aLwrEcJRbo-xS/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1H77BDYebNusyelevFe5-AHZzYCaOB1tid-Vqtmm13oI/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Natural Language Processing</title>
      <link>https://MSAIL.github.io/previous_material/nlp/</link>
      <pubDate>Fri, 19 Feb 2021 17:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/nlp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Natural Language Processing&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;This lesson gave a high level overview of NLP (natural language processing) and how AI can be used to work with text and speech data. Points of discussion included recurrent neural networks, LSTMs/GRUs, and GPT-3 and other transformer models.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1DjwaY3p7vb4N4V7DwvZEBQB2qxjs5okS/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/178FNnk3x8euXO3NHBqQT9VT6-d9FigUVOt56Ru-Lvpo/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep RL for Robotics: A Short Overview</title>
      <link>https://MSAIL.github.io/talk/deeprlrobotics_021621/</link>
      <pubDate>Tue, 16 Feb 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/deeprlrobotics_021621/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Nikhil Devraj&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: A Brief Overview of Deep RL in Robotics&lt;/p&gt;
&lt;p&gt;Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world.&lt;br&gt;
This discussion focused predominantly on the following questions: &lt;br&gt;
(1) What is deep RL and how does it relate to robotics?&lt;br&gt;
(2) What are some examples of studies done with Deep RL in robotics?&lt;br&gt;
(3) What are major challenges faced by researchers who apply deep RL to robotics?&lt;/p&gt;
&lt;p&gt;This discussion is heavily inspired by 
&lt;a href=&#34;https://arxiv.org/pdf/2102.02915.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ibarz et al.&lt;/a&gt;, although it does not dive into that level of detail.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1gshp58d3yce4LhcLpTymch8IA2cbm4ZO/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find the recording of this talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2102.02915.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to Train Your Robot with Deep Reinforcement Learning - Lessons We&amp;rsquo;ve Learned&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1610.00633&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/0278364913495721&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reinforcement learning in robotics: a survey&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Articles:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://medium.com/@vmayoral/reinforcement-learning-in-robotics-d2609702f71b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium: Reinforcement learning in robotics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://ieor8100.github.io/rl/docs/RL%20in%20Robotics.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Better slides (in our presenter&amp;rsquo;s opinion)&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=GX_RonOFe1U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep RL Towards Robotics by Shane Gu (Google Brain)&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=luzOblzznIc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep RL in Robotics with NVIDIA Jetson&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer Vision</title>
      <link>https://MSAIL.github.io/previous_material/computer_vision/</link>
      <pubDate>Sat, 13 Feb 2021 15:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/computer_vision/</guid>
      <description>&lt;p&gt;​
&lt;strong&gt;Topic&lt;/strong&gt;: An Overview of Computer Vision&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang​&lt;/p&gt;
&lt;p&gt;This lesson gave a basic overview of the computer vision problem space. We discussed historically significant developments including convolutional neural networks, AlexNet, ResNet, and more, and we gave a glimpse at ongoing research.​&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/15WxV2hC40Bz4YhcyPVeYqb1gvIViq2Ka/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1MaC9d25kJybNv_pOYQHFv9oNOM1J-65zMkQX2PMlCqg/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction and Basics of Deep Learning</title>
      <link>https://MSAIL.github.io/previous_material/intro_dl_basics/</link>
      <pubDate>Fri, 12 Feb 2021 17:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/intro_dl_basics/</guid>
      <description>&lt;p&gt;​
&lt;strong&gt;Topic&lt;/strong&gt;: Introduction to AI Research and Basics of Deep Learning&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang  ​&lt;/p&gt;
&lt;p&gt;This lesson introduced the format of lessons for the winter 2021 semester, briefly introducing the topics to be presented in the coming weeks. We then gave a high-level overview of neural networks, which form the basis of deep learning and drive much of AI research today. ​&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1lNhpuuxNhW5nHDLavDLqlLOKv-DYfMhO/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;​&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1SkI0i1Y_Dp1lZTCJjJD91f0DVf_CXfB4FMBy8jLweeg/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks</title>
      <link>https://MSAIL.github.io/previous_material/cnn/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/cnn/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Convolutional Neural Networks&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;This lesson covered convolutional neural networks, which serve as the backbone for many modern-day deep learning applications. Most commonly, convolutional neural networks are used for vision tasks (although not exclusively).&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1522OsXalZScvuUxXrOTbUZuISZUY-HqO&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides on CNNs&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://docs.google.com/presentation/d/16TMR2sM9T75qALw3CCigUF_JxMQ5gceM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides on Neural Networks&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
