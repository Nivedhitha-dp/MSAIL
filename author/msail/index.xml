<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MSAIL</title>
    <link>https://MSAIL.github.io/author/msail/</link>
      <atom:link href="https://MSAIL.github.io/author/msail/index.xml" rel="self" type="application/rss+xml" />
    <description>MSAIL</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 28 Mar 2024 18:00:00 -0400</lastBuildDate>
    <image>
      <url>https://MSAIL.github.io/media/logo.png</url>
      <title>MSAIL</title>
      <link>https://MSAIL.github.io/author/msail/</link>
    </image>
    
    <item>
      <title>MSAIL TECH TALK w/ Wesley Tian</title>
      <link>https://MSAIL.github.io/talk/wesleytian_240324/</link>
      <pubDate>Thu, 28 Mar 2024 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/wesleytian_240324/</guid>
      <description>&lt;p&gt;University of Michigan alumni and former MSAIL member Wesley Tian, the Co-founder and CEO of Aragon.ai, will talk about his journey, share what he learned from starting his AI company, and offer career advice.&lt;/p&gt;
&lt;p&gt;RSVP for this event 
&lt;a href=&#34;hhttps://forms.gle/724P8z5wdHryhCf5A&#34;&gt;here&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MSAIL TECH TALK w/ Kiran Prasad</title>
      <link>https://MSAIL.github.io/talk/prasad_210324/</link>
      <pubDate>Thu, 21 Mar 2024 19:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/prasad_210324/</guid>
      <description>&lt;p&gt;University of Michigan and Carnegie Mellon alumni Kiran Prasad will share his professional journey, thoughts about Grad School, and give a talk about machine learning concepts that he uses in industry. Kiran is a Senior ML Engineer at Gather, where he works on end-to-end AI system design. He previously was an Applied Scientist on the Microsoft Turing team (Microsoft&amp;rsquo;s NLP v-team that created CoPilot and spearheaded collaboration with OpenAI).&lt;/p&gt;
&lt;p&gt;RSVP for this event 
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdXVZybnuUJ2zZRsa2jNBnoK4MXrEcgPn41f2HTH5G8vI5Kuw/viewform&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Overview of Binarized Neural Networks</title>
      <link>https://MSAIL.github.io/talk/bnn_041222/</link>
      <pubDate>Tue, 12 Apr 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/bnn_041222/</guid>
      <description>&lt;p&gt;Binarized neural networks (BNNs) are an extreme version of quantized neural networks where all weights and activations are quantized to +/- 1. A key motivation for such a network is to enable one to run powerful neural networks on small battery-powered devices. This talk introduced BNNs, explained how one can train such a network and reviewed some recent work in the area.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1602.02830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, by Courbariaux et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.mdpi.com/2079-9292/8/6/661&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A review of Binarized Neural Networks, by Simons et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=rJfUCoR5KX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Empirical Study of Binarized Neural Networks&amp;rsquo; Optimization, by Alizadeh et al.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning for Intraoperative Diagnosis of Brain Tumors Imaged using Stimulated Raman Histology</title>
      <link>https://MSAIL.github.io/talk/brain_tumor_040522/</link>
      <pubDate>Tue, 05 Apr 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/brain_tumor_040522/</guid>
      <description>&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nature.com/articles/s41591-019-0715-9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper 1: Near real-time intraoperative brain tumor diagnosis using stimulated Raman histology and deep neural networks&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://journals.lww.com/neurosurgery/Abstract/9900/Rapid_Automated_Analysis_of_Skull_Base_Tumor.184.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper 2: Rapid Automated Analysis of Skull Base Tumor Specimens Using Intraoperative Optical Imaging and Artificial Intelligence&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Devil is in the Details: Spatial and Temporal Super-Resolution of Global Climate Models using Adversarial Deep Learning</title>
      <link>https://MSAIL.github.io/talk/climate_adversarial_032922/</link>
      <pubDate>Tue, 29 Mar 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/climate_adversarial_032922/</guid>
      <description>&lt;p&gt;Physics-based global climate simulations are computationally expensive and limited to low spatial and temporal resolutions, making it difficult to predict and track highly localized extreme weather phenomena. To overcome these limitations, we present a novel application of super-resolution using deep convolutional generative adversarial networks (GANs) to increase the resolution of global climate models in both space and time. In this project, we demonstrate the potential to reduce climate simulation computation and storage requirements by two orders of magnitude, as well as democratize relevant and actionable climate information for disaster responses. This work won the Best Paper Award in the 2020 ProjectX international ML research competition hosted by the University of Toronto.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1cbwTb7DNe0vRZiN9hg53W5MZdRbXJqsg/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper, by Chen et al.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Fairness in Machine Learning</title>
      <link>https://MSAIL.github.io/talk/fairness_032222/</link>
      <pubDate>Tue, 22 Mar 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/fairness_032222/</guid>
      <description>&lt;p&gt;The purpose of this presentation was to introduce everyone to fairness aspects of machine learning and discuss Serafina&amp;rsquo;s (
&lt;a href=&#34;https://cse.engin.umich.edu/stories/undergraduate-researcher-takes-first-place-at-acm-competition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;award-winning!&lt;/a&gt;) research in the area.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-93736-2_43&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robustness of Fairness: An Experimental Analysis, by Kamp et al.&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://docs.google.com/presentation/d/1fVakENisRjTh55vB6K25I0JqMMo3Dh77GLBbFJSc-CA/edit#slide=id.g11d502cf596_0_17&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides with additional links and resources&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Overview of Attention and Transformer Mechanisms for NLP</title>
      <link>https://MSAIL.github.io/talk/attention_031522/</link>
      <pubDate>Tue, 15 Mar 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/attention_031522/</guid>
      <description>&lt;p&gt;Nisreen explained the technical aspects of attention and self attention mechanisms, as well as explored how attention is used in the transformer architecture in order to aid in machine translation tasks.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Machine Translation by Jointly Learning to Align and Translate, Bahdanau et al.&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention is all you Need, Vaswani et al.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Problems in Cooperative AI</title>
      <link>https://MSAIL.github.io/talk/cooperative_022222/</link>
      <pubDate>Tue, 22 Feb 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/cooperative_022222/</guid>
      <description>&lt;p&gt;A recent area of study in AI has been focused on the problem of cooperation amongst machine learning agents. These cooperation problems are widespread, from routine challenges such as driving on highways and working collaboratively, all the way up to global challenges like commerce, peace, and pandemic preparedness. If AI is to play a larger role in society, it is important that AI agents will be able to cooperate effectively with other agents (other AI, humans, etc).&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2012.08630.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open Problems in Cooperative AI, Dafoe et al.&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.nature.com/articles/d41586-021-01170-0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cooperative AI: machines must learn to find common ground&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scaling Neural Tangent Kernels via Sketching and Random Features</title>
      <link>https://MSAIL.github.io/talk/ntk_020122/</link>
      <pubDate>Tue, 01 Feb 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/ntk_020122/</guid>
      <description>&lt;p&gt;Kevin presented 
&lt;a href=&#34;https://arxiv.org/pdf/2106.07880.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scaling Neural Tangent Kernels via Sketching and Random Features&lt;/a&gt;, which uses sketching and random feature generation to speed up neural tangent kernels (NTKs). This presentation was really all about introducing the NTK, a mechanism for analyzing the behavior of very wide / infinitely wide neural networks. NTKs made a huge splash in machine learning theory in 2018 for offering a novel approach to analyzing the behavior of neural networks, and there&amp;rsquo;s plenty of ground left to cover with them in research.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kernel trick&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=DObobAnELkU&amp;amp;feature=youtu.be&amp;amp;ab_channel=SoheilFeizi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Professor Feizi&amp;rsquo;s lecture&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://rajatvd.github.io/NTK/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rajat&amp;rsquo;s blog post&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1806.07572&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper #1, introduces NTKs&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1904.11955&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper #2, polynomial bounds NTK complexity and introduces CNTK&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1911.00809&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper #3, enhanced CNTK&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Concrete Problems in AI Safety</title>
      <link>https://MSAIL.github.io/talk/aisafety_012522/</link>
      <pubDate>Tue, 25 Jan 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/aisafety_012522/</guid>
      <description>&lt;p&gt;Ashwin gave an introduction to the field of AI safety, which studies how to ensure that AI, especially artificial general intelligence and super intelligence, will be safe and trustworthy.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1606.06565.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Concrete Problems in AI Safety, by Amodei et al.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Human-in-the-Loop Natural Language Processing</title>
      <link>https://MSAIL.github.io/talk/jkk_120621/</link>
      <pubDate>Mon, 06 Dec 2021 20:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/jkk_120621/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Dr. Jonathan Kummerfeld&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Human-in-the-Loop Natural Language Processing&lt;/p&gt;
&lt;p&gt;Dr. Kummerfeld works on NLP, with many projects crossing into HCI, either in the process of creating datasets or developing systems. In this talk, he gave a brief introduction to NLP and Crowdsourcing + Human Computation, and then dove into two research projects. First, he discussed work on task-oriented dialogue (e.g. Siri), where his team developed new ways to collect more diverse data, which in turn leads to more robust models. Second, he discussed work on understanding a set of conversations occurring in a shared channel (e.g. in Slack).&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://jkk.name/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr. Kummerfeld&amp;rsquo;s Website&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Michigan AI Alumni Panel</title>
      <link>https://MSAIL.github.io/talk/alumni_panel111521/</link>
      <pubDate>Mon, 15 Nov 2021 20:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/alumni_panel111521/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Anthony Zheng, Kiran Prasad, Andong Li Zhao, Christian Kavouras&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Michigan AI Alumni Panel&lt;/p&gt;
&lt;p&gt;We hosted a virtual speaker panel with a few UMich alumni who are currently doing very exciting work with AI in industry and academia.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Anthony Zheng is an Apple research engineer working on search algorithms for Apple Media Products&lt;/li&gt;
&lt;li&gt;Kiran Prasad is an Applied Scientist at Microsoft working on NLP models for Microsoft products and was at CMU for his MS in AI and Innovation&lt;/li&gt;
&lt;li&gt;Andong Li Zhao is a CS PhD student at Northwestern working on making information more democratically accessible&lt;/li&gt;
&lt;li&gt;Christian Kavouras is a former Applied Scientist intern at Amazon working on ML/NLP applications and graduated from UWashington for his MS in Computational Linguistics&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/10JTvSD_VORdCg-E9IYoPhS1mZczLmKywwfkfXWlhOek/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andong&amp;rsquo;s Slides&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://docs.google.com/presentation/d/1rKEofg8u7B7gRVuciozTMA2Ob6ykUszGJV6DhDuEmDI/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anthony&amp;rsquo;s Slides&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://drive.google.com/file/d/1QpWAzvGtJbur6sh08Yyru6h6KD2bgVVR/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christian&amp;rsquo;s Slides&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://drive.google.com/file/d/1kmj0a64CNVaoNex9-sCpBqAn-PVUGbEB/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kiran&amp;rsquo;s Slides&lt;/a&gt;&lt;br&gt;
Check Slack or 
&lt;a href=&#34;https://MSAIL.github.io/contact/&#34;&gt;contact us&lt;/a&gt; if you&amp;rsquo;re interested in getting their contact info!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding MLOps for Computer Vision Pipelines</title>
      <link>https://MSAIL.github.io/talk/datature_110821/</link>
      <pubDate>Mon, 08 Nov 2021 19:30:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/datature_110821/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Datature Team&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Understanding MLOps for Computer Vision Pipelines&lt;/p&gt;
&lt;p&gt;We hosted an industry talk with Datature, a no-code platform that allows teams and enterprises to build computer vision models. In this session, they covered key MLOps practices and the shift from &amp;lsquo;model-centric AI&amp;rsquo; development to a &amp;lsquo;data-centric&amp;rsquo; approach in the context of computer vision. There was also a &amp;lsquo;hands-on&amp;rsquo; aspect where students were able to build a facemask detection / chess piece detection model in under 30 minutes using Datature&amp;rsquo;s no-code platform.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;We have a link with a tutorial for using Datature&amp;rsquo;s MLOps platform, but it is UMich only. If you are a UMich student interested in seeing it, please reach out to the 
&lt;a href=&#34;https://MSAIL.github.io/contact/&#34;&gt;MSAIL admin team&lt;/a&gt; and we will happily pass it along.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://ml-ops.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLOps Website&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://blogs.nvidia.com/blog/2020/09/03/what-is-mlops/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIDIA MLOps Blog Post&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contrastive Learning with Hard Negative Samples</title>
      <link>https://MSAIL.github.io/talk/contrastive_hns_110121/</link>
      <pubDate>Mon, 01 Nov 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/contrastive_hns_110121/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Kevin Wang&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Contrastive Learning with Hard Negative Samples&lt;/p&gt;
&lt;p&gt;Kevin talked about the paper 
&lt;a href=&#34;https://arxiv.org/abs/2010.04592v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contrastive Learning with Hard Negative Samples&lt;/a&gt;, by Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, Stefanie Jegelka. In the past two years, contrastive learning has emerged as a powerful unsupervised computer vision technique for learning effective representations of data for downstream tasks. This theory-focused paper proposes a technique for sampling &amp;ldquo;hard&amp;rdquo; negative examples in contrastive learning. The authors note improved performance on downstream tasks compared to SimCLR and faster training.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lilian Weng&amp;rsquo;s blog post on contrastive representation learning&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ekin Tiu&amp;rsquo;s post on contrastive learning&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google SimCLR&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Latent Semantic Analysis</title>
      <link>https://MSAIL.github.io/talk/lsa_102521/</link>
      <pubDate>Mon, 25 Oct 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/lsa_102521/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Nisreen Bahrainwala&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Latent Semantic Analysis: An Overview&lt;/p&gt;
&lt;p&gt;Latent Semantic Analysis is one of the many methods used to help computers &amp;ldquo;understand&amp;rdquo; meaning behind words and phrases, aiding with tasks such as search response relevance. This discussion will introduce the concept of LSA, some of the methods used during its development, and then explore how this technology has shaped modern NLP methods.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Paper(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.333.7403&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Solution to Plato&amp;rsquo;s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;http://lsa.colorado.edu/papers/dp1.LSAintro.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Latent Semantic Analysis&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speech Emotion Recognition with ML</title>
      <link>https://MSAIL.github.io/talk/speech_emotion_recog_100421/</link>
      <pubDate>Mon, 04 Oct 2021 19:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/speech_emotion_recog_100421/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Lance Ying&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Speech Emotion Recognition with Machine Learning&lt;/p&gt;
&lt;p&gt;This talk begins with a brief introduction of speech emotion recognition (SER) with machine learning and its applications. A few challenges in SER tasks and existing solutions are discussed. The second half of the talk focused on a recent paper and methods (Nonparametric Hierarchical Neural Network) to account for variations in emotional expression due to demographic and contextual factors for SER tasks.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/11JTc3HdFXn3P4QY8tO8egKGy0-rVLegY/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a link to the recording here. (UM only)&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/2109.04316&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Accounting for Variations in Speech Emotion Recognition with Nonparametric Hierarchical Neural Network&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://cse.engin.umich.edu/wp-content/uploads/2019/11/EECS_498_598_Affective_Computing.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 498 - ML and Affective Computing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Effective Representations for Small Molecules</title>
      <link>https://MSAIL.github.io/talk/chemdesc_092721/</link>
      <pubDate>Mon, 27 Sep 2021 19:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/chemdesc_092721/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Mukundh Murthy&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Bioactivity Descriptors for Uncharacterized Chemical Compounds&lt;/p&gt;
&lt;p&gt;Quantitative structure-activity modeling (QSAR) in computational chemistry is a task that involves predicting the binding affinity of a small molecule to a protein target given solely its molecular structure. Now, however, we are also interested in predicting more downstream properties including toxicity, side effects, and effects on gene expression – properties that concern both the biological and chemical properties of a molecule. This talk discussed the paper &amp;ldquo;Bioactivity Descriptors for uncharacterized chemical compounds,&amp;rdquo; which revolves around learning a generalizable and multi-modal representation for small molecules that can be applied across a large array of drug-discovery related tasks through integration of 25 small molecule datasets and a triplet network training task.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1sHALTeoucfLeO1aZGyhrLL6SYBXRgovI/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of this talk here (UM only).&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2020.07.21.214197v1.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bioactivity Descriptors for Uncharacterized Chemical Compounds&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.notion.so/Computational-Biochemistry-e57c4194c4234a898ecf2db36bb74015&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Biochemistry Primer by Mukundh Murthy and Michael Trinh&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://pubs.acs.org/doi/pdf/10.1021/acs.accounts.0c00699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ML for Molecular Property Prediction&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/1703.00564.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MoleculeNet&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AlphaFold 2 and the Protein Folding Problem</title>
      <link>https://MSAIL.github.io/talk/alphafold_2_092021/</link>
      <pubDate>Mon, 20 Sep 2021 19:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/alphafold_2_092021/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Ashwin Sreevatsa&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: AlphaFold 2&lt;/p&gt;
&lt;p&gt;The protein folding problem is one of the central challenges of biology over the past 50 years. The challenge is to identify the 3D structure of a protein given its amino acid sequence. Recently, DeepMind released a deep learning model called AlphaFold 2 that outperformed the state-of-the-art computational methods and predicted the 3D structures of proteins so accurately that many in the field now consider protein folding to be &amp;lsquo;solved&amp;rsquo;. This talk discussed a brief history of the protein folding problem, the architecture behind AlphaFold 2, and the next steps for protein folding and computational biology as a whole.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nature.com/articles/s41586-021-03819-2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaFold 2 Paper in Nature&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://github.com/deepmind/alphafold/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaFold 2 Source Code on Github&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepMind blog post on the initial AlphaFold&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepMind blog post on AlphaFold 2&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=nGVFbPKrRWQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video: AlphaFold and the Grand Challenge to solve protein folding by Arxiv Insights&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb#scrollTo=woIxeCPygt7K&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaFold 2 Example on Google Colab&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How We Built This: TDM Studio and Sentiment Analysis</title>
      <link>https://MSAIL.github.io/talk/proquest_041321/</link>
      <pubDate>Tue, 13 Apr 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/proquest_041321/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Dan Hepp and John Dillon&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: How We Built This: TDM Studio and Sentiment Analysis&lt;/p&gt;
&lt;p&gt;Dan Hepp is a Data Scientist Lead at ProQuest. Dan has thirty years of experience in research and production settings developing complex systems. He has a demonstrated track record of finding creative solutions to difficult technical problems and making them effective in real-world situations. Dan has expertise in machine learning, data
mining, information extraction, pattern recognition, information retrieval, natural language processing, computer vision, artificial intelligence, and optical character recognition.&lt;/p&gt;
&lt;p&gt;John Dillon, Ph.D., is the Text and Data Mining Product Manager at ProQuest. His work focuses on pairing computational text analysis methods with traditional Humanities and Cultural Studies disciplines. He has published papers on Machine Learning and Sentiment Analysis and has worked previously as a postdoctoral researcher with the University of Notre Dame, USAID, and IBM Research.&lt;/p&gt;
&lt;p&gt;This presentation consisted of two parts: The first part provided a history and overview of what it took to build TDM Studio from a product development standpoint. TDM Studio is a text and data mining solution offered by ProQuest. In the first part of the presentation, they gave us some practical insights into what to do and what not to do when trying to create a startup-esque product within a mid-sized company. The second portion of the presentation dug a little deeper into one aspect of TDM Studio, sentiment analysis. They discussed their work with the 2020 MDP Sentiment Analysis team and the results of their approach to the problem.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1VwbknZPyXw20qxhKnMmnaw6gxaIYNjK7/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of his talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://tdmstudio.proquest.com/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TDM Studio&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://mdp.engin.umich.edu/sponsor_teams/proquest-ocr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MDP Team Description&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intelligent Politics: How AI Can Improve Our Political Institutions and Systems</title>
      <link>https://MSAIL.github.io/talk/politics_ai_systems_040521/</link>
      <pubDate>Tue, 06 Apr 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/politics_ai_systems_040521/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://andongluis.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andong Luis Li Zhao&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Intelligent Politics: How AI Can Improve Our Political Institutions and Systems&lt;/p&gt;
&lt;p&gt;Andong Luis Li Zhao is a Computer Science PhD student at Northwestern University, working in the C3 Lab under Prof. Kristian Hammond. His main research focus is modernizing our political systems through AI. He is currently working on making political information more transparent by building systems that can understand vaguely-articulated questions, obtain the correct data analysis, and identify the most appropriate representation of that analysis.&lt;/p&gt;
&lt;p&gt;While his specific focus is currently on providing the public with access to information about our political system, this work is part of a broader goal of improving how society functions through socially-conscious AI grounded in real systems. Too often technologists abdicate their social responsibility by focusing on technical development. Instead, by developing human-centered AI technology that helps inform people and uncover novel insights, we can focus on the betterment of social, political, and economic systems and their impact.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1AvjuKSlNVVurQWtyFg6qUh1F8ZB6X9ft/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of his talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://scales-okn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SCALES: Transforming the Accessibility and Transparency of Federal Courts&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://sites.northwestern.edu/c3lab/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C3 Lab&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Harmful Bias in Natural Language Generation</title>
      <link>https://MSAIL.github.io/talk/harmful_bias_nlg/</link>
      <pubDate>Tue, 30 Mar 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/harmful_bias_nlg/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Yashmeet Gambhir&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Harmful Bias in Natural Language Generation&lt;/p&gt;
&lt;p&gt;Large language models have taken over the NLP scene and have led to a surge of state-of-art development in natural language generation tasks (machine translation, story generation, chatbots, etc.). However, these models have been shown to reflect many harmful societal biases that exist in text around the internet. This talk will go over two major papers studying harmful bias in large LMs: the first identifies and quantifies this bias, the second will attempt to mitigate bias.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1909.01326&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Woman worked as a Babysitter: On Biases in Language Generation&lt;/a&gt;

&lt;a href=&#34;https://www.aclweb.org/anthology/2020.findings-emnlp.291.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Controllable Biases in Language Generation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Transformers for Vision</title>
      <link>https://MSAIL.github.io/talk/image-worth-16x16-words/</link>
      <pubDate>Tue, 09 Mar 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/image-worth-16x16-words/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Andrew Awad and Drake Svoboda&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Using Transformers for Computer Vision&lt;/p&gt;
&lt;p&gt;In recent years we&amp;rsquo;ve seen the rise of transformers in natural language processing research, burgeoning the field to incredible heights. However, these very same transformers were seldom applied to computer vision tasks until recently. Andrew and Drake discussed how transformers have been used in vision tasks in recent years in a presentation covering two papers. The first, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (via Google Brain), is the &amp;ldquo;Attention is All You Need&amp;rdquo; of vision. Namely, this paper covers how one can construct a vision architecture devoid of the commonly applied CNN and still achieve comparable or better performance results while possibly cutting down computing resources. The second paper, End-to-End Object Detection with Transformers (via FAIR), formalizes the object detection task in a unique way that affords the usage of transformers.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2010.11929.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2005.12872.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;End-to-End Object Detection with Transformers&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Proving Theorems with Generative Language Models</title>
      <link>https://MSAIL.github.io/talk/generative_language_modeling/</link>
      <pubDate>Mon, 01 Mar 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/generative_language_modeling/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Ashwin Sreevatsa&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Generative Language Modeling for Automated Theorem Proving Presentation&lt;/p&gt;
&lt;p&gt;In the past decade, deep learning and artificial neural networks have been incredibly successful at a variety of tasks such as computer vision, translation, game playing, and robotics among others. However, there have been less examples of deep learning making progress with reasoning related tasks- such as automated theorem proving, the task of proving mathematical theorems using computer programs. This paper explores the use of transformer-based models to automated theorem proving and presents GPT-f, a deep learning-based automated prover and proof assistant.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2009.03393.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generative Language Modeling for Automated Theorem Proving Presentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lightning Round -- Assorted AI Topics</title>
      <link>https://MSAIL.github.io/previous_material/lightning/</link>
      <pubDate>Sat, 27 Feb 2021 15:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/lightning/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Lightning Round &amp;ndash; Assorted AI Topics&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;Here, we talk about a wide range of topics in AI that haven&amp;rsquo;t received their own slide decks &amp;ndash; the list includes reinforcement learning, optimization, adversarial machine learning, meta learning, active learning, multi-agent systems, and more. We hope that showcasing the breadth of AI research inspires you to dig deeper on your own and find what interests you!&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/169IpCxkST0Fjp7LjQgpdfiDz-Ccs1K-v/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1uQzkFpr4LyslagkloUHs5lnCQ3wNmVLfuaa7UsbTaGA/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ethics</title>
      <link>https://MSAIL.github.io/previous_material/ethics/</link>
      <pubDate>Fri, 26 Feb 2021 17:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/ethics/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Ethics in AI Research&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;We discuss the various ethical problems AI research presents, including well-known problems like bias and weaponized AI and less publicized problems like interpretability and environmental impact of large machine learning models. We also talk about some of the solutions that researchers are attempting to implement and what we can do to contribute.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1C-bWWrhh_hK6ZwNmLEYK95uLJ6eiCbi1/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1KUUqzdz-Te1oNS4AMnxxPqO_mFUpmkDNokr0As9rCHQ/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cognitive Load Estimation</title>
      <link>https://MSAIL.github.io/talk/cognitive_load_estimation/</link>
      <pubDate>Tue, 23 Feb 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/cognitive_load_estimation/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Patrick Morgan&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Cognitive Load Estimation&lt;/p&gt;
&lt;p&gt;Cognitive load has been shown, over hundreds of studies, to be an important variable for understanding human performance. However, establishing practical, non-contact, automated methods for estimating cognitive loads under real-world conditions is an un-solved problem. In this paper, Fridman et. al. proposes two novel vison-based methods for cognitive-load estimation. These methods address a important and challenging problem that has huge implications and can be used to ensure safety in tasks ranging from driving cars to operating machinery.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://www.researchgate.net/profile/Lex-Fridman/publication/324658835_Cognitive_Load_Estimation_in_the_Wild/links/5bf0ba3092851c6b27c74bd1/Cognitive-Load-Estimation-in-the-Wild.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cognitive Load Estimation in the Wild&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised Learning</title>
      <link>https://MSAIL.github.io/previous_material/unsupervised/</link>
      <pubDate>Sat, 20 Feb 2021 15:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/unsupervised/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Unsupervised Learning&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;This lesson went over the unsupervised side of AI, where labels don&amp;rsquo;t exist and models are left on their own to learn useful information. We presented machine learning approaches with and without deep learning that tackle unsupervised problems.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1WXBrrbNDryufUYkzQS1aLwrEcJRbo-xS/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1H77BDYebNusyelevFe5-AHZzYCaOB1tid-Vqtmm13oI/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Natural Language Processing</title>
      <link>https://MSAIL.github.io/previous_material/nlp/</link>
      <pubDate>Fri, 19 Feb 2021 17:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/nlp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Natural Language Processing&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;This lesson gave a high level overview of NLP (natural language processing) and how AI can be used to work with text and speech data. Points of discussion included recurrent neural networks, LSTMs/GRUs, and GPT-3 and other transformer models.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1DjwaY3p7vb4N4V7DwvZEBQB2qxjs5okS/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/178FNnk3x8euXO3NHBqQT9VT6-d9FigUVOt56Ru-Lvpo/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep RL for Robotics: A Short Overview</title>
      <link>https://MSAIL.github.io/talk/deeprlrobotics_021621/</link>
      <pubDate>Tue, 16 Feb 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/deeprlrobotics_021621/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Nikhil Devraj&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: A Brief Overview of Deep RL in Robotics&lt;/p&gt;
&lt;p&gt;Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world.&lt;br&gt;
This discussion focused predominantly on the following questions: &lt;br&gt;
(1) What is deep RL and how does it relate to robotics?&lt;br&gt;
(2) What are some examples of studies done with Deep RL in robotics?&lt;br&gt;
(3) What are major challenges faced by researchers who apply deep RL to robotics?&lt;/p&gt;
&lt;p&gt;This discussion is heavily inspired by 
&lt;a href=&#34;https://arxiv.org/pdf/2102.02915.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ibarz et al.&lt;/a&gt;, although it does not dive into that level of detail.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1gshp58d3yce4LhcLpTymch8IA2cbm4ZO/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find the recording of this talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2102.02915.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to Train Your Robot with Deep Reinforcement Learning - Lessons We&amp;rsquo;ve Learned&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1610.00633&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/0278364913495721&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reinforcement learning in robotics: a survey&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Articles:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://medium.com/@vmayoral/reinforcement-learning-in-robotics-d2609702f71b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium: Reinforcement learning in robotics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://ieor8100.github.io/rl/docs/RL%20in%20Robotics.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Better slides (in our presenter&amp;rsquo;s opinion)&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=GX_RonOFe1U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep RL Towards Robotics by Shane Gu (Google Brain)&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=luzOblzznIc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep RL in Robotics with NVIDIA Jetson&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer Vision</title>
      <link>https://MSAIL.github.io/previous_material/computer_vision/</link>
      <pubDate>Sat, 13 Feb 2021 15:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/computer_vision/</guid>
      <description>&lt;p&gt;​
&lt;strong&gt;Topic&lt;/strong&gt;: An Overview of Computer Vision&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang​&lt;/p&gt;
&lt;p&gt;This lesson gave a basic overview of the computer vision problem space. We discussed historically significant developments including convolutional neural networks, AlexNet, ResNet, and more, and we gave a glimpse at ongoing research.​&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/15WxV2hC40Bz4YhcyPVeYqb1gvIViq2Ka/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1MaC9d25kJybNv_pOYQHFv9oNOM1J-65zMkQX2PMlCqg/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction and Basics of Deep Learning</title>
      <link>https://MSAIL.github.io/previous_material/intro_dl_basics/</link>
      <pubDate>Fri, 12 Feb 2021 17:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/intro_dl_basics/</guid>
      <description>&lt;p&gt;​
&lt;strong&gt;Topic&lt;/strong&gt;: Introduction to AI Research and Basics of Deep Learning&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang  ​&lt;/p&gt;
&lt;p&gt;This lesson introduced the format of lessons for the winter 2021 semester, briefly introducing the topics to be presented in the coming weeks. We then gave a high-level overview of neural networks, which form the basis of deep learning and drive much of AI research today. ​&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1lNhpuuxNhW5nHDLavDLqlLOKv-DYfMhO/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;​&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1SkI0i1Y_Dp1lZTCJjJD91f0DVf_CXfB4FMBy8jLweeg/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faculty Talk: Situated Language Processing and Embodied Dialogue</title>
      <link>https://MSAIL.github.io/talk/chai_120120/</link>
      <pubDate>Tue, 01 Dec 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/chai_120120/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://web.eecs.umich.edu/~chaijy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Joyce Chai&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Situated Language Processing Towards Interactive Task Learning&lt;/p&gt;
&lt;p&gt;Prof. Chai discussed some of her research on situated language processing, which is a field describing the interaction of language and visual/motor processing in embodied, situated, and language-for-action research traditions. This research also aims to unite converging and complementary evidence from behavioral, neuroscientific, neuropsychological and computational methods.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1f0Ye_j-aa1D893AHun9to2_3rgAV7FbY/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of her talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.ijcai.org/Proceedings/2018/0001.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language to Action: Towards Interactive Task Learning with Physical Agents&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;http://sled-group.eecs.umich.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Situated Language and Embodied Dialogue (SLED) Group&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faculty Talk: Prediction Markets and More</title>
      <link>https://MSAIL.github.io/talk/kutty_111720/</link>
      <pubDate>Tue, 17 Nov 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/kutty_111720/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://scholar.google.com/citations?user=8duCIlcAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr. Sindhu Kutty&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Prediction Markets, Recommender Systems, Fairness in AI&lt;/p&gt;
&lt;p&gt;Dr. Kutty discussed some of her research on 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Prediction_market&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;prediction markets&lt;/a&gt;, 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Recommender_system&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recommender systems&lt;/a&gt;, and fairness in AI. The talk mostly focused on some derivations for prediction markets, such as scoring functions for data collection.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Dr. Kutty asked us not to post the recording for this talk. We apologize for any inconvenience.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Scoring_rule&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scoring Rules&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/1402.5458.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Information Aggregation in Exponential Family Markets&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Text Summarization with Deep Learning</title>
      <link>https://MSAIL.github.io/talk/textsummarization_111020/</link>
      <pubDate>Tue, 10 Nov 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/textsummarization_111020/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Yashmeet Gambhir&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Text Summarization with Deep Learning&lt;/p&gt;
&lt;p&gt;Yash discussed 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Automatic_summarization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;text summarization&lt;/a&gt;, where the goal is to&amp;hellip; summarize text. More specifically, he discussed abstractive summarization, of which the goal is to generate &lt;em&gt;novel&lt;/em&gt; sentences using natural language generation techniques. One such method for doing this is using pointer-generator networks. After discussing PGNs, he went on to discuss a paper describing extreme summarization to combat model hallucination for this task. The papers discussed are linked below.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1GCeGWfC_9jF6BDlLalEpKt9-KIlq_Hvv/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of his talk here.&lt;/a&gt; &lt;em&gt;Unfortunately this only includes the second half of the talk about abstractive summarization, because we forgot to record starting at the beginning.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/P17-1099/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Get To The Point: Summarization with Pointer-Generator Networks&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.173/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On Faithfulness and Factuality in Abstractive Summarization&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks</title>
      <link>https://MSAIL.github.io/previous_material/cnn/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/cnn/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Convolutional Neural Networks&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;This lesson covered convolutional neural networks, which serve as the backbone for many modern-day deep learning applications. Most commonly, convolutional neural networks are used for vision tasks (although not exclusively).&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1522OsXalZScvuUxXrOTbUZuISZUY-HqO&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides on CNNs&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://docs.google.com/presentation/d/16TMR2sM9T75qALw3CCigUF_JxMQ5gceM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides on Neural Networks&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Brain-Inspired AI</title>
      <link>https://MSAIL.github.io/talk/brain_insp_110320/</link>
      <pubDate>Tue, 03 Nov 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/brain_insp_110320/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://johnmday.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Day&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Brain-inspired AI&lt;/p&gt;
&lt;p&gt;John started his talk by discussing brain-inspired AI in general, which involves studies like 
&lt;a href=&#34;https://www.nature.com/articles/531S16a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;neural modeling&lt;/a&gt;, 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_consciousness#:~:text=Artificial%20consciousness%20%28AC%29%2C%20also,artificial%20intelligence%20and%20cognitive%20robotics.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;artificial consciousness&lt;/a&gt;, 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Spiking_neural_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spiking neural nets&lt;/a&gt;, and 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Cognitive_architecture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cognitive architectures&lt;/a&gt;. Afterwards, he focused on deep predictive coding networks.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1Gly--En531JIa4F_h15TWAftKrJPEd5W/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of his talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.cell.com/neuron/pdf/S0896-6273%2817%2930509-3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brain-inspired AI&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;http://klab.tch.harvard.edu/publications/PDFs/gk7591_Lotteretal_ICLR2017.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/1802.04762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Predictive Coding Network for Object Recognition&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faculty Talk: Cognitive Architecture</title>
      <link>https://MSAIL.github.io/talk/laird_102720/</link>
      <pubDate>Tue, 27 Oct 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/laird_102720/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://laird.engin.umich.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. John Laird&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Cognitive Architecture&lt;/p&gt;
&lt;p&gt;Prof. Laird discussed 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Cognitive_architecture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cognitive architecture&lt;/a&gt; - more specifically, he discussed 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Soar_%28cognitive_architecture%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SOAR&lt;/a&gt;, a cognitive architecture that his research group has been developing and maintaining for decades. SOAR is simultaneously a theory of cognition and an architecture, with the ultimate goal of enabling general intelligent agents to realize the full cognitive capabilities of humans.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1zlkGdcEo9Ycp2ziZbOXhIGH5R53DQzBJ/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of his talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://soar.eecs.umich.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SOAR Group Homepage&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://mitpress.mit.edu/books/soar-cognitive-architecture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The SOAR Cognitive Architecture, written by John E. Laird&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=VM1PGpvCEHI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A similar talk given to MSAIL back in 2011!&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image-to-Image Translation with Conditional Adversarial Networks</title>
      <link>https://MSAIL.github.io/talk/cgan_102020/</link>
      <pubDate>Tue, 20 Oct 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/cgan_102020/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Andrew Awad&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: 
&lt;a href=&#34;https://arxiv.org/abs/1611.07004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Andrew presented on a CVPR 2017 paper by Isola et al. This paper aimed to investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. The networks in question were 
&lt;a href=&#34;https://arxiv.org/abs/1411.1784&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CGANs&lt;/a&gt;, proposed earlier by Mirza et al. Isola et al. also proposed the 
&lt;a href=&#34;https://paperswithcode.com/method/patchgan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PatchGAN&lt;/a&gt; discriminator.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A certain forgetful lead admin forgot to record this discussion. We apologize for the inconvenience&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Paper(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1611.07004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1411.1784&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CGAN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://paperswithcode.com/method/patchgan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PatchGAN&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://phillipi.github.io/pix2pix/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pix2pix GitHub Page&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://towardsdatascience.com/gan-pix2pix-generative-model-c9bf5d691bac&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium article on pix2pix&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reinforcement Learning Applied to COVID-19 Optimization Problems</title>
      <link>https://MSAIL.github.io/talk/rlcovid_101320/</link>
      <pubDate>Tue, 13 Oct 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/rlcovid_101320/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Nikhil Devraj&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Reinforcement Learning for COVID-19 Optimization Problems&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re not living under a rock, you know that COVID-19 is ravaging current-day society and requires monumental efforts on all scales, be it from individuals or from entire governments. In particular, governments play a major role in helping control the spread of COVID-19 by instituting policies to help with efforts such as lockdown enforcement and vaccine distribution. During this talk Nikhil talked about some previously proposed approaches to modeling such policy problems as control problems that could be solved with reinforcement learning.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1Xi2tofO321kWTMz_2pD9ltuAc-XqHTnY/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of this discussion here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Paper(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2009.04647.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COVID-19 Pandemic Cyclic Lockdown Optimization Using Reinforcement Learning&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://journals.sagepub.com/doi/full/10.1177/0165551520959798&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optimal policy learning for COVID-19 prevention using reinforcement learning&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2009.06602.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VacSIM: LEARNING EFFECTIVE STRATEGIES FOR COVID-19 VACCINE DISTRIBUTION USING REINFORCEMENT LEARNING&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Article(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://towardsdatascience.com/reinforcement-learning-for-covid-19-simulation-and-optimal-policy-b90719820a7f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reinforcement learning for Covid-19: Simulation and Optimal Policy&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Regression, Part 2 (Application)</title>
      <link>https://MSAIL.github.io/previous_material/regression_2/</link>
      <pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/regression_2/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Applications of Regression &lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Robert Aung&lt;/p&gt;
&lt;p&gt;This lesson covered some interesting applications of regression.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://colab.research.google.com/drive/12nmYKp5IcUdUiZmrHaUK7YsUu1vIOXkj?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab notebook&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Human-Centered Autonomous Vehicles</title>
      <link>https://MSAIL.github.io/talk/av_100620/</link>
      <pubDate>Tue, 06 Oct 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/av_100620/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Patrick Morgan&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Human-Centered Autonomous Vehicles&lt;/p&gt;
&lt;p&gt;Patrick focused on discussing 
&lt;a href=&#34;https://arxiv.org/pdf/1810.01835.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human-Centered Autonomous Vehicle Systems: Principles of Effective Shared Autonomy&lt;/a&gt;. This paper proposes that we should build autonomous vehicles with humans in mind, and that getting humans and artificial intelligence systems to collaborate effectively is an achievable and worthy goal. In this light, they propose a human-centered paradigm for engineering shared autonomy systems in the car that erase the boundary between human and machine in the way the driving task is experienced. The researchers propose a 7 principle engineering design process that will make autonomous vehicles safer and greatly lower the cost of development. This discussion also ended up touching on other fundamental issues in AI, such as data privacy.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1fZvu5nDO3qhgwwc1X85aDmQ44RHD9bWF/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of this discussion here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Paper(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/1810.01835.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human-Centered Autonomous Vehicle Systems: Principles of Effective Shared Autonomy&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AlphaZero and its Impact on the World of Chess</title>
      <link>https://MSAIL.github.io/talk/alphazero_chess_092920/</link>
      <pubDate>Tue, 29 Sep 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/alphazero_chess_092920/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Kevin Wang&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: AlphaZero and its Impact on Chess&lt;/p&gt;
&lt;p&gt;The world was appalled when 
&lt;a href=&#34;https://ai.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaGo&lt;/a&gt; first 
&lt;a href=&#34;https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;played Lee Sedol in Go&lt;/a&gt;, winning 4 matches to 1. DeepMind subsequently released 
&lt;a href=&#34;https://deepmind.com/blog/article/alphago-zero-starting-scratch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaGo Zero&lt;/a&gt;, an iteration on AlphaGo that beat it 100-1. Going even further, they released 
&lt;a href=&#34;https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaZero&lt;/a&gt;, which learned how to play games such as Shogi and Chess. Kevin, an avid chess enthusiast, wanted to discuss what this meant for the Chess world.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1jETQcqIZqp24drl8ARH4bABuv963g0Ah/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of this discussion here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Paper(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2009.04374.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Assessing Game Balance with AlphaZero: Exploring Alternative Rule Sets in Chess&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/1712.01815.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mastering Chess and Shogi by Self-Play with a General RL Algorithm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video(s):&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=7L2sUGcOgh0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video From DeepMind&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=mOqmLYlFdBo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaZero VS AlphaZero || THE PERFECT GAME&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Regression, Part 1 (Theory and Implementation)</title>
      <link>https://MSAIL.github.io/previous_material/regression_1/</link>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/regression_1/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Theory and Implementation of Regression&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Robert Aung&lt;/p&gt;
&lt;p&gt;This lesson covered the theory behind and implementation of a linear regression model with gradient descent.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://umich.zoom.us/rec/share/94fSO_w_AT68Td2e0Qr_kckIVBepdNLecMn5mTvFOH994JWIkKSZLl3u9xpFr6J6.oj49dWOJeBFBzPA2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1VHWuE_lqbKnDKZ8HKbVcLArbe8cMWKsd_61FY4FTn-E/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://colab.research.google.com/drive/18MoSHNwUnEKwvokZAZA1AcLc6AJ3Bs81?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson Colab notebook&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faculty Talk: Strategic Reasoning in Dynamic Environments</title>
      <link>https://MSAIL.github.io/talk/wellman_092220/</link>
      <pubDate>Tue, 22 Sep 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/wellman_092220/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;http://strategicreasoning.org/michael-p-wellman/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Michael Wellman&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Strategic Reasoning in Dynamic Environments&lt;/p&gt;
&lt;p&gt;Dr. Wellman presented about his group&amp;rsquo;s research, which generally specializes in game theory and multi-agent reasoning in dynamic environments. Much of his work lies in the domain of markets and commerce. You can find his 
&lt;a href=&#34;https://strategicreasoning.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;group&amp;rsquo;s page here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Prof. Wellman asked us not to post the recording publicly. A recording is available within our Slack channel, so please search in there if you&amp;rsquo;re interested. We apologize for any inconvenience.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Article(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://strategicreasoning.org/empirical-game-theoretic-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Game-Theoretic Analysis&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://strategicreasoning.org/computational-finance/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Finance&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://strategicreasoning.org/world-with-autonomous-agents/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;World with Autonomous Agents&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video(s):&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=SnTf-iWUTpk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Artificially Intelligent Decision Makers in the Real World&amp;rdquo; with Michael Wellman &amp;amp; Bill Powers&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Classification with Logistic Regression</title>
      <link>https://MSAIL.github.io/previous_material/classification_logreg/</link>
      <pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/classification_logreg/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Classification with Logistic Regression&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;Kevin taught some members of MSAIL some basics of machine learning, culminating in building out a classification model for MNIST from scratch using logistic regression. Classification is the process of categorizing data into predetermined groups, and logistic regression is a means to build a classifier (though certainly not the only means to do so).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;We couldn&amp;rsquo;t find the recording of this session, but be sure to check out the supplemental materials.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1YVw4T0E_f6m0NovhS3YbwAMLns0tQfFE/edit#slide=id.p1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://colab.research.google.com/drive/1Cein0r-J9N2vX1xh24cRLEHgBkJx3p7w?authuser=1#scrollTo=ubgi9PVZDDgU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson Colab notebook&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://builtin.com/data-science/basic-linear-algebra-deep-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Basic matrix operations&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.kaggle.com/learn/python&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Basic Python programming&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Trend Towards Large Language Models</title>
      <link>https://MSAIL.github.io/talk/gpt3_091520/</link>
      <pubDate>Tue, 15 Sep 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/gpt3_091520/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Sean Stapleton&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: GPT-3 and its Implications&lt;/p&gt;
&lt;p&gt;In recent years, we’ve seen 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Natural_language_processing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;natural language processing&lt;/a&gt; (NLP) performance accelerate drastically across a number of tasks, including text completion, 
&lt;a href=&#34;https://paperswithcode.com/task/machine-translation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;machine translation&lt;/a&gt;, and 
&lt;a href=&#34;https://paperswithcode.com/task/question-answering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;question answering&lt;/a&gt;. Much of this performance gain has been attributed to two trends in the NLP community, namely the introduction of transformers, and the increase in model size (and consequent need for intense computational power). Capitalizing on these trends, 
&lt;a href=&#34;https://openai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI&lt;/a&gt; recently released a transformer-based model called 
&lt;a href=&#34;https://en.wikipedia.org/wiki/gpt-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT-3&lt;/a&gt; with 175 billion parameters, that was trained on roughly 500 billion tokens scraped from the internet.
This MSAIL discussion focused predominantly on three questions addressed in the paper:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Does a substantial increase in model size actually lead to better performance in downstream tasks?&lt;/li&gt;
&lt;li&gt;Can language models effectively model intelligent and adaptable thought?&lt;/li&gt;
&lt;li&gt;What are the biases and risks associated with training a language model on the entire internet?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Sean also covered the transformer and GPT-3 model architectures, though the focus of the discussion was not on this aspect of the paper.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/12beS3Er1AuiCbmxNR0rAiiot43xTkw_n/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find the recording of this talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language Models are Few-Shot Learners (Brown et al.)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Articles:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Illustrated Transformer&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;http://jalammar.github.io/illustrated-gpt2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Illustrated GPT-2&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bloomberg Tech Talk: Applied Named Entity Recognition</title>
      <link>https://MSAIL.github.io/talk/ner_090920/</link>
      <pubDate>Wed, 09 Sep 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/ner_090920/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://www.preotiuc.ro/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniel Preotiuc-Pietro&lt;/a&gt; and 
&lt;a href=&#34;https://scholar.google.com/citations?user=ycBuNT0AAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mayank Kulkarni&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Applied Named Entity Recognition&lt;/p&gt;
&lt;p&gt;During this talk, two senior research scientists from 
&lt;a href=&#34;https://www.techatbloomberg.com/ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bloomberg&amp;rsquo;s AI Group&lt;/a&gt; presented on some of their work on Applied 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Named-entity_recognition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Named Entity Recognition&lt;/a&gt;. Their discussion focused on applications of NER at Bloomberg, multi-domain NER, and analysis of NER using temporal data.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Due to restrictions from Bloomberg, we were unable to record this session. We apologize for the inconvenience.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers from Bloomberg AI&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.680/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Temporally-Informed Analysis of Named Entity Recognition&lt;/a&gt;, ACL 2020&lt;br&gt;

&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.750/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference&lt;/a&gt;, ACL 2020&lt;br&gt;

&lt;a href=&#34;https://www.aclweb.org/anthology/P19-1587/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Semi-Markov Structured Support Vector Machine Model for High-Precision Named Entity Recognition&lt;/a&gt;, ACL 2019&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides from Bloomberg AI&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://slideslive.com/38929187/multidomain-named-entity-recognition-with-genreaware-and-agnostic-inference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multi-Domain NER&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
