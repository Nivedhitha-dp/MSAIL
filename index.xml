<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>MSAIL</title>
    <link>https://MSAIL.github.io/</link>
      <atom:link href="https://MSAIL.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>MSAIL</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Thu, 28 Mar 2024 18:00:00 -0400</lastBuildDate>
    <image>
      <url>https://MSAIL.github.io/media/logo.png</url>
      <title>MSAIL</title>
      <link>https://MSAIL.github.io/</link>
    </image>
    
    <item>
      <title>MSAIL TECH TALK w/ Wesley Tian</title>
      <link>https://MSAIL.github.io/talk/wesleytian_240324/</link>
      <pubDate>Thu, 28 Mar 2024 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/wesleytian_240324/</guid>
      <description>&lt;p&gt;University of Michigan alumni and former MSAIL member Wesley Tian, the Co-founder and CEO of Aragon.ai, will talk about his journey, share what he learned from starting his AI company, and offer career advice.&lt;/p&gt;
&lt;p&gt;RSVP for this event 
&lt;a href=&#34;hhttps://forms.gle/724P8z5wdHryhCf5A&#34;&gt;here&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MSAIL TECH TALK w/ Kiran Prasad</title>
      <link>https://MSAIL.github.io/talk/prasad_210324/</link>
      <pubDate>Thu, 21 Mar 2024 19:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/prasad_210324/</guid>
      <description>&lt;p&gt;University of Michigan and Carnegie Mellon alumni Kiran Prasad will share his professional journey, thoughts about Grad School, and give a talk about machine learning concepts that he uses in industry. Kiran is a Senior ML Engineer at Gather, where he works on end-to-end AI system design. He previously was an Applied Scientist on the Microsoft Turing team (Microsoft&amp;rsquo;s NLP v-team that created CoPilot and spearheaded collaboration with OpenAI).&lt;/p&gt;
&lt;p&gt;RSVP for this event 
&lt;a href=&#34;https://docs.google.com/forms/d/e/1FAIpQLSdXVZybnuUJ2zZRsa2jNBnoK4MXrEcgPn41f2HTH5G8vI5Kuw/viewform&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Overview of Binarized Neural Networks</title>
      <link>https://MSAIL.github.io/talk/bnn_041222/</link>
      <pubDate>Tue, 12 Apr 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/bnn_041222/</guid>
      <description>&lt;p&gt;Binarized neural networks (BNNs) are an extreme version of quantized neural networks where all weights and activations are quantized to +/- 1. A key motivation for such a network is to enable one to run powerful neural networks on small battery-powered devices. This talk introduced BNNs, explained how one can train such a network and reviewed some recent work in the area.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1602.02830&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Binarized Neural Networks: Training Deep Neural Networks with Weights and Activations Constrained to +1 or -1, by Courbariaux et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.mdpi.com/2079-9292/8/6/661&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A review of Binarized Neural Networks, by Simons et al.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://openreview.net/pdf?id=rJfUCoR5KX&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Empirical Study of Binarized Neural Networks&amp;rsquo; Optimization, by Alizadeh et al.&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Machine Learning for Intraoperative Diagnosis of Brain Tumors Imaged using Stimulated Raman Histology</title>
      <link>https://MSAIL.github.io/talk/brain_tumor_040522/</link>
      <pubDate>Tue, 05 Apr 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/brain_tumor_040522/</guid>
      <description>&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nature.com/articles/s41591-019-0715-9&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper 1: Near real-time intraoperative brain tumor diagnosis using stimulated Raman histology and deep neural networks&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://journals.lww.com/neurosurgery/Abstract/9900/Rapid_Automated_Analysis_of_Skull_Base_Tumor.184.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper 2: Rapid Automated Analysis of Skull Base Tumor Specimens Using Intraoperative Optical Imaging and Artificial Intelligence&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Devil is in the Details: Spatial and Temporal Super-Resolution of Global Climate Models using Adversarial Deep Learning</title>
      <link>https://MSAIL.github.io/talk/climate_adversarial_032922/</link>
      <pubDate>Tue, 29 Mar 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/climate_adversarial_032922/</guid>
      <description>&lt;p&gt;Physics-based global climate simulations are computationally expensive and limited to low spatial and temporal resolutions, making it difficult to predict and track highly localized extreme weather phenomena. To overcome these limitations, we present a novel application of super-resolution using deep convolutional generative adversarial networks (GANs) to increase the resolution of global climate models in both space and time. In this project, we demonstrate the potential to reduce climate simulation computation and storage requirements by two orders of magnitude, as well as democratize relevant and actionable climate information for disaster responses. This work won the Best Paper Award in the 2020 ProjectX international ML research competition hosted by the University of Toronto.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1cbwTb7DNe0vRZiN9hg53W5MZdRbXJqsg/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper, by Chen et al.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Generating Molecular Conformations via Normalizing Flows and Neural ODEs</title>
      <link>https://MSAIL.github.io/post/molecular_iclr_2022/</link>
      <pubDate>Fri, 25 Mar 2022 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/post/molecular_iclr_2022/</guid>
      <description>&lt;p&gt;This post was an accepted submission from MSAIL to the 
&lt;a href=&#34;https://iclr-blog-track.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICLR 2022 Blog Track&lt;/a&gt;. You can find the original post 
&lt;a href=&#34;https://iclr-blog-track.github.io/2022/03/25/conformation-generation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;In this post, we provide an in-depth overview of methods outlined in the paper &amp;ldquo;Learning Neural Generative Dynamics for Molecular Conformation Generation,&amp;rdquo; discuss the impact of the work in the context of other conformation generation approaches, and additionally discuss future potential applications to improve the diversity and stability of generated conformations.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#introduction&#34;&gt;Introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#generative-overview&#34;&gt;An Overview of the Deep Generative Approach&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#distances&#34;&gt;Modeling distributions of distances&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#conformations&#34;&gt;Modeling distributions of conformations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#sampling&#34;&gt;Sampling&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#future-work&#34;&gt;Future Work&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In drug discovery, generating molecular conformations is useful across a variety of applications. For example, docking of various molecular 3D conformations to a specific protein allows drug hunters to decide whether a small molecule binds to a specific pocket in a multitude of conformations or a select few.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;vina.jpg&#34; style=&#34;width: 300px&#34;&gt;
  &lt;b&gt;Figure 1:&lt;/b&gt; Autodock Vina is a computer program that takes a given 3D conformation of a molecule and protein and predicts the binding free energy. An algorithm like the one discussed in this blog could generate a wide variety of conformations for Autodock Vina to test. (&lt;a href=&#34;https://vina.scripps.edu/&#34;&gt;Source&lt;/a&gt;)
&lt;/p&gt;
&lt;p&gt;It may be helpful to define what we mean when we talk about conformations, whether we are talking about a small organic molecule or a macromolecule like a protein. We start off with a graph, with atoms as nodes connected by bonds as edges that represent intramolecular interactions. In essence, we are starting with a specified connectivity defining how atoms are connected to each other. This two-dimensional representation, however, doesn&amp;rsquo;t capture the three-dimensional coordinates of the atoms and how they are spatially arranged.&lt;/p&gt;
&lt;p&gt;Therefore, in theory, one molecular graph could capture an astronomical number of conformations capturing all possible permutations and combinations of spatial arrangements of atoms. However, not all of these possible spatial arrangements are relevant as some may be so unstable that they may not occur. The spatial proximity of bulky organic groups – more formally known as “steric clashing” – reduces the number of degrees of freedom when it comes down to which bonds can rotate and how much they can rotate. Therefore, we are only interested in conformations that fall in stable low energy minima.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Levinthal%27s_paradox#:~:text=Levinthal%27s%20paradox%20is%20a%20thought,astronomical%20number%20of%20possible%20conformations.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Levinthal’s Paradox&lt;/a&gt; is a principle stating that if a protein were to sample all of its possible molecular conformations before arriving in its native state, it would take longer than the age of the universe. Though it may seem excessive to directly extend this analogy to small molecules, which are orders of magnitude less complex than proteins, it becomes intuitive that computationally simulating all of the possible conformations for a large molecule with a large number of rotatable bonds is highly infeasible. For every single bond and the associated substituents, if there are three stable conformations, then there is a maximum bound of $3^n$ stable conformations for a molecule with $n$ bonds. For example, a molecule with ten rotatable bonds could have a maximum of 59,049 conformations.
Now, we’ve arrived at the question that drives this blog post and the work that we’re about to discuss: &lt;strong&gt;Given a molecular graph and its associated connectivity constraints, can we generate a set of low energy stable molecular conformations (a multimodal distribution) that capture the relative spatial positions of atoms in three-dimensional space?&lt;/strong&gt;
There are two subtle components to the question above that address some deficiencies in prior attempts to solve this problem:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;A multimodal distribution – there are multiple low energy minima when it comes to the joint distribution of distances between atoms that defines a conformation. In approaches where distances between pairs of atoms or 3D coordinates are randomly sampled to construct a conformation, dependencies and correlations between atomic spatial positions are not captured and the corresponding joint distribution is inaccurate.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Relative spatial positions – some approaches use 
&lt;a href=&#34;https://distill.pub/2021/gnn-intro/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;graph neural networks&lt;/a&gt; directly on molecular graphs to compute representations for the individual nodes (atoms). These nodes can be further fed into other feedforward networks to predict the 3D coordinates of the atoms in a specified conformation. However, directly predicting the 3D coordinates does not capture the idea that a conformation is defined by the relative spatial arrangement and distances between atoms in 3D space. Put another way, if a rotation or translation transformation was applied to the 3D coordinates, the model should not classify that as an entirely different conformation (rotation/translation invariance is not captured). Distances, rather than 3D coordinates could also be predicted; however (mirroring the bullet point above), since distances are predicted independently of each other, there could only be one predicted conformational mode.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;an-overview-of-the-deep-generative-approach&#34;&gt;An Overview of the Deep Generative Approach&lt;/h2&gt;
&lt;p&gt;In “Learning Neural Generative Dynamics for Molecular Conformation Generation,” Xu et. al approach the above deficiencies, generating low energy conformations while modeling dependencies between atoms.&lt;/p&gt;
&lt;p&gt;Let’s keep in mind – the final goal is to optimize a set of parameters $\theta$ to predict the likelihood of a conformation $R$ given a graph $G$. (i.e. to find $ p_\theta(R|G) $).&lt;/p&gt;
&lt;p&gt;To model this distribution, it is necessary to model intermediate distributions and marginalize over one of the variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;We also need to find $p_\theta(d|G)$ (the distribution of distances $d_{uv}$ between pairs of atoms $u$ and $v$ in the graph).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Finally, we need to find $p_\theta(\boldsymbol{R}|d,G)$  – the probability of a conformation (specified by a set of 3D coordinates given a set of intramolecular distances and an underlying graph).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;With these two distributions, we can find our intended distribution by integrating over the possible distances.&lt;/p&gt;
&lt;p&gt;$$\int{p(\boldsymbol{R}|d,G)*p(d|G)dd}$$&lt;/p&gt;
&lt;p&gt;Let’s walk through the approaches to modeling each of these individual distributions.&lt;/p&gt;
&lt;h2 id=&#34;modeling-distributions-of-distances&#34;&gt;Modeling Distributions of Distances&lt;/h2&gt;
&lt;p&gt;In this approach, the distribution of distances given a graph is modeled using a continuous normalizing flow. To understand this approach, we need to define its sub-techniques and understand how they interact with each other.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Normalizing flows: We initially sample $z_0$  from a starting distribution $p(z_0)$ and a series of invertible transformations transform the initial density function. Here’s a strong 
&lt;a href=&#34;https://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;primer&lt;/a&gt; on flows.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;In this work, $z(t)$ represents our distances between pairs of atoms $d(t)$. The initial distances are pulled from a normal distribution with mean zero and variance one (for all distances). Correspondingly, the initial probability density function $p(z_0)$ is represented by the initial distribution of distances $N(0, \mathbf{I})$.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1806.07366.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural ODE systems&lt;/a&gt;: In a neural ODE, we specify an initial value problem that uses a neural network to specify the “dynamics” of the system (or the derivative of the “state” with respect to time). More concretely, we have that $y(0) = y_0$ and that $\frac{dy}{dt} = f(y(t), t, \theta)$. Using an ODE solver such as &lt;code&gt;odeint&lt;/code&gt;, we can calculate the value of $y$ at any time $t$ as in any initial value problem. &lt;br&gt; In fact, y can be thought of as a 
&lt;a href=&#34;http://implicit-layers-tutorial.org/neural_odes/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;residual network&lt;/a&gt; where we take the limit with respect to the number of layers.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Correspondingly, in this work, the purpose of instantiating an ODE is to be able to predict $d(t)$ – the distances between each pair of atoms at any time point. $\frac{d\mathbf{d}}{dt}$ can be predicted at any time point given $d(t)$, the time point, $t$, the molecular graph, and the parameters of the assigned neural network (in our case an MPNN).&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$$\boldsymbol{d} = F_\theta(\boldsymbol{d}(t_0), \mathcal{G}) = \boldsymbol{d}(t_0) + \int_{t_0}^{t_1} f_\theta(\boldsymbol{d}(t), t; \mathcal{G})dt$$&lt;/p&gt;
&lt;p&gt;To combine the two methods above: We take $z_0$ and define it as the initial value. $\frac{dz}{dt}$ is calculated using a neural network that takes in $z(t)$, $t$, and $\theta$. With $z_0$ and a function $f$ to calculate $\frac{dz}{dt}$ at any time point, $z(t)$ can be calculated as per the traditional initial value problem formulation. The ODESolver also predicts the $\textrm{log}(p(z(t))$ at any time point, thereby encoding the density function for $z(t)$ in addition to just the values of $z(t)$ alone (Figure 2).&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;ode.png&#34;&gt;
  &lt;b&gt; Figure 2: &lt;/b&gt; The neural ODE system computes $\boldsymbol{d}(t)$ and $\textrm{log}(p(\boldsymbol{d}(t))$ at various time points in order to try and approximate the actual functions for $\boldsymbol{d}(t)$ and $\textrm{log}(p(\boldsymbol{d}(t))$.
&lt;/p&gt;
&lt;p&gt;In this case, our $z(t)$ is $\boldsymbol{d}(t)$, a function that outputs a vector with pairwise intramolecular distances. The “continuous-time dynamics&amp;quot; is a function that takes in neural network parameters, the time, and the current state to output the derivative of the distances with respect to time. The neural network is a graph 
&lt;a href=&#34;https://paperswithcode.com/method/mpnn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;message passing neural network&lt;/a&gt; (MPNN) that calculates node and edge representations and aggregates the node and edge representations for each bond to calculate $\frac{dd_{uv}}{dt}$ – the change of the distance between two atoms with respect to time (Figure 3).&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;mpnn.png&#34;&gt;
  &lt;b&gt; Figure 3: &lt;/b&gt; First, the individual nodes and edges are embedded using feedforward networks and sent through message passing layers. For every single bond, the final embeddings for the edge and atoms on each (atoms $u$ and $v$) end are concatenated and sent into a final feedforward network to result in a prediction for $\frac{dd_{uv}}{dt}$.
&lt;/p&gt;
&lt;p&gt;At a higher level, by combining normalizing flows (Figure 4a) with an ODE system, the authors intended to effectively create a normalizing flow with an infinite number of transformations (in the limit) that can therefore model very long-range dependencies between atoms in all the transformations that occur from time $t_0$ to $t_1$ (Figure 4b).&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;nflow.png&#34;&gt;
  &lt;b&gt; Figure 4a (Left): &lt;/b&gt; Traditional normalizing flow. &lt;b&gt; Figure 4b (Right): &lt;/b&gt; Continuous normalizing flow with $z(t)$ as $d(t)$.
&lt;/p&gt;
&lt;h2 id=&#34;modeling-distributions-of-conformations&#34;&gt;Modeling Distributions of Conformations&lt;/h2&gt;
&lt;p&gt;After the distances are sampled and predicted based on the graph, the conformations can be sampled so as to minimize the difference between the a priori distances generated by the continuous graph normalizing flow (CGNF) and the pairwise distances in the sampled conformation.&lt;/p&gt;
&lt;p&gt;$$p(\boldsymbol{R}|d, \mathcal{G}) = \frac{1}{Z}\textrm{exp}{-\sum_{e_{uv}\in{\mathcal{E}}} a_{uv}(\lVert r_u - r_v \rVert_2 - d_{uv})^2}$$&lt;/p&gt;
&lt;p&gt;The euclidean norm of the difference between the position vectors represents the distance between two atoms in a sampled conformation ($\lVert r_u - r_v \rVert_2$). The distance associated with the edge between atoms u and v from the distribution modeled using the CGNF is ($d_{uv}$). The lower the difference between these two values, the higher the numerator. The higher the numerator, the higher the probability of the conformation given the proposed distances and molecular graph.&lt;/p&gt;
&lt;p&gt;In the way that 
&lt;a href=&#34;http://yann.lecun.com/exdb/publis/pdf/lecun-06.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LeCun et. al&lt;/a&gt; initially describe energy-based models, they describe the energy-based function $E(X, Y)$ to calculate the “goodness” or the “badness” of the possible configurations of $X$ and $Y$ or the “degree of compatibility” between the values of $X$ and $Y$. The same idea can be applied when considering the meaning of the energy function taking in a molecular conformation and a graph as input.&lt;/p&gt;
&lt;p&gt;The loss function with which the energy-based model (EBM) is optimized provides additional insight into how it helps guide the generation of conformations.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;nce_loss.png&#34;&gt;
&lt;/p&gt;
&lt;p&gt;Here, $p_{data}$ and  $p_{\theta}$ are two different distributions that generate distances between pairs of atoms. $p_{data}$ pulls from vectors of true distances between atoms in actual conformations, while $p_{\theta}$ pulls from vectors of generated distances from the continuous flow. Therefore, the conformations represented in the second term on the right-hand side of this equation are noisier than the conformations represented in the first term. By being trained against this objective function, the model learns to distinguish real conformations based on true distances from unreal noisy conformations.&lt;/p&gt;
&lt;h2 id=&#34;sampling&#34;&gt;Sampling&lt;/h2&gt;
&lt;p&gt;Conformations are sampled by pulling an initial vector of distances from a normal distribution, passing it through the continuous graph normalizing flow, and finding an initial conformation $R_0$ that minimizes the energy. Then, conformations are sampled using two steps of stochastic gradient 
&lt;a href=&#34;https://henripal.github.io/blog/langevin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Langevin Dynamics&lt;/a&gt;. As in traditional stochastic gradient descent, we subtract the gradient of a secondary energy function that uses both the initial EBM parameters and CGNF parameters from the coordinates from the prior iteration. The “Langevin” part of this stochastic gradient descent implies there is a noise term ($w$) added, the variance of which is equal to the square root of the step size ($\epsilon$). This noise term, and Langevin dynamics more generally, are inspired by modeling 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Brownian_motion&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brownian motion&lt;/a&gt; in particles and have been repurposed for sampling in molecular dynamics.&lt;/p&gt;
&lt;p&gt;The secondary function takes into account both the initial energy function and the $\textrm{log}(p(\boldsymbol{R}|\mathcal{G}))$. Minimizing $E_{\theta, \phi}(R|\mathcal{G})$ involves $E_{\phi}(R|\mathcal{G})$ and simultaneously minimizing $p(\boldsymbol{R}|\mathcal{G})$.&lt;/p&gt;
&lt;p&gt;$$R_k = R_{k-1} - \frac{\epsilon}{2}\nabla_RE_{\theta, \phi}(R|\mathcal{G}) + \sqrt{\epsilon}\omega, \omega \sim \mathcal{N}(0, \mathcal{I})$$&lt;/p&gt;
&lt;h2 id=&#34;future-work&#34;&gt;Future Work&lt;/h2&gt;
&lt;p&gt;One could explore different variations on the approach used to compute the continuous-time dynamic – for example, 
&lt;a href=&#34;https://arxiv.org/abs/2010.09885&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;large-scale pretrained transformers applied on SMILES strings&lt;/a&gt; – to compare how different architectures that are also able to capture long-range dependencies between atoms perform in generating distance distributions and subsequently conformations. Similar to the way that message passing allows for encoding of long-range dependencies, attention also allows for the same. In fact, attention applied to protein sequences has been shown to recover high-level elements of a three-dimensional structural organization; attention weights are a well-calibrated estimator of the probability that two amino acids are in contact in three-dimensional space (Vig et. al).&lt;/p&gt;
&lt;p&gt;One caveat to note concerning the idea above is many models pretrained on protein sequences include evolutionary information regarding the sequences through featurizations such as multiple sequence alignments
(
&lt;a href=&#34;https://www.ebi.ac.uk/Tools/msa/#:~:text=Multiple%20Sequence%20Alignment%20%28MSA%29%20is,relationships%20between%20the%20sequences%20studied.&amp;amp;text=Suitable%20for%20medium%2Dlarge%20alignments.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MSA&lt;/a&gt;) and position-specific scoring matrices (
&lt;a href=&#34;https://en.wikipedia.org/wiki/Position_weight_matrix&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PSSM&lt;/a&gt;) (Rao et. al). There are currently no featurizations for small molecules that encode their  “structural evolution.”&lt;/p&gt;
&lt;p&gt;One could also verify the ability of the different molecular conformation generation methods to generate more stable conformations. Towards the end of the paper, the authors proposed that the EBM shifts generation towards more stable conformations. Developing a metric or computational experiment – for example, calculating the free energy of generated molecules – would verify if this is the case. Or we could potentially even ask the question – is there an architectural or algorithmic knob that we could turn to control the tradeoff the algorithm makes between choosing conformational stability over diversity? To evaluate the model’s ability to especially generate low energy stable conformations, one could re-calculate all metrics solely across reference conformations for molecules bound to a protein in the protein data bank (PDB) (Figure 5) or 
&lt;a href=&#34;https://www.ccdc.cam.ac.uk/solutions/csd-core/components/csd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cambridge Structural Database&lt;/a&gt; (CSD) in a solid-state crystal structure.&lt;/p&gt;
&lt;p align=&#34;center&#34;&gt;
  &lt;img src=&#34;mol.png&#34; style=&#34;width: 300px&#34;&gt;
  &lt;b&gt; Figure 5: &lt;/b&gt; Example of conformational variability for a single PDB ligand between different protein structures (Source: Hawkins et. al).
&lt;/p&gt;
&lt;p&gt;Finally, Hawkins et. al make the distinction between systematic methods and stochastic methods for molecular conformation generation. Systematic methods involve a deterministic brute force search through all possible pairwise distances and torsion angles while stochastic methods involve random sampling and are not deterministic. Rather, in stochastic methods, the final generated conformation is in part determined by some initially sampled random variable). Under these definitions, the current method proposed in this work is stochastic, as the generated conformations are a function of the initial $d(t_0)$’s sampled from a normal distribution.&lt;/p&gt;
&lt;p&gt;For stochastic approaches to finding multiple local minima, it is necessary to have multiple “starts” in order to cover all local minima. To evaluate the efficiency of the approach, one could measure the number of starts it takes to get a certain threshold of coverage over significant low-energy conformations.&lt;/p&gt;
&lt;p&gt;All in all, the approach that Xu et. al employ to generate 3D conformers from a 2D molecular graph is part of a recent frontier in research that involves fewer brute-force physical simulations and more convenient ML-guided predictions that can help accelerate drug discovery.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Chen, R. T. Q., Rubanova, Y., Bettencourt, J., &amp;amp; Duvenaud, D. (2019). Neural Ordinary Differential Equations. arXiv [cs.LG]. Opgehaal van 
&lt;a href=&#34;http://arxiv.org/abs/1806.07366&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://arxiv.org/abs/1806.07366&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Hawkins, P. C. D. (2017). Conformation Generation: The State of the Art. Journal of Chemical Information and Modeling, 57(8), 1747–1756. doi:10.1021/acs.jcim.7b00221&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Madani, A., Krause, B., Greene, E. R., Subramanian, S., Mohr, B. P., Holton, J. M., … Naik, N. (2021). Deep neural language modeling enables functional protein generation across families. bioRxiv. doi:10.1101/2021.07.18.452833&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rao, R., Bhattacharya, N., Thomas, N., Duan, Y., Chen, X., Canny, J., … Song, Y. S. (2019). Evaluating Protein Transfer Learning with TAPE. arXiv [cs.LG]. Opgehaal van 
&lt;a href=&#34;http://arxiv.org/abs/1906.08230&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://arxiv.org/abs/1906.08230&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vig, J., Madani, A., Varshney, L. R., Xiong, C., Socher, R., &amp;amp; Rajani, N. F. (2020). BERTology Meets Biology: Interpreting Attention in Protein Language Models. bioRxiv. doi:10.1101/2020.06.26.174417&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Weng, L. (2018). Flow-based Deep Generative Models. lilianweng. github. io/lil-log. Opgehaal van 
&lt;a href=&#34;http://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://lilianweng.github.io/lil-log/2018/10/13/flow-based-deep-generative-models.html&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Xu, M., Luo, S., Bengio, Y., Peng, J., &amp;amp; Tang, J. (2021). Learning Neural Generative Dynamics for Molecular Conformation Generation. arXiv [cs.LG]. Opgehaal van 
&lt;a href=&#34;http://arxiv.org/abs/2102.10240&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;http://arxiv.org/abs/2102.10240&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and F Huang. A tutorial on energy-based learning. Predicting structured data, 1(0), 2006.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Fairness in Machine Learning</title>
      <link>https://MSAIL.github.io/talk/fairness_032222/</link>
      <pubDate>Tue, 22 Mar 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/fairness_032222/</guid>
      <description>&lt;p&gt;The purpose of this presentation was to introduce everyone to fairness aspects of machine learning and discuss Serafina&amp;rsquo;s (
&lt;a href=&#34;https://cse.engin.umich.edu/stories/undergraduate-researcher-takes-first-place-at-acm-competition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;award-winning!&lt;/a&gt;) research in the area.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://link.springer.com/chapter/10.1007/978-3-030-93736-2_43&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Robustness of Fairness: An Experimental Analysis, by Kamp et al.&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://docs.google.com/presentation/d/1fVakENisRjTh55vB6K25I0JqMMo3Dh77GLBbFJSc-CA/edit#slide=id.g11d502cf596_0_17&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides with additional links and resources&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Overview of Attention and Transformer Mechanisms for NLP</title>
      <link>https://MSAIL.github.io/talk/attention_031522/</link>
      <pubDate>Tue, 15 Mar 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/attention_031522/</guid>
      <description>&lt;p&gt;Nisreen explained the technical aspects of attention and self attention mechanisms, as well as explored how attention is used in the transformer architecture in order to aid in machine translation tasks.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/1409.0473&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural Machine Translation by Jointly Learning to Align and Translate, Bahdanau et al.&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention is all you Need, Vaswani et al.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Open Problems in Cooperative AI</title>
      <link>https://MSAIL.github.io/talk/cooperative_022222/</link>
      <pubDate>Tue, 22 Feb 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/cooperative_022222/</guid>
      <description>&lt;p&gt;A recent area of study in AI has been focused on the problem of cooperation amongst machine learning agents. These cooperation problems are widespread, from routine challenges such as driving on highways and working collaboratively, all the way up to global challenges like commerce, peace, and pandemic preparedness. If AI is to play a larger role in society, it is important that AI agents will be able to cooperate effectively with other agents (other AI, humans, etc).&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2012.08630.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Open Problems in Cooperative AI, Dafoe et al.&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.nature.com/articles/d41586-021-01170-0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cooperative AI: machines must learn to find common ground&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scaling Neural Tangent Kernels via Sketching and Random Features</title>
      <link>https://MSAIL.github.io/talk/ntk_020122/</link>
      <pubDate>Tue, 01 Feb 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/ntk_020122/</guid>
      <description>&lt;p&gt;Kevin presented 
&lt;a href=&#34;https://arxiv.org/pdf/2106.07880.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scaling Neural Tangent Kernels via Sketching and Random Features&lt;/a&gt;, which uses sketching and random feature generation to speed up neural tangent kernels (NTKs). This presentation was really all about introducing the NTK, a mechanism for analyzing the behavior of very wide / infinitely wide neural networks. NTKs made a huge splash in machine learning theory in 2018 for offering a novel approach to analyzing the behavior of neural networks, and there&amp;rsquo;s plenty of ground left to cover with them in research.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://towardsdatascience.com/the-kernel-trick-c98cdbcaeb3f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kernel trick&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=DObobAnELkU&amp;amp;feature=youtu.be&amp;amp;ab_channel=SoheilFeizi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Professor Feizi&amp;rsquo;s lecture&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://rajatvd.github.io/NTK/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rajat&amp;rsquo;s blog post&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1806.07572&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper #1, introduces NTKs&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1904.11955&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper #2, polynomial bounds NTK complexity and introduces CNTK&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1911.00809&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Paper #3, enhanced CNTK&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Concrete Problems in AI Safety</title>
      <link>https://MSAIL.github.io/talk/aisafety_012522/</link>
      <pubDate>Tue, 25 Jan 2022 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/aisafety_012522/</guid>
      <description>&lt;p&gt;Ashwin gave an introduction to the field of AI safety, which studies how to ensure that AI, especially artificial general intelligence and super intelligence, will be safe and trustworthy.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1606.06565.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Concrete Problems in AI Safety, by Amodei et al.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Human-in-the-Loop Natural Language Processing</title>
      <link>https://MSAIL.github.io/talk/jkk_120621/</link>
      <pubDate>Mon, 06 Dec 2021 20:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/jkk_120621/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Dr. Jonathan Kummerfeld&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Human-in-the-Loop Natural Language Processing&lt;/p&gt;
&lt;p&gt;Dr. Kummerfeld works on NLP, with many projects crossing into HCI, either in the process of creating datasets or developing systems. In this talk, he gave a brief introduction to NLP and Crowdsourcing + Human Computation, and then dove into two research projects. First, he discussed work on task-oriented dialogue (e.g. Siri), where his team developed new ways to collect more diverse data, which in turn leads to more robust models. Second, he discussed work on understanding a set of conversations occurring in a shared channel (e.g. in Slack).&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://jkk.name/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr. Kummerfeld&amp;rsquo;s Website&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Michigan AI Alumni Panel</title>
      <link>https://MSAIL.github.io/talk/alumni_panel111521/</link>
      <pubDate>Mon, 15 Nov 2021 20:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/alumni_panel111521/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Anthony Zheng, Kiran Prasad, Andong Li Zhao, Christian Kavouras&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Michigan AI Alumni Panel&lt;/p&gt;
&lt;p&gt;We hosted a virtual speaker panel with a few UMich alumni who are currently doing very exciting work with AI in industry and academia.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Anthony Zheng is an Apple research engineer working on search algorithms for Apple Media Products&lt;/li&gt;
&lt;li&gt;Kiran Prasad is an Applied Scientist at Microsoft working on NLP models for Microsoft products and was at CMU for his MS in AI and Innovation&lt;/li&gt;
&lt;li&gt;Andong Li Zhao is a CS PhD student at Northwestern working on making information more democratically accessible&lt;/li&gt;
&lt;li&gt;Christian Kavouras is a former Applied Scientist intern at Amazon working on ML/NLP applications and graduated from UWashington for his MS in Computational Linguistics&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/10JTvSD_VORdCg-E9IYoPhS1mZczLmKywwfkfXWlhOek/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andong&amp;rsquo;s Slides&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://docs.google.com/presentation/d/1rKEofg8u7B7gRVuciozTMA2Ob6ykUszGJV6DhDuEmDI/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Anthony&amp;rsquo;s Slides&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://drive.google.com/file/d/1QpWAzvGtJbur6sh08Yyru6h6KD2bgVVR/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Christian&amp;rsquo;s Slides&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://drive.google.com/file/d/1kmj0a64CNVaoNex9-sCpBqAn-PVUGbEB/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Kiran&amp;rsquo;s Slides&lt;/a&gt;&lt;br&gt;
Check Slack or 
&lt;a href=&#34;https://MSAIL.github.io/contact/&#34;&gt;contact us&lt;/a&gt; if you&amp;rsquo;re interested in getting their contact info!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Understanding MLOps for Computer Vision Pipelines</title>
      <link>https://MSAIL.github.io/talk/datature_110821/</link>
      <pubDate>Mon, 08 Nov 2021 19:30:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/datature_110821/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Datature Team&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Understanding MLOps for Computer Vision Pipelines&lt;/p&gt;
&lt;p&gt;We hosted an industry talk with Datature, a no-code platform that allows teams and enterprises to build computer vision models. In this session, they covered key MLOps practices and the shift from &amp;lsquo;model-centric AI&amp;rsquo; development to a &amp;lsquo;data-centric&amp;rsquo; approach in the context of computer vision. There was also a &amp;lsquo;hands-on&amp;rsquo; aspect where students were able to build a facemask detection / chess piece detection model in under 30 minutes using Datature&amp;rsquo;s no-code platform.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;We have a link with a tutorial for using Datature&amp;rsquo;s MLOps platform, but it is UMich only. If you are a UMich student interested in seeing it, please reach out to the 
&lt;a href=&#34;https://MSAIL.github.io/contact/&#34;&gt;MSAIL admin team&lt;/a&gt; and we will happily pass it along.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://ml-ops.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLOps Website&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://blogs.nvidia.com/blog/2020/09/03/what-is-mlops/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NVIDIA MLOps Blog Post&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Contrastive Learning with Hard Negative Samples</title>
      <link>https://MSAIL.github.io/talk/contrastive_hns_110121/</link>
      <pubDate>Mon, 01 Nov 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/contrastive_hns_110121/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Kevin Wang&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Contrastive Learning with Hard Negative Samples&lt;/p&gt;
&lt;p&gt;Kevin talked about the paper 
&lt;a href=&#34;https://arxiv.org/abs/2010.04592v2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contrastive Learning with Hard Negative Samples&lt;/a&gt;, by Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, Stefanie Jegelka. In the past two years, contrastive learning has emerged as a powerful unsupervised computer vision technique for learning effective representations of data for downstream tasks. This theory-focused paper proposes a technique for sampling &amp;ldquo;hard&amp;rdquo; negative examples in contrastive learning. The authors note improved performance on downstream tasks compared to SimCLR and faster training.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://lilianweng.github.io/lil-log/2021/05/31/contrastive-representation-learning.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lilian Weng&amp;rsquo;s blog post on contrastive representation learning&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ekin Tiu&amp;rsquo;s post on contrastive learning&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google SimCLR&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Latent Semantic Analysis</title>
      <link>https://MSAIL.github.io/talk/lsa_102521/</link>
      <pubDate>Mon, 25 Oct 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/lsa_102521/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Nisreen Bahrainwala&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Latent Semantic Analysis: An Overview&lt;/p&gt;
&lt;p&gt;Latent Semantic Analysis is one of the many methods used to help computers &amp;ldquo;understand&amp;rdquo; meaning behind words and phrases, aiding with tasks such as search response relevance. This discussion will introduce the concept of LSA, some of the methods used during its development, and then explore how this technology has shaped modern NLP methods.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Paper(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.333.7403&amp;amp;rep=rep1&amp;amp;type=pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Solution to Plato&amp;rsquo;s Problem: The Latent Semantic Analysis Theory of Acquisition, Induction, and Representation of Knowledge&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;http://lsa.colorado.edu/papers/dp1.LSAintro.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;An Introduction to Latent Semantic Analysis&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Speech Emotion Recognition with ML</title>
      <link>https://MSAIL.github.io/talk/speech_emotion_recog_100421/</link>
      <pubDate>Mon, 04 Oct 2021 19:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/speech_emotion_recog_100421/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Lance Ying&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Speech Emotion Recognition with Machine Learning&lt;/p&gt;
&lt;p&gt;This talk begins with a brief introduction of speech emotion recognition (SER) with machine learning and its applications. A few challenges in SER tasks and existing solutions are discussed. The second half of the talk focused on a recent paper and methods (Nonparametric Hierarchical Neural Network) to account for variations in emotional expression due to demographic and contextual factors for SER tasks.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/11JTc3HdFXn3P4QY8tO8egKGy0-rVLegY/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a link to the recording here. (UM only)&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/abs/2109.04316&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Accounting for Variations in Speech Emotion Recognition with Nonparametric Hierarchical Neural Network&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://cse.engin.umich.edu/wp-content/uploads/2019/11/EECS_498_598_Affective_Computing.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 498 - ML and Affective Computing&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Effective Representations for Small Molecules</title>
      <link>https://MSAIL.github.io/talk/chemdesc_092721/</link>
      <pubDate>Mon, 27 Sep 2021 19:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/chemdesc_092721/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Mukundh Murthy&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Bioactivity Descriptors for Uncharacterized Chemical Compounds&lt;/p&gt;
&lt;p&gt;Quantitative structure-activity modeling (QSAR) in computational chemistry is a task that involves predicting the binding affinity of a small molecule to a protein target given solely its molecular structure. Now, however, we are also interested in predicting more downstream properties including toxicity, side effects, and effects on gene expression – properties that concern both the biological and chemical properties of a molecule. This talk discussed the paper &amp;ldquo;Bioactivity Descriptors for uncharacterized chemical compounds,&amp;rdquo; which revolves around learning a generalizable and multi-modal representation for small molecules that can be applied across a large array of drug-discovery related tasks through integration of 25 small molecule datasets and a triplet network training task.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1sHALTeoucfLeO1aZGyhrLL6SYBXRgovI/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of this talk here (UM only).&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.biorxiv.org/content/10.1101/2020.07.21.214197v1.full&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bioactivity Descriptors for Uncharacterized Chemical Compounds&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.notion.so/Computational-Biochemistry-e57c4194c4234a898ecf2db36bb74015&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Biochemistry Primer by Mukundh Murthy and Michael Trinh&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://pubs.acs.org/doi/pdf/10.1021/acs.accounts.0c00699&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ML for Molecular Property Prediction&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/1703.00564.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MoleculeNet&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AlphaFold 2 and the Protein Folding Problem</title>
      <link>https://MSAIL.github.io/talk/alphafold_2_092021/</link>
      <pubDate>Mon, 20 Sep 2021 19:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/alphafold_2_092021/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Ashwin Sreevatsa&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: AlphaFold 2&lt;/p&gt;
&lt;p&gt;The protein folding problem is one of the central challenges of biology over the past 50 years. The challenge is to identify the 3D structure of a protein given its amino acid sequence. Recently, DeepMind released a deep learning model called AlphaFold 2 that outperformed the state-of-the-art computational methods and predicted the 3D structures of proteins so accurately that many in the field now consider protein folding to be &amp;lsquo;solved&amp;rsquo;. This talk discussed a brief history of the protein folding problem, the architecture behind AlphaFold 2, and the next steps for protein folding and computational biology as a whole.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.nature.com/articles/s41586-021-03819-2.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaFold 2 Paper in Nature&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://github.com/deepmind/alphafold/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaFold 2 Source Code on Github&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepMind blog post on the initial AlphaFold&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;DeepMind blog post on AlphaFold 2&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=nGVFbPKrRWQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video: AlphaFold and the Grand Challenge to solve protein folding by Arxiv Insights&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://colab.research.google.com/github/deepmind/alphafold/blob/main/notebooks/AlphaFold.ipynb#scrollTo=woIxeCPygt7K&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaFold 2 Example on Google Colab&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Data privacy and AI</title>
      <link>https://MSAIL.github.io/post/federated_learning/</link>
      <pubDate>Wed, 19 May 2021 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/post/federated_learning/</guid>
      <description>&lt;p&gt;We interact with machine learning algorithms every day&amp;ndash;from scrolling through social media, to navigating around town, to selecting word recommendations when we text. But what do we know about these machine learning methods? They’re data hungry. More specifically, they’re hungry for OUR data. Many of our interactions with technology must be tracked and stored to develop and improve these algorithms, and this need comes with an inherent privacy risk. By giving up complete access to our sensitive data, we allow ourselves to be vulnerable to both the companies that collect our information and third parties who could be interested in exploiting us. Luckily, there is a set of machine learning and statistical techniques that allow model developers to learn from our data while protecting our privacy first. In this post, we’ll specifically discuss how large companies like Google, Apple, and Microsoft use &lt;strong&gt;federated learning&lt;/strong&gt; and &lt;strong&gt;differential privacy&lt;/strong&gt; to develop the state of art AI algorithms and relevant insights that we benefit from every day.&lt;/p&gt;
&lt;h2 id=&#34;federated-learning&#34;&gt;Federated Learning&lt;/h2&gt;
&lt;p&gt;The first concept we’ll discuss is called federated learning, and it&amp;rsquo;s vital for making sure your data stays local to your device.&lt;/p&gt;
&lt;p&gt;Now let&amp;rsquo;s say we wanted to train a 
&lt;a href=&#34;https://towardsdatascience.com/building-a-next-word-predictor-in-tensorflow-e7e681d4f03f#:~:text=Next%20Word%20Prediction%20or%20what,or%20emails%20without%20realizing%20it.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;next word prediction&lt;/a&gt; model to give word recommendations to users as they type. This task involves predicting the next word in a sentence given the previous words, and these predictions can be seen above the keyboard as you type.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig1.png&#34; alt=&#34;Next Word Prediction&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Next word predictions are shown above your keyboard as your type. Machine learning models are trained to predict the next word you will type given the previous words in the &lt;a href=&#34;https://medium.com/@pankaj.karki.786/predicting-the-next-word-f10936cc5d4e&#34;&gt;sentence&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;If we had access to all of our users&amp;rsquo; mobile devices, we would need to access samples of text messages and collect them in a central server for us to train our model. But wait&amp;ndash;text messages contain extremely sensitive data, and this would be an extremely invasive data collection process. Instead of bringing all of the data to a central server with the model, we can use federated learning to learn a global model without a user’s data leaving their device.&lt;/p&gt;
&lt;p&gt;In a typical federated learning setting, one central server communicates with several clients (in this case, mobile phones) and trains a global model in several rounds. Each round consists of the following steps:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Select a sample of available phones for training and send the current global model to each phone&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In parallel, train the global model on each phone on their local text messages for one (or a few) epoch(s). Now each phone contains model updates to the global model that were calculated using their own small datasets&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The model updates from each phone are sent back to the central server and averaged to calculate the gradient update for the global model in this training round.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;fig2.png&#34; alt=&#34;Central Server Communication&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
An illustration of how a central server communicates with client servers in federated learning. Produced by &lt;a href=&#34;https://blog.ml.cmu.edu/2019/11/12/federated-learning-challenges-methods-and-future-directions/&#34;&gt;ML@CMU&lt;/a&gt;.
&lt;/p&gt;
&lt;p&gt;For next word prediction, some of the text data from the messages you send everyday are accessed for federated learning. Your phone might be selected for federated learning when it is being charged (to ensure your phone’s performance doesn’t drop) and you have strong wifi bandwidth. Next word prediction is a semi-supervised task (meaning we can automatically construct labels from the raw text data), so there is no additional need for labelling your local data. In a 
&lt;a href=&#34;https://arxiv.org/pdf/1902.01046.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2018 paper&lt;/a&gt;, Google showcased a 1.4 million parameter LSTM model that was trained for 5 days (3000 total rounds!) on over a million users that matched the performance of a centralized server trained model.&lt;/p&gt;
&lt;p&gt;Now, let’s examine how Apple uses a similar technique to personalize their news recommendation algorithm. As discussed in a 
&lt;a href=&#34;https://arxiv.org/pdf/2102.08503.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recent paper&lt;/a&gt;, they use federated learning to not only learn a global model, but to personalize their models to individual users in a privacy-preserving manner. Apple news is an app that curates personalized news article feeds for users to select and read articles from. These recommendations are based on your interactions with specific articles (e.g what articles you click on, how long you read them, etc.). Article-label pairs can be easily constructed and stored on your device (positive label if read, negative if not read). After learning a global recommendation model, the model can be tested on each client device and remotely fine tuned based on the prediction loss on each user’s specific dataset. The model first takes advantage of the huge knowledge base of users to create a powerful global model, and then can further personalize to each user without ever collecting any data.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig3.png&#34; alt=&#34;News Broadcast&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
An example of how news is broadcasted on the &lt;a href=&#34;https://www.macstories.net/news/apple-releases-ios-136-with-apple-news-audio-features-and-expanded-local-news-coverage-plus-digital-car-key-support/&#34;&gt;Apple News&lt;/a&gt; interface.
&lt;/p&gt;
&lt;p&gt;Now federated learning itself has some privacy limitations&amp;ndash;although Google or Apple might not have access to your specific data, they could theoretically learn about your data from the model updates sent back to the server. For better privacy guarantees, Google implements a method called 
&lt;a href=&#34;https://eprint.iacr.org/2017/281.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Secure Aggregation&lt;/a&gt;. This method encrypts each individual client model update and sends them to a trusted, third party server for encrypted aggregation&amp;ndash;the central server can now only decrypt the aggregated data and has no access to individual model updates.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig4.png&#34; alt=&#34;Model Training&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
The left picture represents data being sent to a central server to model training. The middle showcases how models are sent to local devices to be trained, and aggregate together on the central server. The right demonstrates how FL can be better secured by sending encrypted updates to a third party server for aggregation, and the decrypted result is made available to the central server. Figure used from &lt;a href=&#34;https://eprint.iacr.org/2017/281.pdf&#34;&gt;this paper.&lt;/a&gt;
&lt;/p&gt;
&lt;h2 id=&#34;differential-privacy&#34;&gt;Differential Privacy&lt;/h2&gt;
&lt;p&gt;The next technique that companies use to protect privacy aims to prevent your individual information from leaking through statistical queries, model predictions, and other analyses. This technique is called &lt;strong&gt;differential privacy&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The simplest definition of differential privacy from its 
&lt;a href=&#34;https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;original authors&lt;/a&gt; is: “the data holder makes a promise to the data subject that they will not be affected adversely or otherwise by allowing their data to be used in the study/analysis, no matter what other sources are available”. So if you participate in a dataset, differential privacy gives you some mathematical certainty that any statistical query or model trained on it will not reveal your personal data. From a researcher’s perspective, we want to learn about a dataset without learning about individuals that participate in that dataset. Let’s look at a quick example often used in surveys that collect sensitive information.&lt;/p&gt;
&lt;p&gt;Since the 1960s, sociologists have used a technique called &lt;em&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_response&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;randomized response&lt;/a&gt;&lt;/em&gt; in order to get statistics of a population regarding a sensitive topic while protecting each individual&amp;rsquo;s privacy. If a researcher wanted to know what percentage of a population has jaywalked, for example, they could give each participant in the study the following procedure:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Flip a coin without me seeing it. If it lands on Heads, answer truthfully about whether you’ve jaywalked previously.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;If it lands on Tails, answer your question according to the next coin flip. “Yes” if Heads, and “No” if tails.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Now, every participant can have &lt;em&gt;plausible deniability&lt;/em&gt; if it is discovered they answered yes or no to the question. Furthermore, researchers can still get a rough idea of the true global statistic to the question they asked: if the final amount of “Yes” answers occurs 70% of the time, we know half answered with the probability of the coin flip (50%), and the other half must have answered with the probability of 90%. So in this population, the true statistic must be around 90%. This still may not be exactly accurate due to randomness of the coin flip&amp;ndash;there is some tradeoff between privacy and accuracy when applying random noise to each data point.&lt;/p&gt;
&lt;p&gt;This procedure above is an example of &lt;strong&gt;local differential privacy&lt;/strong&gt;, where the goal is to add random noise to each &lt;em&gt;individual&lt;/em&gt; data point before it is entered into a database. The great thing here is that each individual participant does not need to trust the central data curator&amp;ndash;an ideal setting for the relationship between millions of users and big tech companies.&lt;/p&gt;
&lt;p&gt;Now, let’s take a look at how Apple uses this technique when collecting statistics on user activity. Apple must collect user data to determine what emojis are most popular among users or what specific online domains drain the most energy on Safari. The first step is encoding their data of interest (in this example, let’s say an emoji you used recently) into a fixed size matrix representation using a &lt;em&gt;hash function&lt;/em&gt;. Then, each bit of this matrix representation is changed to an incorrect value with some tunable probability value (anywhere from 1-25%).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig5.png&#34; alt=&#34;Apple emojis&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
An example graph of top emojis of US speakers from Apple’s differential privacy &lt;a href=&#34;https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf&#34;&gt;overview&lt;/a&gt;. Read more about how Apple obtains this visualization &lt;a href=&#34;https://machinelearning.apple.com/research/learning-with-privacy-at-scale&#34;&gt;here&lt;/a&gt;.&lt;/i&gt;
&lt;/p&gt;
&lt;p&gt;After this noise is added to each individual’s records, IP and other personal identifiers are stripped before the data is sent to the server. The final statistics are then aggregated on Apple’s servers for their internal use&amp;ndash;for example, Apple can identify the most popular emojis being used and design better ways of accessing/recommending them. If a user’s specific activity data was leaked from Apple’s central server, each user could have some level of plausible deniability that it wasn’t their correct data.&lt;/p&gt;
&lt;p&gt;Local DP is used by Google in order to track changes to user&amp;rsquo;s Chrome settings and combat malicious software that changes these settings without user permission. Google also employs DP in user facing analysis features like Google Search Trends and Google Maps&amp;rsquo; 
&lt;a href=&#34;https://policies.google.com/technologies/anonymization?hl=en-US&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;busyness&amp;rdquo; feature&lt;/a&gt;, which tells you how busy a place may be at any given time. Whether your data is being used by these companies to improve products or collected and aggregated for users to see, differential privacy is a useful technique that prevents malicious actors from personally identifying your data from a dataset.&lt;/p&gt;
&lt;h2 id=&#34;other-interesting-notes&#34;&gt;Other interesting notes&lt;/h2&gt;
&lt;p&gt;Large tech companies have also built open-source tools that support federated learning and differential privacy, which opens the door for researchers and developers to easily adopt these techniques in their applications. Google has built 
&lt;a href=&#34;https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/42852.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RAPPOR&lt;/a&gt; for differential privacy and 
&lt;a href=&#34;https://www.tensorflow.org/federated&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TensorFlow Federated&lt;/a&gt; for federated learning. Microsoft is using their differential privacy library 
&lt;a href=&#34;https://github.com/opendp/smartnoise-core&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SmartNoise&lt;/a&gt; with nonprofits and health care companies to provide privacy protections to the most sensitive personal data domains.&lt;/p&gt;
&lt;h2 id=&#34;concluding-remarks&#34;&gt;Concluding remarks&lt;/h2&gt;
&lt;p&gt;In this post, we learned about the fundamental concepts of federated learning and differential privacy and how Google and Apple access our data while protecting our individual privacy and ownership of that data. The examples discussed are just early use cases of these tools, and future applications are likely to arise. The discussion of data privacy is a very complex one, and these techniques by themselves won’t be the end all solution. At the end of the day, companies control these algorithms and protocols, and can manipulate them however they choose to. But in the world of big data, privacy preserving machine learning techniques can be the technical gateway to allowing users to regain control of their most personal and sensitive data, while maintaining the utility we gain from powerful machine learning models.&lt;/p&gt;
&lt;p&gt;What other data hungry applications or research projects do you think could take advantage of these useful techniques?&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1602.05629.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Communication-efficient learning of deep networks from decentralized data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/1902.01046.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards federated learning at scale: system design&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The algorithmic foundations of differential privacy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Randomized_response&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Randomized response&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.apple.com/privacy/docs/Differential_Privacy_Overview.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Apple&amp;rsquo;s differential privacy overview&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://policies.google.com/technologies/anonymization?hl=en-US&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How Google anonymizes data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://classroom.udacity.com/courses/ud185&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Udacity&amp;rsquo;s Secure and Private AI course&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>How We Built This: TDM Studio and Sentiment Analysis</title>
      <link>https://MSAIL.github.io/talk/proquest_041321/</link>
      <pubDate>Tue, 13 Apr 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/proquest_041321/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Dan Hepp and John Dillon&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: How We Built This: TDM Studio and Sentiment Analysis&lt;/p&gt;
&lt;p&gt;Dan Hepp is a Data Scientist Lead at ProQuest. Dan has thirty years of experience in research and production settings developing complex systems. He has a demonstrated track record of finding creative solutions to difficult technical problems and making them effective in real-world situations. Dan has expertise in machine learning, data
mining, information extraction, pattern recognition, information retrieval, natural language processing, computer vision, artificial intelligence, and optical character recognition.&lt;/p&gt;
&lt;p&gt;John Dillon, Ph.D., is the Text and Data Mining Product Manager at ProQuest. His work focuses on pairing computational text analysis methods with traditional Humanities and Cultural Studies disciplines. He has published papers on Machine Learning and Sentiment Analysis and has worked previously as a postdoctoral researcher with the University of Notre Dame, USAID, and IBM Research.&lt;/p&gt;
&lt;p&gt;This presentation consisted of two parts: The first part provided a history and overview of what it took to build TDM Studio from a product development standpoint. TDM Studio is a text and data mining solution offered by ProQuest. In the first part of the presentation, they gave us some practical insights into what to do and what not to do when trying to create a startup-esque product within a mid-sized company. The second portion of the presentation dug a little deeper into one aspect of TDM Studio, sentiment analysis. They discussed their work with the 2020 MDP Sentiment Analysis team and the results of their approach to the problem.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1VwbknZPyXw20qxhKnMmnaw6gxaIYNjK7/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of his talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://tdmstudio.proquest.com/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TDM Studio&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://mdp.engin.umich.edu/sponsor_teams/proquest-ocr/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MDP Team Description&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Do convolutional neural networks mimic the human visual system?</title>
      <link>https://MSAIL.github.io/post/cnn_human_visual/</link>
      <pubDate>Sun, 11 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/post/cnn_human_visual/</guid>
      <description>&lt;p&gt;Richard Feynman once said “What I cannot create I do not understand.” Therefore, to truly understand the human visual system, we must learn to create it. One of the most effective forms of such creation is the Convolutional Neural Network (CNN) system to mimic the human visual system. Computer Vision models that use the CNN system have achieved near human-level performances on tasks such as image classification and object detection. There is no question the CNNs have shown us mind-blowing performance, but the question is: do they actually resemble the biological visual system? Are we really creating it? As a quest to answer this question, in this article we will explore the similarities and differences between the CNNs and the biological visual system.&lt;/p&gt;
&lt;p&gt;To start off, let’s explore the roots of the CNN - how does a CNN function and what are its capabilities? In simple terms, the CNN is able to learn features from images, for example - it is able to deduce if an image has a dog; it is able to deduce if the image is a desert, it can tell if an image is a painting. Let’s take a look at some example features that CNNs can learn to extract from an image. In the image underneath, we can see that for a baseball, the CNN meshes the unique features of a baseball such as rounded shape, stripes, into a filter image. This filter image is then used to cross check against input images to determine whether each input image contains a baseball. The same is done for dogs, clouds and buildings, etc, with their own respective filter images. This way, we can use CNNs for tasks such as classification (labeling an image to a group/class), object detection (detecting the presence of certain objects in an image), and image generation (imitating certain styles and patterns to generate unique images), among many other tasks.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig1.png&#34; alt=&#34;Filters for detecting image features&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Use of filters for detecting image features [1]
&lt;/p&gt;
&lt;p&gt;So now that we have an idea of how CNNs function, we can move on to discussing how its design compares to the biological visual system. The CNN is composed of image processing layers that deduce and pass down information from one layer to the next. At each layer, information of different abstractions is deduced. Generally, in the earlier layers, simpler and more basic ideas will be deduced while later layers will use the gathered information from the previous layers to deduce more complex ideas. The following figure lays down an example CNN - in the figure, a boat image is passed from the left end layer to the next until it reaches the right-most layer, where it classifies the class of the image; in this case, it should be “Boat”.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig2.png&#34; alt=&#34;Layer structure&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Layer Structure of a CNN [6]
&lt;/p&gt;
&lt;p&gt;Due to the way the layers learn more complex features the deeper into the network, we can call this a hierarchical learning structure. To see clearly what we mean by the complexity of features, we can observe the following figure. The first group shows the detection of edges - which are simpler features compared to the textures in the second group. The CNN layers detect such “textures” through the combination of “edges” detected in the previous layer. Likewise, the following layers learn patterns through combining textures, and so on and so forth, until the last layers learn the presence of certain objects. In this way, the complexity of features increases as we go deeper down the network, demonstrating a hierarchical learning mechanism.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig3.png&#34; alt=&#34;Complexity of learned features&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Different complexity of learned features [1]
&lt;/p&gt;
&lt;p&gt;This hierarchical layer structure is actually also utilized in the visual cortex ventral pathway, which is a layerlike pathway consisting of the sequence LGN-V1-V2-V4-IT where each of them represents a certain information processing layer (Figure 4). As we proceed through the visual pathway,  the features learned become more complex, just as in the CNN. The receptive visual field size increases as well, as larger receptive field suggests a more holistic and general feature in the image. In a way, this makes sense - to recognize a baseball, the network has to learn stripes and some circular shapes. Likewise, for a dog, the features might be the dog snouts, black and white eyes, furry texture, etc. Such features cannot be instantly detected in just a single step, but rather gathered throughout different layers’ learning, which forms the basis of hierarchical learning.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig4.png&#34; alt=&#34;Visual Cortex Ventral Pathway&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Human Visual Cortex Ventral Pathway [3]
&lt;/p&gt;
&lt;p&gt;In addition to the concept of hierarchical information processing in CNNs, another fundamental concept called pooling is utilized. Pooling is basically the idea of generalizing or approximating a set of values in an area into a smaller set of values. This concept is explained in the following figure. The input image is a grid of 16 values and pooling is applied on the image to result in a grid of 4 values; each of the 4 values are the maximum values taken from their respective areas represented by the color. By taking the maximum, we reduce the size of the information we are looking at and select the most important values that need to be paid attention to.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig5.png&#34; alt=&#34;Pooling Mechanism&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Pooling Mechanism [8]
&lt;/p&gt;
&lt;p&gt;To understand at the higher level, the pooling is used to aggregate information gathered into summarized information. This idea of aggregation allows for the hierarchical information processing - the basic features learned are aggregated and then the details are gotten rid of to learn high level features. Pooling reduces the dimension of the representation and “creates an invariance to small shifts and distortions”. Basically this means that switching an image around by slight pixel changes will not affect the information being deduced from the image. Through pooling, we eliminate repeated learning of similar features that are right next to each other in the image feature representation. Interestingly, this idea behind the pooling layer is found in the relation between simple and complex cells in the biological neural system, where simple cells simply evoke a response on each of their particular spatial locations, while complex cells seem to be pooling over responses from the simple cells and thus showing more spatial invariance in their responses.&lt;/p&gt;
&lt;p&gt;No matter how neat CNN is in capturing visual information like the biological system, there are some outright flaws in it. One is the possibility of adversarial attacks, which involve hacking the CNN by slightly changing the pixel values of images in a way that is undetectable to a human eye but enough to fool the CNN to make faulty conclusions. An example is shown below, where the panda image is altered to be recognized as a gibbon by a CNN although there seems to be no difference to the human eye (Figure 6). This example shows how the CNN is perceiving ideas through meticulous attention to every single pixel in an image, which might not be the case with human visuals; for humans, perception likely happens through directly seeing the patterns and lines rather than individual pixels.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig6.png&#34; alt=&#34;Adversarial Attack on CNN&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Adversarial Attack on CNN (&lt;a href=&#34;https://openai.com/blog/adversarial-example-research/&#34;&gt;OpenAI&lt;/a&gt;)
&lt;/p&gt;
&lt;p&gt;On the other hand, this makes us wonder, “Can the human visual system be hacked as well? Are there ways to fool our eyes although maybe to another species there isn’t noticeable change?” It turns out that there are ways to fool our visual perception as well through small image change. Researchers have found a way to generate images that are designed to tip the perception towards a different idea although there isn’t much change in the image composition. Look at the example below - the left image looks like a cat, but when altered slightly to form the right image, it starts to look more like a dog. Such a hack is akin to the idea of subliminal stimuli - visual or auditory stimuli that the conscious mind cannot detect but that the brain subconsciously processes - maybe adversarial attacks are subliminal messages for the CNN.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;fig7.png&#34; alt=&#34;Adversarial Attack on Human Visual System&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Adversarial Attack on Human Visual System [9]
&lt;/p&gt;
&lt;p&gt;While it’s quite interesting to ponder such ideas and even question our sensual perception, the conclusion is that there are evidently parallels between the way CNN works and the way the human visual system works. However, there are also some fundamental differences between them - although these differences could possibly be reduced through more complex layers and architectural changes in the CNN design.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;[1] 
&lt;a href=&#34;https://distill.pub/2017/feature-visualization/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Feature Visualization - What are CNNs learning?&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2] 
&lt;a href=&#34;https://distill.pub/2018/building-blocks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interpretation with building blocks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3] 
&lt;a href=&#34;https://neurdiness.wordpress.com/2018/05/17/deep-convolutional-neural-networks-as-models-of-the-visual-system-qa/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Neural networks as models of the visual system&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4] 
&lt;a href=&#34;https://www.cs.toronto.edu/~hinton/absps/NatureDeepReview.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LeCun - Nature Deep Learning Review&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5] 
&lt;a href=&#34;https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How Conv layers work&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6] 
&lt;a href=&#34;https://www.vision-systems.com/boards-software/article/14037858/fundamentals-of-deep-neural-networks&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fundamentals of Deep Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[7] 
&lt;a href=&#34;http://fourier.eng.hmc.edu/e180/lectures/v1/node7.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Simple and Complex cells in the Human visual system&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8] 
&lt;a href=&#34;https://towardsdatascience.com/understanding-convolutions-and-pooling-in-neural-networks-a-simple-explanation-885a2d78f211&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Convolutions and Pooling in Neural Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9] 
&lt;a href=&#34;https://spectrum.ieee.org/the-human-os/artificial-intelligence/machine-learning/hacking-the-brain-with-adversarial-images&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hacking the Brain with Adversarial Images&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Intelligent Politics: How AI Can Improve Our Political Institutions and Systems</title>
      <link>https://MSAIL.github.io/talk/politics_ai_systems_040521/</link>
      <pubDate>Tue, 06 Apr 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/politics_ai_systems_040521/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://andongluis.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andong Luis Li Zhao&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Intelligent Politics: How AI Can Improve Our Political Institutions and Systems&lt;/p&gt;
&lt;p&gt;Andong Luis Li Zhao is a Computer Science PhD student at Northwestern University, working in the C3 Lab under Prof. Kristian Hammond. His main research focus is modernizing our political systems through AI. He is currently working on making political information more transparent by building systems that can understand vaguely-articulated questions, obtain the correct data analysis, and identify the most appropriate representation of that analysis.&lt;/p&gt;
&lt;p&gt;While his specific focus is currently on providing the public with access to information about our political system, this work is part of a broader goal of improving how society functions through socially-conscious AI grounded in real systems. Too often technologists abdicate their social responsibility by focusing on technical development. Instead, by developing human-centered AI technology that helps inform people and uncover novel insights, we can focus on the betterment of social, political, and economic systems and their impact.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1AvjuKSlNVVurQWtyFg6qUh1F8ZB6X9ft/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of his talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://scales-okn.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SCALES: Transforming the Accessibility and Transparency of Federal Courts&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://sites.northwestern.edu/c3lab/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;C3 Lab&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Harmful Bias in Natural Language Generation</title>
      <link>https://MSAIL.github.io/talk/harmful_bias_nlg/</link>
      <pubDate>Tue, 30 Mar 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/harmful_bias_nlg/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Yashmeet Gambhir&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Harmful Bias in Natural Language Generation&lt;/p&gt;
&lt;p&gt;Large language models have taken over the NLP scene and have led to a surge of state-of-art development in natural language generation tasks (machine translation, story generation, chatbots, etc.). However, these models have been shown to reflect many harmful societal biases that exist in text around the internet. This talk will go over two major papers studying harmful bias in large LMs: the first identifies and quantifies this bias, the second will attempt to mitigate bias.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1909.01326&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Woman worked as a Babysitter: On Biases in Language Generation&lt;/a&gt;

&lt;a href=&#34;https://www.aclweb.org/anthology/2020.findings-emnlp.291.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Towards Controllable Biases in Language Generation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>What could AI do for developing countries?</title>
      <link>https://MSAIL.github.io/post/developing_countries/</link>
      <pubDate>Sun, 14 Mar 2021 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/post/developing_countries/</guid>
      <description>&lt;p&gt;10% of the world’s population lives in extreme poverty. This means that around 700 million people are living on less than $1.90 a day (1). And while $1.90 is the standard set by the World Bank defining the international poverty line (2), by no means does it imply that a person earning more than this amount has anywhere near suitable standards of living. Poorer populations in countries such as South Sudan, Bolivia, and India (2) still struggle to access basic features that are usually taken for granted in more developed countries, such as clean water, sanitation, education, and healthcare. 2.1 billion people in the world do not have access to safe drinking water (3). Around 1.8 billion people do not have access to adequate sanitation (4). And a report from the World Health Organization and the World Bank found that 400 million do not have access to essential health services (5).&lt;/p&gt;
&lt;p&gt;Much work is already being undertaken in attempts to address these key issues, but more is still needed. Artificial intelligence (AI) is one such area where there is potential in alleviating some of the problems accompanying poverty. AI encompasses areas such as natural language processing and robotics, and has seen success in recent years with technology like autonomous vehicles, disease diagnosis, and recommendation algorithms. It is a powerful tool that also has the potential to be applied in ways beneficial to those dealing with the circumstances and living standards that typically go with poverty, especially in the realms of sanitation and education.&lt;/p&gt;
&lt;h2 id=&#34;sanitation&#34;&gt;Sanitation&lt;/h2&gt;
&lt;p&gt;Issues of sanitation for impoverished regions encompass topics such as lack of clean water, proper toilet systems, and waste management.&lt;/p&gt;
&lt;p&gt;One initiative attempting to address the problem of unsafe drinking water comes from an organization called Clean Water AI. Developers created a device that uses a “deep learning neural network to detect dangerous bacteria and harmful particles in water” (6). This technology employs computer vision to continuously monitor water quality and observe at a microscopic level. Widespread adoption of such an application could be beneficial for avoiding illnesses that arise from consumption of unsafe drinking water, such as cholera, typhoid, and polio (11).&lt;/p&gt;
&lt;p&gt;Here&amp;rsquo;s a short video detailing what Clean Water AI does:

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/Df1X1Km9riQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;Lack of proper waste management is another huge issue for underdeveloped regions. Without systems in place to take care of waste, it has become common for garbage to be thrown out in the surrounding areas or tossed into nearby water sources, where it sometimes becomes a playground for the neighboring children (7). While the establishment and utilization of governmental systems and resources could help manage this waste problem, there are also innovations in the AI sector that could be applied to target the problem. Recycling is an important process for these communities, and a robot designed by engineers from Simon Fraser University could help distinguish waste from recyclable materials (8). Growing company CleanRobotics is also taking steps to aid the waste management process with an autonomous system called TrashBot, which uses “robotics, computer vision and artificial intelligence to detect and separate landfill from recyclables” (12). Establishing automated systems for removing trash build-up could make the process easier in underdeveloped regions that lack the proper systems, and this could be crucial for lowering rates of disease caused by unhygienic environments. Such use of artificial intelligence could also potentially improve financial outlooks for the citizens in these areas through reuse and repurposing of recyclable materials.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;assembly_recycle.png&#34; alt=&#34;Recycling&#34;&gt;&lt;/p&gt;
&lt;center style=&#34;font-size:15px;&#34;&gt;
Credit: Arvinder Singh
&lt;/center&gt;
&lt;h2 id=&#34;education&#34;&gt;Education&lt;/h2&gt;
&lt;p&gt;The standard of education and the availability of educational resources also suffer in impoverished regions. Improved educational resources correlates positively with poverty statistics, as better education can benefit a region overall and positively impact its economy; “Education promotes economic growth because it provides skills that increase employment opportunities and income” (13)(14).  It is no surprise, then, that poorer regions have less access to proper education and fewer members of the community acquire an education. Lack of nearby schools, teachers, transportation, and time are all contributors to this problem.&lt;/p&gt;
&lt;p&gt;Universal education is a popular initiative and there is work being done with the AI field to progress this goal. The rise of the internet has provided opportunities for bringing education into the home with the growing format of online schooling. Having the capability to view pre-taught lessons online is an important development in tackling the aforementioned impediments to education in underdeveloped regions. While there already exist many online resources that provide information on a variety of subjects for anyone to browse, language barriers still persist. The Presentation Translator from Microsoft is one such tool involving AI that is attempting to address language barriers (15). It uses speech recognition and translation to create subtitles of presentations in the desired language, which users can access through the tool’s affiliated app or browser. Google’s translation application can also be used for reading, writing, and speaking translations. There are many other recent developments in the field of natural language processing that can help tackle language barriers, and further expansion of available languages in these applications would be advantageous for poorer regions with lesser-known tongues.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;translation.png&#34; alt=&#34;Learning&#34;&gt;&lt;/p&gt;
&lt;center style=&#34;font-size:15px;&#34;&gt;
Credit: Arvinder Singh
&lt;/center&gt;
&lt;h2 id=&#34;challenges-to-ai-access&#34;&gt;Challenges to AI Access&lt;/h2&gt;
&lt;p&gt;There is much more growth happening in the AI field than what has been discussed here, and these technological advancements may be useful for targeting the persisting problems of poverty. But a major obstacle of using these tools to benefit developing countries is that many of these regions do not have the necessary structures in place to even implement such technologies.&lt;/p&gt;
&lt;p&gt;If specialists are needed for the establishment and maintenance of these AI applications, it is unlikely there will be someone in these areas who can fulfill that role, especially with the lack of access to education. It could become even more costly to hire outside help for maintaining these applications than it already is just to instate them. It must also be determined who will fund these tools – if the creators of these applications are not providing them for free, who will the responsibility fall upon for payment? The impoverished region presumably cannot bear that burden. Governmental assistance is often what is needed in these situations, but these are developing areas and governmental instability is a common feature, as well as the fact that the developing economy often does not yet supply the financial resources needed to establish such systems. Lack of internet access is another deterrent to providing these AI tools. When the poorer regions of Asia and Africa are the ones more likely to have fewer people using the internet (16), and less than 5% of people are online in the poorest countries (10), this creates difficulty in making use of resources such as online education.&lt;/p&gt;
&lt;p&gt;Some initiatives seeking to resolve these obstacles are in the works, but they encounter their own obstacles in the sheer size of the issue and lack of financial resources (like Project Loon, which was attempting to provide worldwide internet access through essential parts of cell towers attached to balloons (10) but has since shut down in January of 2021, stating they “haven’t found a way to get the costs low enough to build a long-term, sustainable business” (17)). The barriers to technological access in underdeveloped areas are widespread and persistent, and it is difficult for one well-meaning company or organization to solve them. These significant concerns will need to be addressed if artificial intelligence is to play a role in reducing poverty in our world.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1]: 
&lt;a href=&#34;https://lifewater.org/blog/9-world-poverty-statistics-to-know-today/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;World Poverty Statistics via lifewater.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2]: 
&lt;a href=&#34;https://www.worldbank.org/en/topic/poverty#:~:text=Today%2C%20less%20than%2010%20percent,less%20than%20%241.90%20a%20day.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;World Bank&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3]: 
&lt;a href=&#34;https://ourworldindata.org/water-access&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Water Access Data&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4]: 
&lt;a href=&#34;https://www.cdc.gov/healthywater/global/wash_statistics.html#:~:text=An%20estimated%20790%20million%20people,to%20an%20improved%20water%20supply.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CDC Wash Statistics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5]: 
&lt;a href=&#34;https://www.who.int/mediacentre/news/releases/2015/uhc-report/en/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;New report shows that 400 million do not have access to essential health services via WHO&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6]: 
&lt;a href=&#34;https://cleanwaterai.com/#intro&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Clean Water AI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[7]: 
&lt;a href=&#34;https://www.pbs.org/newshour/world/in-worlds-poorest-slums-landfills-and-polluted-rivers-become-a-childs-playground&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;via PBS&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8]: 
&lt;a href=&#34;https://www.sfu.ca/sfunews/stories/2019/07/artificial-intelligence-robot-reduces-waste-contamination-at-sfu.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Artificial intelligence robot reduces waste contamination at SFU&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9]: 
&lt;a href=&#34;https://loon.com/technology/flight-systems/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Flight systems via Loon&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[10]: 
&lt;a href=&#34;https://ourworldindata.org/internet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Internet Access&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[11]: 
&lt;a href=&#34;https://www.who.int/news-room/fact-sheets/detail/drinking-water#:~:text=Contaminated%20water%20can%20transmit%20diseases,000%20diarrhoeal%20deaths%20each%20year&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Contaminated water can transmit diseases&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[12]: 
&lt;a href=&#34;https://cleanrobotics.com/trashbot/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TrashBot via CleanRobotics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[13]: 
&lt;a href=&#34;https://www.globalcitizen.org/en/content/poverty-education-satistics-facts/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Poverty Education Statistics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[14]: 
&lt;a href=&#34;https://www.un.org/sustainabledevelopment/blog/2017/06/millions-could-escape-poverty-by-finishing-secondary-education-says-un-cultural-agency/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Millions could escape poverty by finishing secondary education&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[15]: 
&lt;a href=&#34;https://www.microsoft.com/en-us/translator/APPS/PRESENTATION-TRANSLATOR/#:~:text=About%20Presentation%20Translator,-Presentation%20Translator%20subtitles&amp;amp;text=As%20you%20speak%2C%20Presentation%20Translator,deaf%20or%20hard%20of%20hearing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Microsoft Presentation Translator&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[16]: 
&lt;a href=&#34;https://www.pewresearch.org/global/2016/02/22/internet-access-growing-worldwide-but-remains-higher-in-advanced-economies/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Internet access growing worldwide but remains higher in advanced economies via Pew Research&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[17]: 
&lt;a href=&#34;https://medium.com/loon-for-all/loon-draft-c3fcebc11f3f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Loon for all&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Transformers for Vision</title>
      <link>https://MSAIL.github.io/talk/image-worth-16x16-words/</link>
      <pubDate>Tue, 09 Mar 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/image-worth-16x16-words/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Andrew Awad and Drake Svoboda&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Using Transformers for Computer Vision&lt;/p&gt;
&lt;p&gt;In recent years we&amp;rsquo;ve seen the rise of transformers in natural language processing research, burgeoning the field to incredible heights. However, these very same transformers were seldom applied to computer vision tasks until recently. Andrew and Drake discussed how transformers have been used in vision tasks in recent years in a presentation covering two papers. The first, An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale (via Google Brain), is the &amp;ldquo;Attention is All You Need&amp;rdquo; of vision. Namely, this paper covers how one can construct a vision architecture devoid of the commonly applied CNN and still achieve comparable or better performance results while possibly cutting down computing resources. The second paper, End-to-End Object Detection with Transformers (via FAIR), formalizes the object detection task in a unique way that affords the usage of transformers.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2010.11929.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AN IMAGE IS WORTH 16X16 WORDS: TRANSFORMERS FOR IMAGE RECOGNITION AT SCALE&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://arxiv.org/pdf/2005.12872.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;End-to-End Object Detection with Transformers&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Proving Theorems with Generative Language Models</title>
      <link>https://MSAIL.github.io/talk/generative_language_modeling/</link>
      <pubDate>Mon, 01 Mar 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/generative_language_modeling/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Ashwin Sreevatsa&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Generative Language Modeling for Automated Theorem Proving Presentation&lt;/p&gt;
&lt;p&gt;In the past decade, deep learning and artificial neural networks have been incredibly successful at a variety of tasks such as computer vision, translation, game playing, and robotics among others. However, there have been less examples of deep learning making progress with reasoning related tasks- such as automated theorem proving, the task of proving mathematical theorems using computer programs. This paper explores the use of transformer-based models to automated theorem proving and presents GPT-f, a deep learning-based automated prover and proof assistant.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2009.03393.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generative Language Modeling for Automated Theorem Proving Presentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Lightning Round -- Assorted AI Topics</title>
      <link>https://MSAIL.github.io/previous_material/lightning/</link>
      <pubDate>Sat, 27 Feb 2021 15:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/lightning/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Lightning Round &amp;ndash; Assorted AI Topics&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;Here, we talk about a wide range of topics in AI that haven&amp;rsquo;t received their own slide decks &amp;ndash; the list includes reinforcement learning, optimization, adversarial machine learning, meta learning, active learning, multi-agent systems, and more. We hope that showcasing the breadth of AI research inspires you to dig deeper on your own and find what interests you!&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/169IpCxkST0Fjp7LjQgpdfiDz-Ccs1K-v/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1uQzkFpr4LyslagkloUHs5lnCQ3wNmVLfuaa7UsbTaGA/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Ethics</title>
      <link>https://MSAIL.github.io/previous_material/ethics/</link>
      <pubDate>Fri, 26 Feb 2021 17:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/ethics/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Ethics in AI Research&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;We discuss the various ethical problems AI research presents, including well-known problems like bias and weaponized AI and less publicized problems like interpretability and environmental impact of large machine learning models. We also talk about some of the solutions that researchers are attempting to implement and what we can do to contribute.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1C-bWWrhh_hK6ZwNmLEYK95uLJ6eiCbi1/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1KUUqzdz-Te1oNS4AMnxxPqO_mFUpmkDNokr0As9rCHQ/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Cognitive Load Estimation</title>
      <link>https://MSAIL.github.io/talk/cognitive_load_estimation/</link>
      <pubDate>Tue, 23 Feb 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/cognitive_load_estimation/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Patrick Morgan&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Cognitive Load Estimation&lt;/p&gt;
&lt;p&gt;Cognitive load has been shown, over hundreds of studies, to be an important variable for understanding human performance. However, establishing practical, non-contact, automated methods for estimating cognitive loads under real-world conditions is an un-solved problem. In this paper, Fridman et. al. proposes two novel vison-based methods for cognitive-load estimation. These methods address a important and challenging problem that has huge implications and can be used to ensure safety in tasks ranging from driving cars to operating machinery.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://www.researchgate.net/profile/Lex-Fridman/publication/324658835_Cognitive_Load_Estimation_in_the_Wild/links/5bf0ba3092851c6b27c74bd1/Cognitive-Load-Estimation-in-the-Wild.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cognitive Load Estimation in the Wild&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Protecting the environment with AI</title>
      <link>https://MSAIL.github.io/post/ai_environment/</link>
      <pubDate>Mon, 22 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/post/ai_environment/</guid>
      <description>&lt;p&gt;Employing forest guards to protect endangered species. Organising volunteers for beach cleanups. Encouraging people to lead greener lifestyles. These are just some of the traditional methods that have been used to protect our environment. But, as Alan Turing once said, &amp;lsquo;At some stage,&amp;hellip; we should have to expect the machines to take control&amp;rsquo; [1] and this couldn’t be more true in these conservation efforts. Be it preserving wildlife, cleaning up the environment or reducing green-house gas emissions, AI has started playing an increasingly dominant role as it has become good at numerous tasks such as visually identifying and tracking poachers or designing the most energy efficient buildings.&lt;/p&gt;
&lt;p&gt;Google, for example, has used AI to increase the efficiency and significantly reduce the carbon footprint of its data centers. Data centers are massive energy consumers, being turned on 24/7 and requiring significant cooling. In 2016  it was reported that data centers used more electricity than Britain, producing roughly the same carbon footprint as the aviation industry [2]. A google search of the video &amp;ldquo;Despacito&amp;rdquo; activates six to eight of Google&amp;rsquo;s data centers [3]. In line with Google’s effort to stay carbon neutral (which they achieved in 2007 [3]), Google implemented a reinforcement learning algorithm in 2016 to determine what “cooling configurations (in its data centers) would reduce energy consumption” [5]. A reinforcement algorithm is a subset of machine learning in which the environment sends a ‘state’ and ‘reward’ to the ‘agent’ - the reinforcement learning algorithm - which in turn tries to maximise the reward [4]. The data centers using this reinforcement algorithm now consistently use 30% less energy than expected [6].&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;data-center.jpg&#34; alt=&#34;A Google Data Center in Iowa&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
A Google data center in Iowa (&lt;a href=&#34;https://www.technologyreview.com/2018/08/17/140987/google-just-gave-control-over-data-center-cooling-to-an-ai/&#34;&gt;MIT Technology Review&lt;/a&gt;)
&lt;/p&gt;
&lt;p&gt;Big companies are not the only ones that are taking advantage of AI to be more environmentally friendly. Small startups are using AI to tackle environmental problems as well. In a remote reserve in Ecuador, Topher White - founder of Rainforest connection (a startup that aims to combat deforestation) - climbs a tree using a small harness and installs a small box containing an old cell phone and solar cells [7]. These phones recoprd the natural rainforest sounds and use AI to detect logging noises, upon which a notification is sent to a ranger. He had started his firm after learning of the severity of deforestation: the number of trees has fallen by almost 50% since the beginning of human civilisation, and still over 15 billion trees are currently cut down every year [8]. Topher White&amp;rsquo;s first encounter with deforestation came ironically when he was on a trip to the rainforests of Borneo and was shocked to find illegal loggers just a hundred meters from the ranger station. Monitoring forests using AI to identify deforestation on satellite imagery often comes too late as satellite imagery requires some amount of deforestation before it becomes visible. However, the sound, he recalls, was deafening, which gave him the idea to mount these small phones on the treetops to identify logging. The very next day after the first installation, a logger was apprehended, and it is now preventing deforestation in 10 countries [7].&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;topher-white.jpg&#34; alt=&#34;Topher White&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Topher White (&lt;a href=&#34;https://www.fastcompany.com/90435386/this-network-of-microphones-listens-for-the-chainsaws-of-illegal-loggers-in-the-rainforest&#34;&gt;Rolex/Stefan Walter via FastCompany&lt;/a&gt;)
&lt;/p&gt;  
&lt;p&gt;In fact even industries have turned to using AI to reduce costs and also increase their sustainability. One example is in Norway, where AI is being used to increase the efficiency of salmon farms [9]. Telenor Research - the research wing of the Norwegian telecoms company Telenor that is conducting research on the use of AI in fish farms - has come up with a neural network that uses a video feed from underwater cameras to determine when the salmon have finished feeding. The salmon swim in schools which disperse when food is thrown, but when the salmon are full, there are tiny deviations in this behaviour which the neural network picks up with 80% accuracy. Identifying these cues to better feed the right amount of food is beneficial economically (40% of the cost of salmon farms comes from fish food), and also helps prevent low oxygen levels, algae blooms and high nitrate levels which are toxic to fish, among other problems [12].&lt;/p&gt;
&lt;p&gt;There are numerous other AI applications in this area: IBM has come up with a machine learning tool named AquaCloud to predict lice outbreaks in Salmon with 70% accuracy using a random forest algorithm [10]. The industries&amp;rsquo; increasingly intensive salmon and rainbow trout fishing encourages lice growth, which in turn makes them unsellable because lice feed on salmon skin [9].&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;graph.jpg&#34; alt=&#34;AquaCloud lice predictions&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
The results of AquaCloud: the green line shows the lice outbreak predicted two weeks in advance. (&lt;a href=&#34;https://www.ibm.com/blogs/cloud-computing/2018/09/17/data-science-norway-fish-farmers/&#34;&gt;NCE Seafood Innovation Center&lt;/a&gt;)
&lt;/p&gt;
&lt;p&gt;The use of AI on fish farms is especially beneficial as fish farms are sustainable and help to reduce the dependency on precious ocean fish populations. Wild salmon are being depleted at an alarming rate, yet Norway’s salmon exports are only rising. Even here, AI is also being used to track the wild salmon population in efforts to sustain its population [9].&lt;/p&gt;
&lt;p&gt;These examples are but a small subset of AI-related projects that have been done. A brief search of new competitions regarding environmental solutions (most of which use AI, even if the competition implies only a possible technological solution), confirms the increasing interest in sustainability. The Xprize - a nonprofit that runs public competitions -  is currently running a $10 million prize rainforest competition to “develop novel technologies to rapidly and comprehensively survey rainforest biodiversity and use that data to improve our understanding of this complex ecosystem” [11]. Microsoft’s AI for Good grants are given to projects that enhance climate, water, and biodiversity. Prince William recently launched the Earthshot prize - a series of annual one million pound prize competitions in five categories such as reviving the oceans, cleaning the air, and protecting nature. There are countless other competitions and the examples discussed in this article are just a taste of the AI environmental projects to come.&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References&lt;/h3&gt;
&lt;p&gt;[1]: 
&lt;a href=&#34;https://analyticsindiamag.com/ten-famous-quotes-about-artificial-intelligence/#:~:text=%E2%80%9CArtificial%20intelligence%20will%20reach%20human,%E2%80%93%20a%20billion%2Dfold.%E2%80%9D&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ten Famous Quotes about Artificial Intelligence&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2]:

&lt;a href=&#34;https://www.computerworld.com/article/3431148/why-data-centres-are-the-new-frontier-in-the-fight-against-climate-change.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Why Data Centres are the New Frontier in the Fight Against Climate Change&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3]:
&lt;a href=&#34;https://fortune.com/2019/09/18/internet-cloud-server-data-center-energy-consumption-renewable-coal/#:~:text=Data%20centers%20contribute%200.3%25%20to,one%20for%20every%20100%20Americans&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Internet Cloud has a Dirty Secret&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4]:

&lt;a href=&#34;https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Various Reinforcement Learning Algorithms&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5]:

&lt;a href=&#34;https://www.technologyreview.com/2018/08/17/140987/google-just-gave-control-over-data-center-cooling-to-an-ai/#:~:text=Google%20revealed%20today%20that%20it,order%20to%20lower%20power%20consumption&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google Just Gave Control Over Data Center Cooling to an AI&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6]: 
&lt;a href=&#34;https://services.google.com/fh/files/misc/google_2019-environmental-report.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Google 2019 Environmental Report&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[7]: 
&lt;a href=&#34;https://www.fastcompany.com/90435386/this-network-of-microphones-listens-for-the-chainsaws-of-illegal-loggers-in-the-rainforest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;This Network of Microphones Listens for the Chainsaws of Illegal Loggers in the Rainforest&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[8]:

&lt;a href=&#34;https://www.nature.com/articles/nature14967.epdf?sharing_token=662fd6kwyoW2pWYP7gTtqdRgN0jAjWel9jnR3ZoTv0PVqBqhRh-xvvJTzHFUU9TMfiseqc7XBtw3yJeJCBwHMEd325JNCYv-3DvRroRPJJJkTX95golfBeN0XF1aaW8P59jvn5Sk0G_AU4O4V7AYAm5mOs3rWaxWrsHFRlurXF8zdFAS7FkVY9jYIW5ojMN-nvvGu0eucnWzPgZF2qvs0ca--T3nUj6njgtSI9LJcUU%3D&amp;amp;tracking_referrer=www.nationalgeographic.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Crowther et al., Mapping tree density at a global scale&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[9]: 
&lt;a href=&#34;https://www.wsj.com/articles/ai-could-help-find-cheaper-and-smarter-ways-to-raise-fish-11601458200?mod=djemfoe&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AI Could Help Find Cheaper and Smarter Ways to Raise Fish&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[10]: 
&lt;a href=&#34;https://www.ibm.com/case-studies/the-seafood-innovation-cluster-hybrid-cloud-fish-farming&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Seafood Innovation Cluster&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[11]: 
&lt;a href=&#34;https://rainforest.xprize.org/prizes/rainforest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Rainforest XPrize&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[12]: 
&lt;a href=&#34;https://www.petcoach.co/article/why-overfeeding-fish-is-a-problem-and-how-to-avoid-it/#:~:text=Accumulated%20uneaten%20food%20and%20fish,changes%20in%20the%20water%20chemistry.&amp;amp;text=High%20ammonia%20and%20nitrites%20%2D%20The,are%20extremely%20toxic%20to%20fish&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Why overfeeding fish is a problem and how to avoid it&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Unsupervised Learning</title>
      <link>https://MSAIL.github.io/previous_material/unsupervised/</link>
      <pubDate>Sat, 20 Feb 2021 15:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/unsupervised/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Unsupervised Learning&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;This lesson went over the unsupervised side of AI, where labels don&amp;rsquo;t exist and models are left on their own to learn useful information. We presented machine learning approaches with and without deep learning that tackle unsupervised problems.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1WXBrrbNDryufUYkzQS1aLwrEcJRbo-xS/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1H77BDYebNusyelevFe5-AHZzYCaOB1tid-Vqtmm13oI/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Natural Language Processing</title>
      <link>https://MSAIL.github.io/previous_material/nlp/</link>
      <pubDate>Fri, 19 Feb 2021 17:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/nlp/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Natural Language Processing&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;This lesson gave a high level overview of NLP (natural language processing) and how AI can be used to work with text and speech data. Points of discussion included recurrent neural networks, LSTMs/GRUs, and GPT-3 and other transformer models.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1DjwaY3p7vb4N4V7DwvZEBQB2qxjs5okS/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/178FNnk3x8euXO3NHBqQT9VT6-d9FigUVOt56Ru-Lvpo/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep RL for Robotics: A Short Overview</title>
      <link>https://MSAIL.github.io/talk/deeprlrobotics_021621/</link>
      <pubDate>Tue, 16 Feb 2021 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/deeprlrobotics_021621/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Nikhil Devraj&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: A Brief Overview of Deep RL in Robotics&lt;/p&gt;
&lt;p&gt;Deep reinforcement learning (RL) has emerged as a promising approach for autonomously acquiring complex behaviors from low level sensor observations. Although a large portion of deep RL research has focused on applications in video games and simulated control, which does not connect with the constraints of learning in real environments, deep RL has also demonstrated promise in enabling physical robots to learn complex skills in the real world.&lt;br&gt;
This discussion focused predominantly on the following questions: &lt;br&gt;
(1) What is deep RL and how does it relate to robotics?&lt;br&gt;
(2) What are some examples of studies done with Deep RL in robotics?&lt;br&gt;
(3) What are major challenges faced by researchers who apply deep RL to robotics?&lt;/p&gt;
&lt;p&gt;This discussion is heavily inspired by 
&lt;a href=&#34;https://arxiv.org/pdf/2102.02915.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ibarz et al.&lt;/a&gt;, although it does not dive into that level of detail.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1gshp58d3yce4LhcLpTymch8IA2cbm4ZO/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find the recording of this talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2102.02915.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How to Train Your Robot with Deep Reinforcement Learning - Lessons We&amp;rsquo;ve Learned&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1610.00633&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://journals.sagepub.com/doi/abs/10.1177/0278364913495721&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reinforcement learning in robotics: a survey&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Articles:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://medium.com/@vmayoral/reinforcement-learning-in-robotics-d2609702f71b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium: Reinforcement learning in robotics&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://ieor8100.github.io/rl/docs/RL%20in%20Robotics.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Better slides (in our presenter&amp;rsquo;s opinion)&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=GX_RonOFe1U&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep RL Towards Robotics by Shane Gu (Google Brain)&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=luzOblzznIc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep RL in Robotics with NVIDIA Jetson&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Computer Vision</title>
      <link>https://MSAIL.github.io/previous_material/computer_vision/</link>
      <pubDate>Sat, 13 Feb 2021 15:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/computer_vision/</guid>
      <description>&lt;p&gt;​
&lt;strong&gt;Topic&lt;/strong&gt;: An Overview of Computer Vision&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang​&lt;/p&gt;
&lt;p&gt;This lesson gave a basic overview of the computer vision problem space. We discussed historically significant developments including convolutional neural networks, AlexNet, ResNet, and more, and we gave a glimpse at ongoing research.​&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/15WxV2hC40Bz4YhcyPVeYqb1gvIViq2Ka/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1MaC9d25kJybNv_pOYQHFv9oNOM1J-65zMkQX2PMlCqg/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introduction and Basics of Deep Learning</title>
      <link>https://MSAIL.github.io/previous_material/intro_dl_basics/</link>
      <pubDate>Fri, 12 Feb 2021 17:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/intro_dl_basics/</guid>
      <description>&lt;p&gt;​
&lt;strong&gt;Topic&lt;/strong&gt;: Introduction to AI Research and Basics of Deep Learning&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang  ​&lt;/p&gt;
&lt;p&gt;This lesson introduced the format of lessons for the winter 2021 semester, briefly introducing the topics to be presented in the coming weeks. We then gave a high-level overview of neural networks, which form the basis of deep learning and drive much of AI research today. ​&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1lNhpuuxNhW5nHDLavDLqlLOKv-DYfMhO/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;​&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1SkI0i1Y_Dp1lZTCJjJD91f0DVf_CXfB4FMBy8jLweeg/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Presenting in a Reading Group</title>
      <link>https://MSAIL.github.io/post/presenting_reading_group/</link>
      <pubDate>Sun, 17 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/post/presenting_reading_group/</guid>
      <description>&lt;p&gt;If you&amp;rsquo;re involved in research, you&amp;rsquo;re probably going to give a reading group presentation at some point. Many professors push their PhD students to give talks. Giving these talks helps researchers build the ability to read and understand papers quickly, and the ability to communicate findings effectively.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Volunteer or be volunTOLD.&lt;/p&gt;
&lt;p&gt;— &lt;cite&gt;Prof. David Fouhey&lt;/cite&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Dr. Fouhey told this joke multiple times during the 
&lt;a href=&#34;https://sites.google.com/umich.edu/cv-reading-group/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;computer vision reading group&lt;/a&gt; last semester and the other professors agreed. It succinctly summarizes the emphasis placed on giving these talks.&lt;/p&gt;
&lt;h2 id=&#34;whats-a-reading-group&#34;&gt;What&amp;rsquo;s a reading group?&lt;/h2&gt;
&lt;p&gt;Reading groups regularly meet to discuss topics in research. Most of the time, the group will focus on one specific paper detailing an important finding. In AI, many of these reading groups may be focused on award-winning papers from recent conferences or on methods relevant to the participants&amp;rsquo; research.&lt;/p&gt;
&lt;p&gt;Reading groups exist mainly to enrich the participants&amp;rsquo; knowledge. Sometimes the talks will focus on a broader topic as an introduction and sometimes the talks will focus on a specific method in a specific paper. The more niche the audience of the reading group, the more advanced the topic tends to be. At MSAIL, we try to strike a balance between our younger, less-experienced audience (i.e. underclassmen) and our older, experienced audience (upperclassmen, graduate students, etc.).&lt;/p&gt;
&lt;h2 id=&#34;choosing-a-topic&#34;&gt;Choosing a topic&lt;/h2&gt;
&lt;p&gt;Choosing a topic may be the hardest part of the presentation process. Generally, you can present any topic you want, given that it hasn&amp;rsquo;t already been presented recently. Present on something that grabs your interest immediately, or something you have some familiarity with - it&amp;rsquo;ll make the preparation process more bearable. If you&amp;rsquo;re open to topics and are confident you can adapt, then just go to a conference or journal page and search through some of the accepted papers that catch your eye (at the time of writing, I&amp;rsquo;ve been looking at ICLR 2021 papers).&lt;/p&gt;
&lt;p&gt;In my opinion, the main question you should ask yourself when you&amp;rsquo;ve identified a potential topic is:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Am I willing to read about this topic in depth, even to the extent of falling into a rabbit hole?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;You obviously don&amp;rsquo;t need to know everything about the topic you choose (no one does), but persistence is the key to having a strong presentation. The more comfy you are with the overall subject area, the more natural your presentation will flow and the less likely you are to trip up. (For MSAIL, however, if you&amp;rsquo;re a newcomer and haven&amp;rsquo;t really done a reading group presentation before, we&amp;rsquo;ll help you out!)&lt;/p&gt;
&lt;p&gt;Here are some questions you should ask yourself when looking at a paper or topic that you&amp;rsquo;re about to choose:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Why is the topic important? What do I hope to get out of it?
&lt;ul&gt;
&lt;li&gt;If it&amp;rsquo;s not clearly important or you don&amp;rsquo;t gain anything from knowledge of the topic itself, you&amp;rsquo;ll just be wasting time.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Is this interesting to the naked eye?
&lt;ul&gt;
&lt;li&gt;Important for getting people to attend your talk, and also helpful in gauging whether your audience will stay engaged. If they aren&amp;rsquo;t engaged in the beginning how can they be expected to in the end?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Can I learn about this topic within a reasonable amount of time?
&lt;ul&gt;
&lt;li&gt;A &amp;ldquo;reasonable amount of time&amp;rdquo; is generally a week or so.&lt;/li&gt;
&lt;li&gt;You need to choose papers of reasonable length. We often suggest presenting on conference papers because they&amp;rsquo;re less than 10 pages on average. Longer papers and topics are more feasible down the line when you&amp;rsquo;ve become comfortable with these types of presentations.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the examples in this post, I will go through the process of choosing a topic for one of my previous talks. I&amp;rsquo;ve given plenty of talks on uninteresting topics and papers, but some were received particularly well. I will talk about my process for presenting 
&lt;a href=&#34;https://arxiv.org/pdf/1904.01766.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VideoBERT&lt;/a&gt;, which I presented way back in Fall 2019. This was actually my first ever MSAIL talk, and at the time I had only recently become acquainted with AI research. The talk had plenty of faults, which I&amp;rsquo;ll try to use as examples.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;videobert_flow.png&#34; alt=&#34;VideoBERT Flow Diagram&#34;&gt;&lt;/p&gt;
&lt;center style=&#34;font-size:15px;&#34;&gt;
The central flow diagram from the VideoBERT paper by Sun et al.
&lt;/center&gt;
&lt;h2 id=&#34;reading-relevant-sources&#34;&gt;Reading relevant sources&lt;/h2&gt;
&lt;h3 id=&#34;for-a-specific-paper&#34;&gt;For a specific paper&lt;/h3&gt;
&lt;p&gt;Even if you choose one paper, that paper is probably not the only source you&amp;rsquo;re looking at to understand all the content.
When you first read through the paper itself, you should annotate the key points (this is just a common reading skill, but it&amp;rsquo;s easy to forget!) and note the portions that confuse you. Depending on your background, you may or may not be able to finish the first pass. You should aim to have a big picture understanding of the paper, so maybe about 30%. If you can&amp;rsquo;t reach that level on your first read - don&amp;rsquo;t fret. You need to go read some supplementary materials. In particular, any decent paper will reference prior/related work in a section near the introduction - this is where you can dive into their citations and read up on the things that confuse you. Alternatively, I&amp;rsquo;ve found that Medium posts are particularly helpful as well for understanding more basic content.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;related-work.png&#34; alt=&#34;Related Work&#34;&gt;&lt;/p&gt;
&lt;center style=&#34;font-size:15px;&#34;&gt;
Related Work section in VideoBERT paper. 
&lt;/center&gt;
&lt;br&gt;
&lt;p&gt;Note the underlined portions here from the Related Work section of the VideoBERT paper. These highlight topics that might be worth searching up. You don&amp;rsquo;t need to dive into everything, but having a general understanding of what cross-modal learning and BERT are would help to better understand this paper.&lt;/p&gt;
&lt;p&gt;If after all that, you still can&amp;rsquo;t understand 30% of the material in the paper, then I&amp;rsquo;m afraid you probably need to read further on basic material and possibly postpone your talk. I don&amp;rsquo;t expect this to happen because the pool of people who choose to present is self-selecting (as in, you&amp;rsquo;re more likely to want to present in a reading group if you already have basic background), but just in case, don&amp;rsquo;t be afraid to start at the beginning. I too have had to withdraw after signing up for a reading group before because I just did not understand what I was reading at all.&lt;/p&gt;
&lt;p&gt;After the first pass, you have an idea of what the paper&amp;rsquo;s central ideas are. You can then start outlining what you want to talk about. Any subsequent passes will simply be to reinforce your understanding of the paper.&lt;/p&gt;
&lt;h3 id=&#34;for-a-broader-topic&#34;&gt;For a broader topic&lt;/h3&gt;
&lt;p&gt;For a broader topic, you should still choose to focus on a few papers in order to narrow the scope of your presentation. If you choose this, you likely have an idea in mind for how you wish to synthesize the ideas in the papers. Knowing this, you should focus your reading based on which points you hope to elucidate most. The process will very much feel like the process in the above section, except you&amp;rsquo;ll spend less time focusing on the intricate details of any one paper and you&amp;rsquo;ll focus more on the key ideas that you can use to build toward whatever main idea you&amp;rsquo;re focusing on.&lt;/p&gt;
&lt;p&gt;In general, giving these types of talks is difficult. Even professors struggle to present so much content in a clear way. If you intend to give a talk like this, make sure to spend extra time in advance to really nail a cohesive argument. Otherwise, just stick to one paper since usually the time you have is only enough for one.&lt;/p&gt;
&lt;p&gt;An example of a decent talk that synthesizes ideas in multiple papers is 
&lt;a href=&#34;https://web.eecs.umich.edu/~justincj/slides/eecs498/FA2020/598_FA2020_lecture15.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Justin Johnson&amp;rsquo;s lecture on Object Segmentation&lt;/a&gt;. This is obviously not a reading group talk and is an entire course lecture - but the principles are relatively similar since the topics presented here are from recent papers. Another good example is the talk 
&lt;a href=&#34;https://MSAIL.github.io/talk/chai_120120/&#34;&gt;Dr. Chai gave us in Fall 2020&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Some of our own, more tame talks presenting multiple papers include 
&lt;a href=&#34;https://MSAIL.github.io/talk/brain_insp_110320/&#34;&gt;John Day&amp;rsquo;s Brain-Inspired AI talk&lt;/a&gt;, 
&lt;a href=&#34;https://MSAIL.github.io/talk/textsummarization_111020/&#34;&gt;Yash Gambhir&amp;rsquo;s Text Summarization talk&lt;/a&gt;, and 
&lt;a href=&#34;https://MSAIL.github.io/talk/rlcovid_101320/&#34;&gt;my talk on using reinforcement learning for optimization in COVID-19 problems&lt;/a&gt;. If you watch them you&amp;rsquo;ll notice some of the difficulties we had with balancing our content and finishing in time.&lt;/p&gt;
&lt;h2 id=&#34;creating-slides&#34;&gt;Creating slides&lt;/h2&gt;
&lt;p&gt;Most of the time you&amp;rsquo;ll be preparing slides to assist you in your talk. Organizing your slides properly is the key to getting a good presentation going.&lt;/p&gt;
&lt;p&gt;Something that helps me is using a general slide outline and then identifying where in the paper I can get the information for a specific section. Then I fill in the sections and occasionally add subsections based on the subtitles in the paper.&lt;/p&gt;
&lt;p&gt;In general, you want to introduce the following points in any regular paper presentation. You can change the order to suit your preferred flow, but the one presented here works well normally. Note that you can use any number of slides for each section:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Motivation
&lt;ul&gt;
&lt;li&gt;Why did the authors explore this topic? Who and how does it help solve a big problem?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Major Contributions
&lt;ul&gt;
&lt;li&gt;What are the authors proposing or introducing?&lt;/li&gt;
&lt;li&gt;Make this clear at the beginning. Then your audience will know what to expect.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Background
&lt;ul&gt;
&lt;li&gt;What does your audience need to know (at a high level) before you dive into the details of the topic?&lt;/li&gt;
&lt;li&gt;This is not always necessary, but if you&amp;rsquo;re presenting something technically challenging you may want to briefly introduce this.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Method/Theory
&lt;ul&gt;
&lt;li&gt;This is the novel part of the paper. What did the authors do and how did they do it?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Results/Experiments
&lt;ul&gt;
&lt;li&gt;How did they validate their methods and what did they compare it to? What are the deliverables?&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Discussion/Takeaways/Future Work
&lt;ul&gt;
&lt;li&gt;Restate the major contributions. Also, talk about the implications for the future.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;general-principles&#34;&gt;General Principles&lt;/h3&gt;
&lt;p&gt;You&amp;rsquo;ve probably presented to someone before. In that case, you should be well aware of standard principles, but I&amp;rsquo;ll write some in case you aren&amp;rsquo;t:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Low amount of text on any one slide
&lt;ul&gt;
&lt;li&gt;This is a technical talk. Please don&amp;rsquo;t make your readers lose you.&lt;/li&gt;
&lt;li&gt;Personally, I tend to put around 2 lines of text on a slide and then explain the rest verbally. Putting less text and explaining it instead helps me better understand the content too!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Tables, images, diagrams, and videos wherever possible
&lt;ul&gt;
&lt;li&gt;I don&amp;rsquo;t need to tell you that a picture is worth a thousand words, but they&amp;rsquo;ll help a ton. You can usually just steal these from the paper and its supplementary materials. If they don&amp;rsquo;t have any and you feel that one would be appropriate, don&amp;rsquo;t be afraid to create one!&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Avoid equations when you can
&lt;ul&gt;
&lt;li&gt;Sometimes the talk is devoted to an equation or the theory you&amp;rsquo;re discussing is heavily reliant on equations (I can&amp;rsquo;t imagine some reinforcement learning papers without Bellman&amp;rsquo;s equation.). But if the paper has a lot of equations, try only to include the most important ones.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Take a look at my 
&lt;a href=&#34;https://docs.google.com/presentation/d/1ObLuNm1A8ZWVInmuYeAd1NPIYrYsm3jLb7OPvSoC6kk/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VideoBERT slides&lt;/a&gt; and note that I absolutely did not follow these principles and the above listed structure during that talk. I consider my VideoBERT talk to be of poor quality. Don&amp;rsquo;t worry about the technical content. (Note that this link is Michigan only)&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://MSAIL.github.io/slides/videobert/&#34;&gt;Here are a few sample slides depicting how I would&amp;rsquo;ve roughly modified my VideoBERT talk to be easier to follow and listen to.&lt;/a&gt; I only wrote up to the methods section, because I just wanted to depict some of the principles in action. Again, don&amp;rsquo;t worry about the technical content. (This link is open to everyone)&lt;/p&gt;
&lt;p&gt;Also, feel free to take a look at 
&lt;a href=&#34;https://www.princeton.edu/~archss/webpdfs08/BaharMartonosi.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this slidedeck for general tips&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;presenting-your-slides&#34;&gt;Presenting your slides&lt;/h2&gt;
&lt;p&gt;Presentation is very important for a technical talk. I&amp;rsquo;m pretty sure most presenters don&amp;rsquo;t want to bore their audience. During one reading group a while ago, I delivered a one hour talk that included even professors in the audience. After that talk I didn&amp;rsquo;t receive a single question. I can only speculate whether they got lost, whether we were out of time, or whether I just completely bored them. Let&amp;rsquo;s hope that doesn&amp;rsquo;t happen to you.&lt;/p&gt;
&lt;p&gt;Here are some steps you can take to reduce the chance of losing your audience:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Reiterating the importance of preparing your slides properly. Prepare them as if you were presenting them, and then practice presenting them at least once before your talk.&lt;/li&gt;
&lt;li&gt;This is a given - you should be speaking and never reading.&lt;/li&gt;
&lt;li&gt;Don&amp;rsquo;t go on diversions. Save them till the end.&lt;/li&gt;
&lt;li&gt;Leave room for questions during your presentation. I doubt most people will remember their questions by the end. A good rule might be to ask for questions every 5 minutes.&lt;/li&gt;
&lt;li&gt;Similarly, you should be gauging understanding as you go along. If the audience can attest to understand what you&amp;rsquo;re saying, you&amp;rsquo;re fine.&lt;/li&gt;
&lt;li&gt;Speak slowly. I don&amp;rsquo;t know about you, but I&amp;rsquo;d rather have my entire audience understand 80% of my presentation and not finish within time than finishing and not having anyone understand anything. I sometimes break this rule without realizing.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are probably many more principles to follow, but in reading groups these are the ones I&amp;rsquo;ve found to be the most blatant errors that I wish I corrected.&lt;/p&gt;
&lt;h2 id=&#34;can-i-forgo-preparing-slides&#34;&gt;Can I forgo preparing slides?&lt;/h2&gt;
&lt;p&gt;If you don&amp;rsquo;t want to prepare slides, you can either:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Walk through the paper itself&lt;/li&gt;
&lt;li&gt;Prepare questions and facilitate a discussion rather than giving a talk&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I wouldn&amp;rsquo;t advise a newcomer (or anyone, for that matter) to choose the first option. The point of preparing slides is to make material more presentable and to help you, the presenter, understand the paper better. I&amp;rsquo;ve only experienced people presenting straight from the paper when they knew what they were talking about but had last minute obligations come up. For reference, the last two times I saw this done were from a student who &lt;em&gt;wrote&lt;/em&gt; the paper he was presenting on, and from a senior research scientist at Google Brain. It is generally okay, however, to supplement your slides during your talk by briefly visiting the paper to discuss something like a figure or a table, or to answer questions.&lt;/p&gt;
&lt;p&gt;The second option is far more feasible, and at MSAIL we actually recommend this format. Discussion questions help the audience engage with the material more. However, good discussions usually occur around people with background, so be wary of your audience. You&amp;rsquo;ll usually be presenting &lt;em&gt;something&lt;/em&gt; in addition to the questions - for example, last Winter we had a discussion about using vision to analyze CT scans for the purpose of detecting COVID-19. All we did was play a video prepared by another organization and then discussed it in detail. This is perfectly fine, given that you have an interesting topic.&lt;/p&gt;
&lt;h2 id=&#34;going-forward&#34;&gt;Going forward&lt;/h2&gt;
&lt;p&gt;Yeah, preparing to present at a reading group is a lot of work the first time around. After a while, you&amp;rsquo;ll be comfortable enough with both approaching novel technical content and with your presentation skills, so you&amp;rsquo;ll be able to take shortcuts and structure things as you wish. You&amp;rsquo;ll also just become faster. In the long term, this skill will certainly help you as a researcher.&lt;/p&gt;
&lt;p&gt;Gone are the days when the MSAIL Admin team was scrambling to prepare entire talks within 5 hours on the day of (we were quite notorious for this during the &amp;lsquo;19-&amp;lsquo;20 school year). This happened because we had very few speakers, but we&amp;rsquo;re much better off now. I hope you never prepare a talk within such a constraint because I can guarantee that the talk will fail miserably. The further along you go as a researcher, the later you&amp;rsquo;ll be able to start preparing reading group presentations, but you&amp;rsquo;ll still wish you started earlier.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re ready to try your hand at a talk, sign up with your reading group(s). For University of Michigan students, here are some reading groups you might be interested in:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Group Name&lt;/th&gt;
&lt;th&gt;Page&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;MSAIL Reading Group&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://msail.github.io/join/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://msail.github.io/join/&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Computer Vision Reading Group&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://sites.google.com/umich.edu/cv-reading-group/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sites.google.com/umich.edu/cv-reading-group/home&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Natural Language Processing Reading Group&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://lit.eecs.umich.edu/reading_group.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://lit.eecs.umich.edu/reading_group.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Reach out to us at 
&lt;a href=&#34;mailto:msail-admin@umich.edu&#34;&gt;msail-admin@umich.edu&lt;/a&gt; if you&amp;rsquo;re interested in giving a talk at MSAIL or for help with preparing a talk. Happy presenting!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conserving Labeling Resources and Mitigating Bias from Skewed Datasets</title>
      <link>https://MSAIL.github.io/post/conserving_label_musicer/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/post/conserving_label_musicer/</guid>
      <description>&lt;p&gt;     Say you are a researcher in autonomous vehicles, and as part of a project for a funder, you want to make an algorithm to automatically tell if a frame of video has a sidewalk or not. So you get a dataset of driver-view video that you recently collected, hand it off to your labelers, and a month or so later train the model on the labeled data. You get about 90% accuracy on both your train and test sets, and so you push the code to your team’s GitHub, tell your team you have a working algorithm, and celebrate. When the team uses it, however, they make an unpleasant discovery: the algorithm does not detect sidewalks at all! Your troubleshooting brings you to that labeled dataset, and you hang your head in your hands: 90% of the frames in the dataset don’t have sidewalks in them! Instead of learning meaningful decision-making rules, your model has learned to predict that all images do not contain sidewalks.&lt;/p&gt;
&lt;p&gt;     Or let’s say you’re working at a security company, and your team is tasked with creating a facial recognition algorithm. You spend a few months labeling a dataset of human faces, where the distribution of ethnicities in the dataset matches the distribution of ethnicities in the United States. According to the US Census from 2010, the dataset will be roughly 75% “white alone” faces and 13% “black or African American” faces (as seen in [1], Tab. P3). Then you tuck the dataset away into some folder somewhere and move on to designing the algorithm. When it comes to test time, your algorithm does significantly worse at recognizing dark-skinned faces. This reflects findings from a paper published in 2018 by Buolamwini and Gebru [2], where three commercial facial recognition algorithms trained on two benchmark datasets consisting of roughly 80% / 85% “white” faces had an astonishing 35% error rate on dark-skinned female faces. The consequences of an algorithm biased like this are not theory; less than an hour away from Ann Arbor, a dark-skinned man in Farmington Hills was wrongfully arrested based on a mistake made by a facial recognition algorithm, forcing him to spend thousands of dollars on an attorney to defend himself for a crime he did not commit [3].&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;obama-white.png&#34; alt=&#34;Obama regeneration&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Obama being upsampled into a light-skinned man (&lt;a href=&#34;https://www.theverge.com/21298762/face-depixelizer-ai-machine-learning-tool-pulse-stylegan-obama-bias&#34;&gt;The Verge&lt;/a&gt;)
&lt;/p&gt;  
&lt;p&gt;     In either case, you have a critical failure, and your project manager firmly tells you that the product needs to ship in three weeks. One possible solution is to add more data to “balance out” the different image classes: have your labelers find and label more images with sidewalks, or have them find and label more images of people of color. But labeling the original dataset took months, and with the degree of imbalance that you have, it would take a similar amount of resources to successfully address that. And you only have three weeks! Even if you had more time, how do you tell your boss that you now need the team to spend more resources on more labeling work? If only there was a way that you could have balanced the dataset earlier!&lt;/p&gt;
&lt;p&gt;     Both of these are motivating examples for the problem I am working on in my lab, the Data Elements from Video using Impartial Algorithm Tools for Extraction lab at the University of Michigan Transportation Research Institute (UMTRI-DEVIATE). This is a subproblem we are trying to solve as part of our broader objective to create tools for the Federal Highway Administration (FHWA) to assist labelers with video data. Specifically in this subproblem, we want to save FHWA labelers time with future research tasks, and we want the models they train to avoid bias. We call this problem the “human sampling problem”.&lt;/p&gt;
&lt;p&gt;     Labeling lots of data is costly. Hiring labelers requires money, and those labelers need time to accurately label a dataset. When it comes to a non-trivial image / video labeling task (for example, drawing bounding boxes around cell phones in videos of someone driving a vehicle), you also need to create a rubric so that all of your labelers are on the same page.&lt;/p&gt;
&lt;p&gt;     In addition, you may not want to use all that data. If a large dataset is skewed, an algorithm trained on that dataset will in turn be skewed towards making a particular decision. This is the root of both of the failures in the examples presented above – a sidewalk detector trained on skewed data cannot recognize sidewalks, and a facial recognition algorithm trained on skewed data struggles to identify the faces of dark-skinned women.&lt;/p&gt;
&lt;p&gt;     Considering both of these problems, one possible remedy would be to sample a subset of your original dataset before handing it over to the labelers such that the subset we select is as informative as possible. We hypothesize that a subset with maximum information will not be skewed, i.e. we think a dataset of 10 cats and 90 dogs will probably have less information than a dataset with 50 cats and 50 dogs. The way we have framed this is that we want to maximize &lt;em&gt;gain in information&lt;/em&gt; per &lt;em&gt;unit of cost in terms of labeler time&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;     In short, we want to reduce labeling time as much as possible while minimizing loss in accuracy by sampling a subset of our dataset that contains as much information as possible.&lt;/p&gt;
&lt;p&gt;     At this point, we are still investigating possible solutions. We are currently testing our ideas on images, with plans to transfer our work to applications on video data. Here’s what we’ve already done and what we’re working on right now:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sanity checks&lt;/strong&gt;: We first confirmed that skewed datasets tend to have lower accuracy. Using the Stanford Cars and the FGCV-Aircraft datasets, we looked at samples of 50 of one class versus 150 of the other and compared the results of a model trained on those samples to a model trained on 100 of each. We confirmed that the model trained on the balanced sample tended to outperform the model trained on the skewed samples.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;stanford-cars.png&#34; alt=&#34;Stanford Cars&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
Some examples from the &lt;a href=&#34;https://ai.stanford.edu/~jkrause/cars/car_dataset.html&#34;&gt;Stanford Cars dataset&lt;/a&gt;
&lt;/p&gt;  
&lt;p&gt;     The rest of our techniques essentially attempt to learn some representation of each example image in our dataset, then find a sampling strategy such that the examples we sample into our subset are as dissimilar as possible.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;SSD&lt;/strong&gt;: We tried using SSD (sum of square differences) as a distance between every pair of images in our dataset and sampling a subset that maximizes the sum of SSDs between images in the dataset. We did not find a correlation between sum of SSDs in a dataset and model accuracy, because SSD does not encode relevant information. Moreover, we would have to solve an NP-hard problem to accurately maximize the sum of SSDs in a sampled subset, making this strategy unviable on large datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Keypoint/descriptors-based methods&lt;/strong&gt;: Algorithms like SIFT, SURF, and ORB detect some number of keypoints that describe an image, along with descriptors for each of those keypoints. We first tried getting a single value for each sample image by summing the squared sums of descriptor components, but this did not produce correlations because we lost the spatial information from the keypoints. We are currently using k-means clustering to separate example images into k different image classes determined via unsupervised learning.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;sift.png&#34; alt=&#34;SIFT Descriptors&#34;&gt;&lt;/p&gt;
&lt;p align=&#34;center&#34; style=&#34;font-size:15px;&#34;&gt;
The &lt;a href=&#34;https://en.wikipedia.org/wiki/Scale-invariant_feature_transform&#34;&gt;scale-invariant feature transform &lt;/a&gt; (SIFT) algorithm in action.
&lt;/p&gt;  
&lt;p&gt;&lt;strong&gt;Edge detection&lt;/strong&gt;: Edges can imply what content is in an image, and a dataset whose images have varying amounts of horizontal and vertical edges may have more information than a dataset whose images’ horizontal and vertical edges are not as varied. We are currently investigating ways to implement and evaluate this idea.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Unsupervised feature encoding with neural networks&lt;/strong&gt;: By having a neural network (possibly sequence model, GAN) attempt to perform a task, it will learn the features of the data it attempts to perform that task on; we can use those learned features to describe our examples. This is likely appropriate to our problem, but we have not started investigating this yet. I personally think that this approach has merit simply because of how much recent work there has been on this subject, but I wonder if using a learning algorithm to address a problem with a learning algorithm may recurse the problem back into the solution.&lt;/p&gt;
&lt;p&gt;     For DEVIATE, successfully solving the human sampling problem means we’ll have found a way to reduce labeler workload while also mitigating problems with accuracy and bias that accompany skewed datasets. Our hope is that creating such a tool will let organizations train more effective models with fewer resources, allowing them to invest effort elsewhere in their projects.&lt;/p&gt;
&lt;p&gt;     If working on this problem interests you, consider following up with me (
&lt;a href=&#34;mailto:musicer@umich.edu&#34;&gt;musicer@umich.edu&lt;/a&gt;) or the advisor for the subteam working on this problem, Dr. Carol Flannagan (
&lt;a href=&#34;mailto:cacf@umich.edu&#34;&gt;cacf@umich.edu&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;citations&#34;&gt;Citations:&lt;/h3&gt;
&lt;p&gt;[1]	U.S. Government, “2010 Census Summary File 1,” U.S. Census Bureau, United States, SF1/10-4 RV, 2010.&lt;br&gt;
[2] 	J. Buolamwini and T. Gebru, “Gender Shades: Intersectional Accuracy Disparities in Commercial Gender Classification”, presented at the 1st  Conference on Fairness, Accountability, and Transparency, New York, NY, United States, February 23-24, 2018.&lt;br&gt;
[3]	A. Winn, “A Local Case Amplifies Opposition To Facial Recognition Technology”, Hour Detroit, Sept. 14, 2020. [Online]. Available: 
&lt;a href=&#34;https://www.hourdetroit.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.hourdetroit.com&lt;/a&gt;. [Accessed Oct. 25, 2020].&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faculty Talk: Situated Language Processing and Embodied Dialogue</title>
      <link>https://MSAIL.github.io/talk/chai_120120/</link>
      <pubDate>Tue, 01 Dec 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/chai_120120/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://web.eecs.umich.edu/~chaijy/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Joyce Chai&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Situated Language Processing Towards Interactive Task Learning&lt;/p&gt;
&lt;p&gt;Prof. Chai discussed some of her research on situated language processing, which is a field describing the interaction of language and visual/motor processing in embodied, situated, and language-for-action research traditions. This research also aims to unite converging and complementary evidence from behavioral, neuroscientific, neuropsychological and computational methods.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1f0Ye_j-aa1D893AHun9to2_3rgAV7FbY/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of her talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.ijcai.org/Proceedings/2018/0001.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language to Action: Towards Interactive Task Learning with Physical Agents&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;http://sled-group.eecs.umich.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Situated Language and Embodied Dialogue (SLED) Group&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faculty Talk: Prediction Markets and More</title>
      <link>https://MSAIL.github.io/talk/kutty_111720/</link>
      <pubDate>Tue, 17 Nov 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/kutty_111720/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://scholar.google.com/citations?user=8duCIlcAAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Dr. Sindhu Kutty&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Prediction Markets, Recommender Systems, Fairness in AI&lt;/p&gt;
&lt;p&gt;Dr. Kutty discussed some of her research on 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Prediction_market&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;prediction markets&lt;/a&gt;, 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Recommender_system&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;recommender systems&lt;/a&gt;, and fairness in AI. The talk mostly focused on some derivations for prediction markets, such as scoring functions for data collection.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Dr. Kutty asked us not to post the recording for this talk. We apologize for any inconvenience.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Scoring_rule&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Scoring Rules&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/1402.5458.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Information Aggregation in Exponential Family Markets&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Text Summarization with Deep Learning</title>
      <link>https://MSAIL.github.io/talk/textsummarization_111020/</link>
      <pubDate>Tue, 10 Nov 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/textsummarization_111020/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Yashmeet Gambhir&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Text Summarization with Deep Learning&lt;/p&gt;
&lt;p&gt;Yash discussed 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Automatic_summarization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;text summarization&lt;/a&gt;, where the goal is to&amp;hellip; summarize text. More specifically, he discussed abstractive summarization, of which the goal is to generate &lt;em&gt;novel&lt;/em&gt; sentences using natural language generation techniques. One such method for doing this is using pointer-generator networks. After discussing PGNs, he went on to discuss a paper describing extreme summarization to combat model hallucination for this task. The papers discussed are linked below.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1GCeGWfC_9jF6BDlLalEpKt9-KIlq_Hvv/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of his talk here.&lt;/a&gt; &lt;em&gt;Unfortunately this only includes the second half of the talk about abstractive summarization, because we forgot to record starting at the beginning.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.aclweb.org/anthology/P17-1099/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Get To The Point: Summarization with Pointer-Generator Networks&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.173/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On Faithfulness and Factuality in Abstractive Summarization&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Convolutional Neural Networks</title>
      <link>https://MSAIL.github.io/previous_material/cnn/</link>
      <pubDate>Thu, 05 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/cnn/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Convolutional Neural Networks&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;This lesson covered convolutional neural networks, which serve as the backbone for many modern-day deep learning applications. Most commonly, convolutional neural networks are used for vision tasks (although not exclusively).&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1522OsXalZScvuUxXrOTbUZuISZUY-HqO&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides on CNNs&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://docs.google.com/presentation/d/16TMR2sM9T75qALw3CCigUF_JxMQ5gceM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Slides on Neural Networks&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Brain-Inspired AI</title>
      <link>https://MSAIL.github.io/talk/brain_insp_110320/</link>
      <pubDate>Tue, 03 Nov 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/brain_insp_110320/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://johnmday.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Day&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Brain-inspired AI&lt;/p&gt;
&lt;p&gt;John started his talk by discussing brain-inspired AI in general, which involves studies like 
&lt;a href=&#34;https://www.nature.com/articles/531S16a&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;neural modeling&lt;/a&gt;, 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Artificial_consciousness#:~:text=Artificial%20consciousness%20%28AC%29%2C%20also,artificial%20intelligence%20and%20cognitive%20robotics.&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;artificial consciousness&lt;/a&gt;, 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Spiking_neural_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;spiking neural nets&lt;/a&gt;, and 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Cognitive_architecture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cognitive architectures&lt;/a&gt;. Afterwards, he focused on deep predictive coding networks.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1Gly--En531JIa4F_h15TWAftKrJPEd5W/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of his talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://www.cell.com/neuron/pdf/S0896-6273%2817%2930509-3.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brain-inspired AI&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;http://klab.tch.harvard.edu/publications/PDFs/gk7591_Lotteretal_ICLR2017.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Predictive Coding Networks for Video Prediction and Unsupervised Learning&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/1802.04762.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Deep Predictive Coding Network for Object Recognition&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faculty Talk: Cognitive Architecture</title>
      <link>https://MSAIL.github.io/talk/laird_102720/</link>
      <pubDate>Tue, 27 Oct 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/laird_102720/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://laird.engin.umich.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. John Laird&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Cognitive Architecture&lt;/p&gt;
&lt;p&gt;Prof. Laird discussed 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Cognitive_architecture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;cognitive architecture&lt;/a&gt; - more specifically, he discussed 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Soar_%28cognitive_architecture%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SOAR&lt;/a&gt;, a cognitive architecture that his research group has been developing and maintaining for decades. SOAR is simultaneously a theory of cognition and an architecture, with the ultimate goal of enabling general intelligent agents to realize the full cognitive capabilities of humans.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1zlkGdcEo9Ycp2ziZbOXhIGH5R53DQzBJ/view&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of his talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://soar.eecs.umich.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SOAR Group Homepage&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://mitpress.mit.edu/books/soar-cognitive-architecture&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The SOAR Cognitive Architecture, written by John E. Laird&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=VM1PGpvCEHI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A similar talk given to MSAIL back in 2011!&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Image-to-Image Translation with Conditional Adversarial Networks</title>
      <link>https://MSAIL.github.io/talk/cgan_102020/</link>
      <pubDate>Tue, 20 Oct 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/cgan_102020/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Andrew Awad&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: 
&lt;a href=&#34;https://arxiv.org/abs/1611.07004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Andrew presented on a CVPR 2017 paper by Isola et al. This paper aimed to investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. The networks in question were 
&lt;a href=&#34;https://arxiv.org/abs/1411.1784&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CGANs&lt;/a&gt;, proposed earlier by Mirza et al. Isola et al. also proposed the 
&lt;a href=&#34;https://paperswithcode.com/method/patchgan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PatchGAN&lt;/a&gt; discriminator.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;A certain forgetful lead admin forgot to record this discussion. We apologize for the inconvenience&lt;/em&gt;.&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Paper(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1611.07004&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Image-to-Image Translation with Conditional Adversarial Networks&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/1411.1784&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CGAN&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Other&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://paperswithcode.com/method/patchgan&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PatchGAN&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://phillipi.github.io/pix2pix/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;pix2pix GitHub Page&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://towardsdatascience.com/gan-pix2pix-generative-model-c9bf5d691bac&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Medium article on pix2pix&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reinforcement Learning Applied to COVID-19 Optimization Problems</title>
      <link>https://MSAIL.github.io/talk/rlcovid_101320/</link>
      <pubDate>Tue, 13 Oct 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/rlcovid_101320/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Nikhil Devraj&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Reinforcement Learning for COVID-19 Optimization Problems&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re not living under a rock, you know that COVID-19 is ravaging current-day society and requires monumental efforts on all scales, be it from individuals or from entire governments. In particular, governments play a major role in helping control the spread of COVID-19 by instituting policies to help with efforts such as lockdown enforcement and vaccine distribution. During this talk Nikhil talked about some previously proposed approaches to modeling such policy problems as control problems that could be solved with reinforcement learning.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1Xi2tofO321kWTMz_2pD9ltuAc-XqHTnY/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of this discussion here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Paper(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2009.04647.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;COVID-19 Pandemic Cyclic Lockdown Optimization Using Reinforcement Learning&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://journals.sagepub.com/doi/full/10.1177/0165551520959798&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Optimal policy learning for COVID-19 prevention using reinforcement learning&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2009.06602.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;VacSIM: LEARNING EFFECTIVE STRATEGIES FOR COVID-19 VACCINE DISTRIBUTION USING REINFORCEMENT LEARNING&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Article(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://towardsdatascience.com/reinforcement-learning-for-covid-19-simulation-and-optimal-policy-b90719820a7f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reinforcement learning for Covid-19: Simulation and Optimal Policy&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Regression, Part 2 (Application)</title>
      <link>https://MSAIL.github.io/previous_material/regression_2/</link>
      <pubDate>Thu, 08 Oct 2020 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/regression_2/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Applications of Regression &lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Robert Aung&lt;/p&gt;
&lt;p&gt;This lesson covered some interesting applications of regression.&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://colab.research.google.com/drive/12nmYKp5IcUdUiZmrHaUK7YsUu1vIOXkj?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Colab notebook&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Human-Centered Autonomous Vehicles</title>
      <link>https://MSAIL.github.io/talk/av_100620/</link>
      <pubDate>Tue, 06 Oct 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/av_100620/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Patrick Morgan&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Human-Centered Autonomous Vehicles&lt;/p&gt;
&lt;p&gt;Patrick focused on discussing 
&lt;a href=&#34;https://arxiv.org/pdf/1810.01835.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human-Centered Autonomous Vehicle Systems: Principles of Effective Shared Autonomy&lt;/a&gt;. This paper proposes that we should build autonomous vehicles with humans in mind, and that getting humans and artificial intelligence systems to collaborate effectively is an achievable and worthy goal. In this light, they propose a human-centered paradigm for engineering shared autonomy systems in the car that erase the boundary between human and machine in the way the driving task is experienced. The researchers propose a 7 principle engineering design process that will make autonomous vehicles safer and greatly lower the cost of development. This discussion also ended up touching on other fundamental issues in AI, such as data privacy.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1fZvu5nDO3qhgwwc1X85aDmQ44RHD9bWF/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of this discussion here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Paper(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/1810.01835.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Human-Centered Autonomous Vehicle Systems: Principles of Effective Shared Autonomy&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>AlphaZero and its Impact on the World of Chess</title>
      <link>https://MSAIL.github.io/talk/alphazero_chess_092920/</link>
      <pubDate>Tue, 29 Sep 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/alphazero_chess_092920/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Kevin Wang&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: AlphaZero and its Impact on Chess&lt;/p&gt;
&lt;p&gt;The world was appalled when 
&lt;a href=&#34;https://ai.googleblog.com/2016/01/alphago-mastering-ancient-game-of-go.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaGo&lt;/a&gt; first 
&lt;a href=&#34;https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;played Lee Sedol in Go&lt;/a&gt;, winning 4 matches to 1. DeepMind subsequently released 
&lt;a href=&#34;https://deepmind.com/blog/article/alphago-zero-starting-scratch&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaGo Zero&lt;/a&gt;, an iteration on AlphaGo that beat it 100-1. Going even further, they released 
&lt;a href=&#34;https://deepmind.com/blog/article/alphazero-shedding-new-light-grand-games-chess-shogi-and-go&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaZero&lt;/a&gt;, which learned how to play games such as Shogi and Chess. Kevin, an avid chess enthusiast, wanted to discuss what this meant for the Chess world.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/1jETQcqIZqp24drl8ARH4bABuv963g0Ah/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find a recording of this discussion here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Paper(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/2009.04374.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Assessing Game Balance with AlphaZero: Exploring Alternative Rule Sets in Chess&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/pdf/1712.01815.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mastering Chess and Shogi by Self-Play with a General RL Algorithm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video(s):&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=7L2sUGcOgh0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video From DeepMind&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=mOqmLYlFdBo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AlphaZero VS AlphaZero || THE PERFECT GAME&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Regression, Part 1 (Theory and Implementation)</title>
      <link>https://MSAIL.github.io/previous_material/regression_1/</link>
      <pubDate>Thu, 24 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/regression_1/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Theory and Implementation of Regression&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Robert Aung&lt;/p&gt;
&lt;p&gt;This lesson covered the theory behind and implementation of a linear regression model with gradient descent.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://umich.zoom.us/rec/share/94fSO_w_AT68Td2e0Qr_kckIVBepdNLecMn5mTvFOH994JWIkKSZLl3u9xpFr6J6.oj49dWOJeBFBzPA2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can view a recording of this lesson here.&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1VHWuE_lqbKnDKZ8HKbVcLArbe8cMWKsd_61FY4FTn-E/edit?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://colab.research.google.com/drive/18MoSHNwUnEKwvokZAZA1AcLc6AJ3Bs81?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson Colab notebook&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Faculty Talk: Strategic Reasoning in Dynamic Environments</title>
      <link>https://MSAIL.github.io/talk/wellman_092220/</link>
      <pubDate>Tue, 22 Sep 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/wellman_092220/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;http://strategicreasoning.org/michael-p-wellman/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Prof. Michael Wellman&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Strategic Reasoning in Dynamic Environments&lt;/p&gt;
&lt;p&gt;Dr. Wellman presented about his group&amp;rsquo;s research, which generally specializes in game theory and multi-agent reasoning in dynamic environments. Much of his work lies in the domain of markets and commerce. You can find his 
&lt;a href=&#34;https://strategicreasoning.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;group&amp;rsquo;s page here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Prof. Wellman asked us not to post the recording publicly. A recording is available within our Slack channel, so please search in there if you&amp;rsquo;re interested. We apologize for any inconvenience.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Article(s)&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://strategicreasoning.org/empirical-game-theoretic-analysis/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Empirical Game-Theoretic Analysis&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://strategicreasoning.org/computational-finance/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Computational Finance&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://strategicreasoning.org/world-with-autonomous-agents/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;World with Autonomous Agents&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Video(s):&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://www.youtube.com/watch?v=SnTf-iWUTpk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Artificially Intelligent Decision Makers in the Real World&amp;rdquo; with Michael Wellman &amp;amp; Bill Powers&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Classification with Logistic Regression</title>
      <link>https://MSAIL.github.io/previous_material/classification_logreg/</link>
      <pubDate>Thu, 17 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/previous_material/classification_logreg/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Topic&lt;/strong&gt;: Classification with Logistic Regression&lt;br&gt;
&lt;strong&gt;Presenter&lt;/strong&gt;: Kevin Wang&lt;/p&gt;
&lt;p&gt;Kevin taught some members of MSAIL some basics of machine learning, culminating in building out a classification model for MNIST from scratch using logistic regression. Classification is the process of categorizing data into predetermined groups, and logistic regression is a means to build a classifier (though certainly not the only means to do so).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;We couldn&amp;rsquo;t find the recording of this session, but be sure to check out the supplemental materials.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://docs.google.com/presentation/d/1YVw4T0E_f6m0NovhS3YbwAMLns0tQfFE/edit#slide=id.p1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson slides&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://colab.research.google.com/drive/1Cein0r-J9N2vX1xh24cRLEHgBkJx3p7w?authuser=1#scrollTo=ubgi9PVZDDgU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lesson Colab notebook&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://builtin.com/data-science/basic-linear-algebra-deep-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Basic matrix operations&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.kaggle.com/learn/python&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Basic Python programming&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The Trend Towards Large Language Models</title>
      <link>https://MSAIL.github.io/talk/gpt3_091520/</link>
      <pubDate>Tue, 15 Sep 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/gpt3_091520/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: Sean Stapleton&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: GPT-3 and its Implications&lt;/p&gt;
&lt;p&gt;In recent years, we’ve seen 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Natural_language_processing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;natural language processing&lt;/a&gt; (NLP) performance accelerate drastically across a number of tasks, including text completion, 
&lt;a href=&#34;https://paperswithcode.com/task/machine-translation&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;machine translation&lt;/a&gt;, and 
&lt;a href=&#34;https://paperswithcode.com/task/question-answering&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;question answering&lt;/a&gt;. Much of this performance gain has been attributed to two trends in the NLP community, namely the introduction of transformers, and the increase in model size (and consequent need for intense computational power). Capitalizing on these trends, 
&lt;a href=&#34;https://openai.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;OpenAI&lt;/a&gt; recently released a transformer-based model called 
&lt;a href=&#34;https://en.wikipedia.org/wiki/gpt-3&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GPT-3&lt;/a&gt; with 175 billion parameters, that was trained on roughly 500 billion tokens scraped from the internet.
This MSAIL discussion focused predominantly on three questions addressed in the paper:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Does a substantial increase in model size actually lead to better performance in downstream tasks?&lt;/li&gt;
&lt;li&gt;Can language models effectively model intelligent and adaptable thought?&lt;/li&gt;
&lt;li&gt;What are the biases and risks associated with training a language model on the entire internet?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Sean also covered the transformer and GPT-3 model architectures, though the focus of the discussion was not on this aspect of the paper.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://drive.google.com/file/d/12beS3Er1AuiCbmxNR0rAiiot43xTkw_n/view?usp=sharing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;You can find the recording of this talk here.&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;https://arxiv.org/abs/2005.14165&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Language Models are Few-Shot Learners (Brown et al.)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Articles:&lt;/strong&gt;&lt;br&gt;

&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Illustrated Transformer&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;http://jalammar.github.io/illustrated-gpt2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Illustrated GPT-2&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bloomberg Tech Talk: Applied Named Entity Recognition</title>
      <link>https://MSAIL.github.io/talk/ner_090920/</link>
      <pubDate>Wed, 09 Sep 2020 18:00:00 -0400</pubDate>
      <guid>https://MSAIL.github.io/talk/ner_090920/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Speaker(s)&lt;/strong&gt;: 
&lt;a href=&#34;https://www.preotiuc.ro/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Daniel Preotiuc-Pietro&lt;/a&gt; and 
&lt;a href=&#34;https://scholar.google.com/citations?user=ycBuNT0AAAAJ&amp;amp;hl=en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mayank Kulkarni&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Topic&lt;/strong&gt;: Applied Named Entity Recognition&lt;/p&gt;
&lt;p&gt;During this talk, two senior research scientists from 
&lt;a href=&#34;https://www.techatbloomberg.com/ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bloomberg&amp;rsquo;s AI Group&lt;/a&gt; presented on some of their work on Applied 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Named-entity_recognition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Named Entity Recognition&lt;/a&gt;. Their discussion focused on applications of NER at Bloomberg, multi-domain NER, and analysis of NER using temporal data.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Due to restrictions from Bloomberg, we were unable to record this session. We apologize for the inconvenience.&lt;/em&gt;&lt;/p&gt;
&lt;h3 id=&#34;supplemental-resources&#34;&gt;Supplemental Resources&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;Papers from Bloomberg AI&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.680/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Temporally-Informed Analysis of Named Entity Recognition&lt;/a&gt;, ACL 2020&lt;br&gt;

&lt;a href=&#34;https://www.aclweb.org/anthology/2020.acl-main.750/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multi-Domain Named Entity Recognition with Genre-Aware and Agnostic Inference&lt;/a&gt;, ACL 2020&lt;br&gt;

&lt;a href=&#34;https://www.aclweb.org/anthology/P19-1587/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Semi-Markov Structured Support Vector Machine Model for High-Precision Named Entity Recognition&lt;/a&gt;, ACL 2019&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Slides from Bloomberg AI&lt;/strong&gt;:&lt;br&gt;

&lt;a href=&#34;https://slideslive.com/38929187/multidomain-named-entity-recognition-with-genreaware-and-agnostic-inference&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multi-Domain NER&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Alumni</title>
      <link>https://MSAIL.github.io/alumni/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/alumni/</guid>
      <description>&lt;p&gt;MSAIL dates back to 2008 and has a history of many students who went on to do great things in the tech industry. We&amp;rsquo;d like to showcase as many of our alumni here as possible. If you are a previous member of MSAIL and would like to be featured on this page, please 
&lt;a href=&#34;https://MSAIL.github.io/contact/&#34;&gt;let the admin team know!&lt;/a&gt;&lt;/p&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;style.css&#34;&gt;
&lt;div id=&#34;leadership&#34;&gt;
&lt;!-- Robert Aung --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/robert_aung.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Robert Aung&lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Fall 2020 &lt;/div&gt;
&lt;/div&gt;
&lt;!-- Andrew Awad --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
      &lt;img src=&#34;portraits/andrew_awad.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Andrew Awad&lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Winter 2021 &lt;/div&gt;
&lt;/div&gt;
&lt;!-- Kierra Davis --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/kierra_davis.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt; Kierra Davis &lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Winter 2021 &lt;/div&gt;
&lt;/div&gt;
&lt;!-- Nikhil Devraj --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/nikhil_devraj.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Nikhil Devraj&lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Fall 2020 &lt;/div&gt;
&lt;/div&gt;
&lt;!-- Isaac Fung --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/isaac_fung.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Isaac Fung&lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Winter 2022 &lt;/div&gt;
&lt;/div&gt;
&lt;!-- Abhay Shakhapur.jpg --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/abhay_shakhapur.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Abhay Shakhapur&lt;/div&gt;
	&lt;div class=&#34;leader-info&#34;&gt; Winter 2024 &lt;/div&gt;	
&lt;/div&gt;
&lt;!-- Andrew Li --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/andrew_li.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Andrew Li&lt;/div&gt;
	&lt;div class=&#34;leader-info&#34;&gt; Winter 2024 &lt;/div&gt;	
&lt;/div&gt;
&lt;!-- Anthony Liang --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/anthony_liang.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt; Anthony Liang &lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Winter 2020 (BS), Winter 2021 (MS) &lt;/div&gt;
&lt;/div&gt;
&lt;!-- Patrick Morgan --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/patrick_morgan.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt; Patrick Morgan &lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Winter 2021 &lt;/div&gt;
&lt;/div&gt;
&lt;!-- Ashwin Sreevatsa --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/ashwin_sreevatsa.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Ashwin Sreevatsa&lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Winter 2022 &lt;/div&gt;	
&lt;/div&gt;
&lt;!-- Sean Stapleton --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/sean_stapleton.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt; Sean Stapleton &lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Fall 2020 &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;\!-- Kevin Wang -\-&gt; --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/kevin_wang.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt; Kevin Wang &lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Fall 2021 &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;\!-- Nina Li -\-&gt; --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/nina_li.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt; Nina Li &lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Winter 2023 &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;\!-- Chloe Snyders -\-&gt; --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/chloe_snyders.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt; Chloe Snyders &lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Winter 2023 &lt;/div&gt;
&lt;/div&gt;
&lt;!-- &lt;\!-- William Wang -\-&gt; --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/william_wang.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt; William Wang &lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Winter 2024 &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Contact Us</title>
      <link>https://MSAIL.github.io/contact/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/contact/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Contact us at 
&lt;a href=&#34;msail-admin@umich.edu&#34;&gt;msail-admin@umich.edu&lt;/a&gt;!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can find our professors&amp;rsquo; and admin team&amp;rsquo;s individual emails 
&lt;a href=&#34;https://MSAIL.github.io/aboutus/&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Joining MSAIL</title>
      <link>https://MSAIL.github.io/join/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/join/</guid>
      <description>&lt;p&gt;Add yourself to the email list on 
&lt;a href=&#34;https://mcommunity.umich.edu/#group:Michigan%20Student%20AI%20Lab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MCommunity&lt;/a&gt; by logging in and then clicking &amp;ldquo;Join Group&amp;rdquo; in the top-left corner of the panel (see the image below). After doing this, you will receive emails from us.&lt;/p&gt;
&lt;p&gt;Also, don&amp;rsquo;t forget to 
&lt;a href=&#34;https://join.slack.com/t/msail-team/signup&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;join our Slack group&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;mcommunity.png&#34; alt=&#34;MSAIL MCommunity Page&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;leaving-msail&#34;&gt;Leaving MSAIL&lt;/h2&gt;
&lt;p&gt;If you later wish to leave, simply go to the 
&lt;a href=&#34;https://mcommunity.umich.edu/#group:Michigan%20Student%20AI%20Lab&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MCommunity&lt;/a&gt; page and click &amp;ldquo;Resign&amp;rdquo;, which will be in the top left corner in place of &amp;ldquo;Join Group&amp;rdquo; in the above photo.&lt;/p&gt;
</description>
    </item>
    
    
    
    <item>
      <title>MSAIL Governance: A Brief Constitution</title>
      <link>https://MSAIL.github.io/constitution/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/constitution/</guid>
      <description>&lt;p&gt;&lt;em&gt;[Last updated on 2017-10-02]&lt;/em&gt;&lt;/p&gt;
&lt;h2&gt; Terminology &lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Cosine&lt;/strong&gt;: The lead admin.&lt;br&gt;
&lt;strong&gt;Sine&lt;/strong&gt;: A member of the admin team.&lt;/p&gt;
&lt;h2&gt;Article 0: MSAIL seeks to increase its members&#39; Machine Learning knowledge.&lt;/h2&gt;
&lt;p&gt;Indeed.  MSAIL will maintain a list of Members, defined as participants in at least one major communications channels such as Slack or a mailing list.  MSAIL will strive to discuss Machine Learning literature regularly throughout each school year.
&lt;p&gt;Upon joining the organization, all members agree not to undermine the purpose or mission of MSAIL.
&lt;p&gt;It falls to the Cosine to execute this Article.
&lt;h2&gt;Article 1: Legislative powers lie in the Sines.&lt;/h2&gt;
&lt;p&gt;The body of Sines possesses complete power: a simple majority of Sines will suffice to amend this Constitution, to appoint a Cosine, to override any decision of the Cosine, or to act in place of the Cosine.  Sines who are informed of a decision to be made but offer no prompt response are not counted in the denominator of the &#34;simple majority&#34;.
&lt;p&gt;The Sines will not allow the number of Cosines to fall below 1.  By default, the Sines lie dormant and all powers and responsibilities lie with the Cosine. 
&lt;h2&gt;Article 2: Executive powers lie in the Cosine.&lt;/h2&gt;
&lt;p&gt;Specifically, the Cosine is responsible for the day-to-day functioning of MSAIL, and to that end may act and delegate arbitrarily within Constitutional bounds.  It is traditional for the Cosine to distribute significant short-term tasks among the Sines.  The Cosine will report to the Sines, and the Sines will vote promptly on presented issues.
&lt;p&gt;The Cosine may appoint new Sines with the advice and consent of the old Sines.  The Cosine will not allow the number of Sines to fall below 3.  The Cosine shall break ties among the Sines.  The Cosine may be a Sine.
&lt;h2&gt;Article 3: MSAIL may also have Faculty Mentors.&lt;/h2&gt;
&lt;p&gt;A faculty mentor can help us just by association.  MSAIL may mention their names on its official communications.  MSAIL shall inform each Faculty Mentor of its activities via brief weekly emails with no reply needed.
&lt;p&gt;Faculty mentors can point us to literature.  Faculty mentors are always welcome to share cool papers.  MSAIL may also request recommendations within a specific topic.
&lt;p&gt;A Faculty Mentor need not take on additional responsibilities, but may choose to do so if requested.
&lt;p&gt;The Sines and Cosine will endeavor to use Faculty Mentors’ time effectively.  Faculty Mentors need not make any administrative decisions.  Faculty Mentors are always welcome but never obliged to attend MSAIL meetings.
&lt;h2&gt;Article 4: MSAIL is committed to inclusivity and transparency.&lt;/h2&gt;
&lt;p&gt;MSAIL will not discriminate based on academic affiliation(s) or lack thereof, age, breastfeeding or lack thereof, career status, color, criminal record, disability or lack thereof, ethnicity, employment status, gender expression, gender identity, HIV status, marital status, national origin, parental status, personal association, physical features such as height and weight, political activity, pregnancy or lack thereof, race, religion or lack thereof, sex, sexual orientation, socioeconomic background, or veteran status.
&lt;p&gt;MSAIL will, moreover, actively include members, no matter the properties listed above.  The creation and maintenance of an inclusive environment touches all aspects of our activities, from communications to recruitment and from discussion topics to leadership positions.  
&lt;p&gt;MSAIL’s motto will be &#34;the more, the merrier&#34;; a corollary is that information such as planning discussions will be available to all members, so long as it does not conflict with privacy concerns.
&lt;p&gt;To rephrase a subset of the above in a university-required formula: MSAIL is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, national origin, age, marital status, sex, sexual orientation, gender identity, gender expression, disability, religion, height, weight, or veteran status in its membership or activities unless permitted by university policy for gender specific organizations.
&lt;p&gt;It falls to the Cosine to execute this Article.
&lt;h2&gt;Article 5: This Constitution may be amended by the Sines.&lt;/h2&gt;
&lt;p&gt;Any Member may propose an amendment’s text.  See Article 1 for voting details.
&lt;h2&gt;Article 6: This Constitution will be MSAIL&#39;s supreme law.&lt;/h2&gt;
&lt;p&gt;(Modulo University Policy.)
&lt;h2&gt;Article 7: This Constitution will be ratified by the Sines.&lt;/h2&gt;
&lt;p&gt;This Constitution will be re-written and ratified at least once in any 1024-day window.  See Article 1 for voting details.
</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>https://MSAIL.github.io/projects/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/projects/</guid>
      <description>&lt;h4 id=&#34;winter-2024-building-mlp-multi-layer-perceptron-from-scratch-using-python&#34;&gt;Winter 2024: Building MLP (Multi-layer Perceptron) from scratch using Python&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn how to use python&lt;/li&gt;
&lt;li&gt;Learn about various machine learning techniques (Perceptron, GD)&lt;/li&gt;
&lt;li&gt;Have more questions? Contact 
&lt;a href=&#34;mailto:msail-admin@umich.edu&#34;&gt;msail-admin@umich.edu&lt;/a&gt;!&lt;/li&gt;
&lt;li&gt;Meeting time: Projects 7-8pm at NUB 1567&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;fall-2023-car-accident-prediction&#34;&gt;Fall 2023: Car accident prediction&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Learn how to predict the next car accident in a specific area of the U.S.&lt;/li&gt;
&lt;li&gt;You can check out the project material 
&lt;a href=&#34;https://docs.google.com/document/d/1lwHD0Hb50h97z3mJ5uylCEKen8cT61VNl-OxxC5n5IU/edit&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Have more questions? Contact kevindw@ or andson@&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Resources for MSAIL Members</title>
      <link>https://MSAIL.github.io/resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/resources/</guid>
      <description>&lt;p&gt;&lt;em&gt;This page will be updated periodically with new resources. Keep an eye out!&lt;/em&gt;&lt;br&gt;
Feel free to send any resource requests that you&amp;rsquo;d like listed here to 
&lt;a href=&#34;mailto:msail-admin@umich.edu&#34;&gt;msail-admin@umich.edu&lt;/a&gt;.&lt;br&gt;
Furthermore, if you are a resource owner and would like links to your work removed from this page, contact us at the email above.&lt;/p&gt;
&lt;h2 id=&#34;table-of-contents&#34;&gt;Table of Contents&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#learning-concepts&#34;&gt;Learning Concepts&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;#intro-to-deep-learning&#34;&gt;Intro to Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#computer-vision&#34;&gt;Computer Vision&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#natural-language-processing&#34;&gt;Natural Language Processing&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#reinforcement-learning&#34;&gt;Reinforcement Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#campus-involvement&#34;&gt;Campus Involvement&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#conferences&#34;&gt;Conferences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://MSAIL.github.io/talk/&#34;&gt;Past Talks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#medium-blog-articles&#34;&gt;Medium/Blog articles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#meta-skills-and-mindset&#34;&gt;Meta-skills and Mindset&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;!--
  + [comment]: # (+ [Opportunities](#opportunities))
  + [Classical Methods in AI](#classical-methods-in-ai)
--&gt;
&lt;h2 id=&#34;about-this-page&#34;&gt;About this page&lt;/h2&gt;
&lt;p&gt;This page serves as a conglomeration of resources for MSAIL members to gain access to knowledge regarding AI. This includes 
&lt;a href=&#34;#learning-concepts&#34;&gt;technical content&lt;/a&gt;, 
&lt;a href=&#34;#campus-involvement&#34;&gt;ways to get involved on campus&lt;/a&gt;, 
&lt;a href=&#34;#opportunities&#34;&gt;opportunities for research, jobs, and networking&lt;/a&gt;, and 
&lt;a href=&#34;#meta-skills-and-mindset&#34;&gt;other advice regarding involvement in the field&lt;/a&gt;.&lt;br&gt;
We intend to continually update this page with more resources and will occasionally restructure it to provide better depth.&lt;/p&gt;
&lt;!--
uh I need to finish this but didn&#39;t have time - ND
### There are too many resources! How do I approach this???
The very first thing we recommend you do is to understand why you want to learn from this page and about AI in general. And this isn&#39;t just saying &#34;I think AI is the future, with endless possibilities&#34;. Yes, that&#39;s a good way to hype yourself up about it. But it&#39;s important to really align it with your personal goals so you can pursue it further. Do you see yourself working on AI research and/or engineering? Are you trying to use it as part of research in a different subject area? Are you scoping the field to determine if you&#39;re even truly interested? Asking these questions will help you identify what resources to look at.

That being said, you don&#39;t need to know the exact path you&#39;re going to go in. PhD candidates in AI usually don&#39;t know their exact research direction. In fact, one of our admins spoke to one of the vision professors here at Michigan and the professor said &#34;I became a professor so I could figure out what I was interested in.&#34; So the advice here is to understand your general motivation and worry about the details as time goes on.

--&gt;
&lt;h2 id=&#34;learning-concepts&#34;&gt;Learning Concepts&lt;/h2&gt;
&lt;p&gt;It&amp;rsquo;s important to know that AI is a broad field spanning the past century. We shouldn&amp;rsquo;t simply think of AI as &amp;ldquo;deep learning&amp;rdquo;, because it&amp;rsquo;s not. We&amp;rsquo;re listing resources regarding the most popular topics first because we understand they&amp;rsquo;re likely what initially grabbed your interest, but do note that there&amp;rsquo;s plenty of research being carried out on topics that we&amp;rsquo;ve probably never even heard of.&lt;/p&gt;
&lt;p&gt;Sam Finlayson from Harvard/MIT has a fantastic 
&lt;a href=&#34;https://sgfin.github.io/learning-resources/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page on resources you can use to dive into ML&lt;/a&gt;. It&amp;rsquo;s quite advanced but provides a good starting point for those looking for a comprehensive list. We&amp;rsquo;ll be using some of these resources in our own lists.&lt;/p&gt;
&lt;p&gt;Keep in mind that just because we link a large list doesn&amp;rsquo;t mean you should be going through the entire thing. There&amp;rsquo;s way too much stuff to look at. The links within links within links are all meant to provide options; choose a specific topic that interests you (for example, generative models) and slowly explore it.&lt;/p&gt;
&lt;h3 id=&#34;intro-to-deep-learning&#34;&gt;Intro to Deep Learning&lt;/h3&gt;
&lt;p&gt;Understanding deep learning to a satisfactory degree requires working familiarity (but not necessarily mastery) with the following prerequisite topics:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Vector and Matrix Operations&lt;/li&gt;
&lt;li&gt;Calculus (Partial and total derivatives, gradients)&lt;/li&gt;
&lt;li&gt;Probability&lt;/li&gt;
&lt;li&gt;Basic statistics&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Check out the slides/recordings from our 
&lt;a href=&#34;https://MSAIL.github.io/education/&#34;&gt;education sessions&lt;/a&gt;, where some of these concepts are explained.&lt;/p&gt;
&lt;p&gt;For a more thorough introduction to the field, we suggest the following resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://course.fast.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Fast AI&amp;rsquo;s Deep Learning course&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://web.eecs.umich.edu/~justincj/teaching/eecs498/FA2020/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 498/598 - Deep Learning for Computer Vision @ University of Michigan&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://cs231n.stanford.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS231n - Convolutional Neural Networks for Visual Recognition @ Stanford University&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In particular, the second listed resource (EECS 498/598) is a course offered here every Fall. It is very similar to CS231n, so just one of the two would be satisfactory. These two courses are extremely well designed and we recommend them as a starting point.&lt;/p&gt;
&lt;h3 id=&#34;computer-vision&#34;&gt;Computer Vision&lt;/h3&gt;
&lt;p&gt;EECS 598 and CS231n (linked above) are a good start for getting involved with vision. These courses are heavily focused on deep learning, so if you want to learn about some of the methods that were popular before deep learning took off, try materials from 
&lt;a href=&#34;https://web.eecs.umich.edu/~justincj/teaching/eecs442/WI2020/schedule.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 442&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://github.com/jbhuang0604/awesome-computer-vision#readme&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Here&amp;rsquo;s a massive resource list called Awesome Computer Vision&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The reason we link those courses above is because they cover a good breadth of topics in vision. You will know what you need to in order to make proper searches for your own research and projects once you&amp;rsquo;ve gone through one of them.&lt;/p&gt;
&lt;h3 id=&#34;natural-language-processing&#34;&gt;Natural Language Processing&lt;/h3&gt;
&lt;p&gt;NLP also has a few courses worth looking at:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://web.stanford.edu/class/cs224n/index.html#schedule&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS224n - NLP with Deep Learning @ Stanford University&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://web.eecs.umich.edu/~wangluxy/courses/eecs598_fa2020/eecs598_fa2020.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 598 - NLP with Deep Learning @ University of Michigan&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;This class was a seminar. It was less focused on educational material and more focused on contemporary research. So scroll this page if you&amp;rsquo;re looking for interesting papers.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/keon/awesome-nlp&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome NLP&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Similar to Awesome CV linked in the previous section, this is a massive list of resources to get acquainted with the field.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NLP Textbook by Jacob Eisenstein&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://jalammar.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jay Alammar&amp;rsquo;s Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://nlp.seas.harvard.edu/2018/04/03/attention.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Annotated Transformer&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;This is a really nice guide going through a &amp;ldquo;line by line&amp;rdquo; implementation of 
&lt;a href=&#34;https://arxiv.org/abs/1706.03762&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Attention is All You Need&lt;/a&gt; (the seminal transformer paper)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;reinforcement-learning&#34;&gt;Reinforcement Learning&lt;/h3&gt;
&lt;p&gt;Here are some courses you can look at to learn about reinforcement learning:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://rail.eecs.berkeley.edu/deeprlcourse/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 285 - Deep Reinforcement Learning @ UC Berkeley&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.youtube.com/watch?v=FgzM3zpZ55o&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CS 234 - Reinforcement Learning @ Stanford University&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;This links to a series of lecture videos because the course website was taken down for some reason.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Open AI put a ton of effort into creating a comprehensive resource for people to learn RL:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://spinningup.openai.com/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Spinning Up in Deep RL&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other resources:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://github.com/aikorea/awesome-rl&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Awesome RL&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Similar to Awesome CV/NLP linked in the previous sections, this is a massive list of resources to get acquainted with the field.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://deepmind.com/learning-resources&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Resources from DeepMind&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sutton &amp;amp; Barto - Intro to RL Textbook&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;This is the de facto textbook for people to self-study RL. We can&amp;rsquo;t guarantee that this link will always work, but if it&amp;rsquo;s taken down, &amp;ldquo;Sutton and Barto&amp;rdquo; is all you&amp;rsquo;d need to search up.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://lilianweng.github.io/lil-log/tag/reinforcement-learning&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Lilian Weng&amp;rsquo;s Lil&amp;rsquo; Log&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;Her blog contains more than just RL, but her RL posts are thorough and accessible (provided you have a basic ML background). In general, we really recommend blog posts from professionals because they&amp;rsquo;re easy to read yet rife with information.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Finally, we also have the reinforcement learning theory course (EECS 598) here at Michigan. However, materials aren&amp;rsquo;t posted online and enrollment is, as usual, heavily limited - so we recommend looking at the materials from other courses in the meantime.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;We&amp;rsquo;re in the process of adding more learning resources!&lt;/strong&gt;&lt;/p&gt;
&lt;!--

### Robotics

### Classical Methods in AI
Note that this section&#39;s header is a major generalization. As noted above, AI is a broad field, and we should note that NLP, CV, and RL are as much fields of study as they are *methods* (loosely speaking). They are a means to an end. As to what end you wish to pursue - that falls to you. 
--&gt;
&lt;h2 id=&#34;campus-involvement&#34;&gt;Campus involvement&lt;/h2&gt;
&lt;p&gt;Getting involved during your time on campus is the fastest way to learn about AI. You should definitely take relevant courses, but we also recommend joining a research group or relevant team to get more practice and familiarity with relevant topics. This includes participating in MSAIL-sponsored projects.
MSAIL has a reading group, but it&amp;rsquo;s hard to balance all the different subfields of AI in just ~15 sessions in a semester. We highly recommend joining reading groups for more depth regarding the topics you&amp;rsquo;re interested in.&lt;/p&gt;
&lt;h3 id=&#34;classes&#34;&gt;Classes&lt;/h3&gt;
&lt;p&gt;&lt;em&gt;Last updated on 9/23/21.&lt;/em&gt;
This is a listing of courses related to AI (from a technical perspective) here at the University of Michigan. We tried to be as comprehensive as possible, but there are far too many courses to sift through, so we may have missed some. More information is available on the 
&lt;a href=&#34;https://www.lsa.umich.edu/cg/default.aspx&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LSA course guide&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As a side note, there are many courses that we can argue are related to AI from a less technical perspective. For example, take classes in the cognitive sciences - the development of human-like AI is heavily motivated by studies in this field. We leave these classes out for brevity&amp;rsquo;s sake.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Undergraduate-level Classes&lt;/strong&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Class Code&lt;/th&gt;
&lt;th&gt;Class Name&lt;/th&gt;
&lt;th&gt;Last offered?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 442&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Computer Vision&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20445&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 445&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Machine Learning&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20492&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 492&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Intro to AI&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20467&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 467&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Autonomous Robotics (MDE)&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/LING%20441/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LING 441&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Introduction to Computational Linguistics&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/ROB%20102/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROB 102&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Introduction to Robotics Algorithms and Programming&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20367&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 367&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Introduction to Autonomous Robotics&lt;/td&gt;
&lt;td&gt;F20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/ROB%20464&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROB 464&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Hands-on Robotics&lt;/td&gt;
&lt;td&gt;W20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Graduate-level Classes&lt;/strong&gt;:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Class Code&lt;/th&gt;
&lt;th&gt;Class Name&lt;/th&gt;
&lt;th&gt;Last offered?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20505/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 505&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Computational Data Science and Machine Learning&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20545&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 545&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Machine Learning&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20592/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 592&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Foundations of AI&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20542/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 542&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Advanced Topics in Computer Vision&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20551/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 551&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Matrix Methods for Signal Processing, Data Analysis, and Machine Learning&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20595/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 595/LING 541&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Natural Language Processing&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/ROB%20535/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ROB 535&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Self Driving Cars: Perception and Control&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/AEROSP%20567/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;AEROSP 567&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;AEROSP 567: Inference Estimation and Learning&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/ROB%20530/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 568/ROB 530&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Mobile Robotics&lt;/td&gt;
&lt;td&gt;W21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20692/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 692&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Advanced Artificial Intelligence&lt;/td&gt;
&lt;td&gt;W21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;
&lt;a href=&#34;https://atlas.ai.umich.edu/course/EECS%20504/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EECS 504&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Foundations of Computer Vision&lt;/td&gt;
&lt;td&gt;F20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;strong&gt;Special Topics Classes&lt;/strong&gt;:&lt;br&gt;
Each of these classes is listed under EECS 498, 598 or both - you will need to select the relevant section when registering.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Class Code&lt;/th&gt;
&lt;th&gt;Class Name&lt;/th&gt;
&lt;th&gt;Last offered?&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;EECS 498&lt;/td&gt;
&lt;td&gt;Principles of Machine Learning&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 598&lt;/td&gt;
&lt;td&gt;Randomized Numerical Linear Algebra for Machine Learning&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 498&lt;/td&gt;
&lt;td&gt;Intro to Algorithmic Robotics&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 498&lt;/td&gt;
&lt;td&gt;Conversational AI&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 598&lt;/td&gt;
&lt;td&gt;Human-Computer Interaction&lt;/td&gt;
&lt;td&gt;F21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 498&lt;/td&gt;
&lt;td&gt;Intro to Natural Language Processing&lt;/td&gt;
&lt;td&gt;W21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 498/598&lt;/td&gt;
&lt;td&gt;Ethics for AI and Robotics&lt;/td&gt;
&lt;td&gt;W21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 498/598&lt;/td&gt;
&lt;td&gt;Applied Machine Learning for Affective Computing&lt;/td&gt;
&lt;td&gt;W21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 598&lt;/td&gt;
&lt;td&gt;Statistical Learning Theory&lt;/td&gt;
&lt;td&gt;W21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 598&lt;/td&gt;
&lt;td&gt;Unsupervised Visual Learning&lt;/td&gt;
&lt;td&gt;W21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 598&lt;/td&gt;
&lt;td&gt;Adversarial Machine Learning&lt;/td&gt;
&lt;td&gt;W21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 598&lt;/td&gt;
&lt;td&gt;Systems for AI&lt;/td&gt;
&lt;td&gt;W21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 556/598&lt;/td&gt;
&lt;td&gt;Image Processing&lt;/td&gt;
&lt;td&gt;W21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 498/598&lt;/td&gt;
&lt;td&gt;Deep Learning for Computer Vision&lt;/td&gt;
&lt;td&gt;F20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 598&lt;/td&gt;
&lt;td&gt;Reinforcement Learning Theory&lt;/td&gt;
&lt;td&gt;F20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 598&lt;/td&gt;
&lt;td&gt;Deep Learning for NLP&lt;/td&gt;
&lt;td&gt;F20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 598&lt;/td&gt;
&lt;td&gt;Situated Language Processing for Embodied AI Agents&lt;/td&gt;
&lt;td&gt;W20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EECS 598&lt;/td&gt;
&lt;td&gt;The Ecological Approach to Vision&lt;/td&gt;
&lt;td&gt;W20&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;research-labs&#34;&gt;Research Labs&lt;/h3&gt;
&lt;p&gt;A list of professors is available on the 
&lt;a href=&#34;https://ai.engin.umich.edu/people/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Michigan AI Lab faculty page&lt;/a&gt;. Each professor&amp;rsquo;s lab will be linked to on their homepage.&lt;/p&gt;
&lt;h3 id=&#34;reading-groups&#34;&gt;Reading Groups&lt;/h3&gt;
&lt;p&gt;Right now, we are aware of three relevant reading groups that allow for public participation. Many research labs have internal reading groups as well.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Group Name&lt;/th&gt;
&lt;th&gt;Page&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Computer Vision Reading Group&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://sites.google.com/umich.edu/cv-reading-group/home&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sites.google.com/umich.edu/cv-reading-group/home&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Natural Language Processing Reading Group&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://lit.eecs.umich.edu/reading_group.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://lit.eecs.umich.edu/reading_group.html&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reinforcement Learning Reading Group&lt;/td&gt;
&lt;td&gt;
&lt;a href=&#34;https://sites.google.com/umich.edu/rl-reading-group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://sites.google.com/umich.edu/rl-reading-group&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;conferences&#34;&gt;Conferences&lt;/h2&gt;
&lt;p&gt;AI researchers nowadays usually write papers with the goal of submitting them to a conference. Conferences are a great way to meet other people in the field, get feedback on your work, and discuss ideas about further research. These are usually good places to start if you want to look for recent literature on a given topic.&lt;/p&gt;
&lt;p&gt;Listed below are links to the pages of some highly-ranked machine learning conferences. Note that most of these links are for specific years (mostly 2021 because that&amp;rsquo;s when this list was first made); use Google to look up pages for other years.&lt;/p&gt;
&lt;p&gt;This is not intended to be a complete list of AI/ML conferences. Usually, your first paper will be at a lower-tier conference as you get used to publishing.&lt;/p&gt;
&lt;h3 id=&#34;general-ml-conferences&#34;&gt;General ML Conferences&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://icml.cc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICML&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://iclr.cc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICLR&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://neurips.cc/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeurIPS&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;computer-vision-conferences&#34;&gt;Computer Vision Conferences&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://cvpr2021.thecvf.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CVPR&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://iccv2021.thecvf.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ICCV&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;natural-language-processing-conferences&#34;&gt;Natural Language Processing Conferences&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://2021.emnlp.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;EMNLP&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://2021.aclweb.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACL&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://naacl.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NAACL&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;previous-talks&#34;&gt;Previous Talks&lt;/h2&gt;
&lt;p&gt;MSAIL has hosted a number of 
&lt;a href=&#34;https://MSAIL.github.io/talk/&#34;&gt;talks&lt;/a&gt; over the years given by Michigan professors and students. You might find
them insightful in providing a survey of current and past research.&lt;/p&gt;
&lt;h2 id=&#34;mediumblog-articles&#34;&gt;Medium/Blog Articles&lt;/h2&gt;
&lt;!-- this is just meant to be a start to this section. I haven&#39;t read every Medium article in existence. --&gt;
&lt;p&gt;Medium articles are nice because they tend to be much shorter and easier to read than bona fide research papers. However, not all Medium
articles are high quality. As such, we have provided a selection of high-quality Medium articles and blog posts in the vein of a Medium article.
MSAIL also publishes its own 
&lt;a href=&#34;https://MSAIL.github.io/post/&#34;&gt;blog&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Under construction &amp;ndash; we will be adding more articles in the near future!&lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&#34;introductory&#34;&gt;Introductory&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://medium.com/machine-learning-in-practice/a-gentle-introduction-to-machine-learning-concepts-cfe710910eb&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;A Gentle Introduction to Machine Learning Concepts (Robbie Allen)&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;computer-vision-1&#34;&gt;Computer Vision&lt;/h3&gt;
&lt;p&gt;Overview of GANS (Zak Jost): 
&lt;a href=&#34;https://blog.zakjost.com/post/gans_overview_1/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Part 1 (GAN)&amp;rdquo;&lt;/a&gt;, 
&lt;a href=&#34;https://blog.zakjost.com/post/gans_overview_2/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Part 2 (DCGAN)&amp;rdquo;&lt;/a&gt;, 
&lt;a href=&#34;https://blog.zakjost.com/post/gans_overview_3/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Part 3 (InfoGAN)&amp;rdquo;&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Understanding Variational Autoencoders (VAES) (Joseph Rocca)&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;nlp&#34;&gt;NLP&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;http://jalammar.github.io/illustrated-transformer/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;The Illustrated Transformer (Jay Alammar)&amp;rdquo;&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;http://jalammar.github.io/how-gpt3-works-visualizations-animations/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How GPT3 Works - Visualizations and Animations (Jay Alammar)&amp;rdquo;&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://kazemnejad.com/blog/transformer_architecture_positional_encoding/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Transformer Architecture: The Positional Encoding&amp;rdquo;&lt;/a&gt;&lt;/p&gt;
&lt;!-- ### RL --&gt;
&lt;h3 id=&#34;uncategorized&#34;&gt;Uncategorized&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://theaisummer.com/graph-convolutional-networks/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;How Graph Neural Networks (GNN) work: introduction to graph convolutions from scratch (Nikolas Adaloglou)&amp;rdquo;&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Understanding Latent Space in Machine Learning (Ekin Tiu)&amp;rdquo;&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://github.com/papers-we-love/papers-we-love/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&amp;ldquo;Papers we love&amp;rdquo; repository&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;meta-skills-and-mindset&#34;&gt;Meta-skills and Mindset&lt;/h2&gt;
&lt;h3 id=&#34;conducting-research&#34;&gt;Conducting Research&lt;/h3&gt;
&lt;p&gt;Richard Hamming: 
&lt;a href=&#34;http://www.cs.virginia.edu/~robins/YouAndYourResearch.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“You and your research”&lt;/a&gt;&lt;br&gt;
Michael Nielsen: 
&lt;a href=&#34;https://michaelnielsen.org/blog/principles-of-effective-research/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“Principles of Effective Research”&lt;/a&gt;&lt;br&gt;
John Schulman: 
&lt;a href=&#34;http://joschu.net/blog/opinionated-guide-ml-research.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“An Opinionated Guide to ML Research”&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;reading-research-papers&#34;&gt;Reading research papers&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf&lt;/a&gt;&lt;br&gt;

&lt;a href=&#34;https://www.eecs.harvard.edu/~michaelm/postscripts/ReadPaper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://www.eecs.harvard.edu/~michaelm/postscripts/ReadPaper.pdf&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;giving-talks&#34;&gt;Giving Talks&lt;/h3&gt;
&lt;p&gt;
&lt;a href=&#34;https://web.eecs.umich.edu/~cscott/talk_advice.htm&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://web.eecs.umich.edu/~cscott/talk_advice.htm&lt;/a&gt;&lt;/p&gt;
&lt;h3 id=&#34;grad-school&#34;&gt;Grad School&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.cs.cmu.edu/~harchol/gradschooltalk.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Mor Harschol-Balter (CMU): Applying to CS PhD programs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://docs.google.com/document/u/1/d/11D3kHElzS2HQxTwPqcaTnU5HCJ8WGE5brTXI4KLf4dM/mobilebasic&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Eric Gilbert (Umich CSE, SI): Advice to his students&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://ruder.io/10-tips-for-research-and-a-phd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Sebastian Ruder (Deepmind): 10 Tips for Research and a PhD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.cs.unc.edu/~azuma/hitch4.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ronald Azuma (UNC): “So long, and thanks for the Ph.D.!”&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://karpathy.github.io/2016/09/07/phd/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrej Karpathy (Tesla, OpenAI): A Survival Guide to a PhD&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://hongtaoh.com/en/2021/09/22/philip-guo-phd-advice/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Philip Guo (UCSD): Advice for early-stage Ph.D. students&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://www.andreykurenkov.com/writing/life/lessons-learned-from-failures/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Andrey Kurenkov (Stanford): Lessons Learned the Hard Way in Grad School&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>VideoBERT Revised</title>
      <link>https://MSAIL.github.io/slides/videobert/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/slides/videobert/</guid>
      <description>&lt;h1 id=&#34;videobert&#34;&gt;VideoBERT&lt;/h1&gt;
&lt;h2 id=&#34;c-sun-et-al&#34;&gt;C. Sun et al.&lt;/h2&gt;
&lt;h3 id=&#34;google-research&#34;&gt;Google Research&lt;/h3&gt;
&lt;p&gt;Presented by: Nikhil Devraj&lt;/p&gt;
&lt;p style=&#34;color:grey&#34;&gt; MSAIL &lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Representations of video data generally capture only low-level features and not semantic data&lt;/li&gt;
&lt;/ul&gt;
&lt;center&gt;
&lt;img src=&#34;https://MSAIL.github.io/slides/low_level.png&#34; alt=&#34;Low-level features&#34; height=&#34;200&#34;/&gt;
&lt;/center&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/1810.04805&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BERT&lt;/a&gt; performs really well on language modeling tasks&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;contributions&#34;&gt;Contributions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Combined 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Speech_recognition&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ASR&lt;/a&gt;, 
&lt;a href=&#34;https://en.wikipedia.org/wiki/Vector_quantization&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vector Quantization&lt;/a&gt;, and BERT to learn high-level features over long time spans in video tasks&lt;/li&gt;
&lt;li&gt;A first step in the direction of learning high-level joint representations&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;img src=&#34;https://MSAIL.github.io/slides/videobert_flow.png&#34; alt=&#34;VideoBERT Flow&#34;&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;background&#34;&gt;Background&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;bert&#34;&gt;BERT&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Pretrained language model used to generate a probability distribution of tokens&lt;/li&gt;
&lt;li&gt;Obtained by training model on &amp;ldquo;masking&amp;rdquo; task&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;center&gt;
&lt;img src=&#34;https://MSAIL.github.io/slides/bert.png&#34; alt=&#34;BERT&#34; height=&#34;600&#34;/&gt;
&lt;/center&gt;
&lt;hr&gt;
&lt;h2 id=&#34;supervised-learning&#34;&gt;Supervised Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Expensive to get labeled data&lt;/li&gt;
&lt;li&gt;Short term events in video data&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;unsupervised-learning&#34;&gt;Unsupervised Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Learns from unlabeled data&lt;/li&gt;
&lt;li&gt;Normal approaches used latent variables (i.e. GAN, VAE)
&lt;ul&gt;
&lt;li&gt;differ from BERT&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;self-supervised-learning&#34;&gt;Self-supervised Learning&lt;/h2&gt;
&lt;p&gt;&lt;img src=&#34;https://MSAIL.github.io/slides/self-sup-lecun.png&#34; alt=&#34;Self-supervised example&#34;&gt;&lt;/p&gt;
&lt;p&gt;
&lt;a href=&#34;https://lilianweng.github.io/lil-log/2019/11/10/self-supervised-learning.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;More on self supervised learning&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;cross-modal-learning&#34;&gt;Cross-Modal Learning&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Synchronized audio and visual signals allow them to supervise each other&lt;/li&gt;
&lt;li&gt;Use ASR as a source of crossmodal supervision&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;instructional-video-datasets&#34;&gt;Instructional Video Datasets&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Papers used LMs to analyze these videos with manually provided data&lt;/li&gt;
&lt;li&gt;Datasets were too small&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h1 id=&#34;method&#34;&gt;Method&lt;/h1&gt;
&lt;hr&gt;
&lt;h2 id=&#34;omitted-the-rest&#34;&gt;Omitted the rest&lt;/h2&gt;
&lt;p&gt;You get the principles I&amp;rsquo;m getting at though right?&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Who are we?</title>
      <link>https://MSAIL.github.io/aboutus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://MSAIL.github.io/aboutus/</guid>
      <description>&lt;p&gt;MSAIL is a large organization of over 400 members, and as such requires our Admin Team to help keep operations running smoothly. Our Admin Team is advised by faculty mentors involved in AI research at the University of Michigan.&lt;/p&gt;
&lt;link rel=&#34;stylesheet&#34; href=&#34;style.css&#34;&gt;
&lt;h2&gt;Faculty Mentors&lt;/h2&gt;
These astounding professors make MSAIL possible with their advice and support.
&lt;div id=&#34;faculty&#34;&gt;
&lt;!-- Sindhu Kutty --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
    &lt;img src=&#34;portraits/sindhu_kutty.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt; Sindhu Kutty &lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Lecturer III, EECS &lt;/div&gt;
    &lt;div class=&#34;leader-email&#34;&gt; skutty@ &lt;/div&gt;
&lt;/div&gt;
&lt;!-- Laura Balzano --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
     &lt;a href=&#34;https://web.eecs.umich.edu/~girasole/&#34;&gt;
    &lt;img src=&#34;portraits/laura_balzano.jpg&#34;&gt;
    &lt;/a&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt; &lt;a href=&#34;https://web.eecs.umich.edu/~girasole/&#34;&gt; Laura Balzano &lt;/a&gt; &lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Associate Professor, EECS &lt;/div&gt;
    &lt;div class=&#34;leader-email&#34;&gt; girasole@&lt;/div&gt;
&lt;/div&gt;
&lt;!-- Danai Koutra --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
    &lt;a href=&#34;https://web.eecs.umich.edu/~dkoutra/&#34;&gt;
    &lt;img src=&#34;portraits/danai_koutra.jpg&#34;&gt;
    &lt;/a&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;  &lt;a href=&#34;https://web.eecs.umich.edu/~dkoutra/&#34;&gt; Danai Koutra &lt;/a&gt; &lt;/div&gt;
    &lt;div class=&#34;leader-info&#34;&gt; Assistant Professor, EECS &lt;/div&gt;
    &lt;div class=&#34;leader-email&#34;&gt; dkoutra@&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;h2&gt; Admin Team &lt;/h2&gt;
Our administrative team is responsible for planning MSAIL&#39;s activities
and holding the organization together. Our &lt;a href=&#34;https://MSAIL.github.io/constitution/&#34;&gt;constitution&lt;/a&gt; codifies our
roles. The following details our current admin team&#39;s roles and emails (at umich.edu).
&lt;!-- &lt;div id=&#34;leader&#34;&gt; &lt;/div&gt; --&gt;
&lt;div id=&#34;leadership&#34;&gt;
&lt;!-- Alex Ji --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/alex_ji.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Alex Ji&lt;/div&gt;
    &lt;!-- &lt;div class=&#34;leader-info&#34;&gt; Education, Cohort &lt;/div&gt; --&gt;
	&lt;div class=&#34;leader-info&#34;&gt; ajys@ &lt;/div&gt;
&lt;/div&gt;
&lt;!-- Asad Khan --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/asad_khan.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Asad Khan&lt;/div&gt;
    &lt;!-- &lt;div class=&#34;leader-info&#34;&gt; Education, Cohort &lt;/div&gt; --&gt;
	&lt;div class=&#34;leader-info&#34;&gt; asadk@ &lt;/div&gt;
&lt;/div&gt;
&lt;!-- Michael Moffatt  --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/michael_moffatt.png&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Michael Moffatt&lt;/div&gt;
    &lt;!-- &lt;div class=&#34;leader-info&#34;&gt; Funding &lt;/div&gt; --&gt;
	&lt;div class=&#34;leader-info&#34;&gt; mmoffatt@ &lt;/div&gt;
&lt;/div&gt;
&lt;!-- Terry Shi --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/terry_shi.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Terry Shi&lt;/div&gt;
	&lt;div class=&#34;leader-info&#34;&gt; weiceica@ &lt;/div&gt;	
&lt;/div&gt;
&lt;!-- Andrew Carlson --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/andrew_carlson.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Andrew Carlson&lt;/div&gt;
	&lt;div class=&#34;leader-info&#34;&gt; andson@ &lt;/div&gt;	
&lt;/div&gt;
&lt;!-- Hemil Shah --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/hemil_shah.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Hemil Shah&lt;/div&gt;
	&lt;div class=&#34;leader-info&#34;&gt; heshah@ &lt;/div&gt;	
&lt;/div&gt;
&lt;!-- Chinmay Purushottam --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/chinmay_purushottam.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Chinmay Purushottam&lt;/div&gt;
	&lt;div class=&#34;leader-info&#34;&gt; chinzo@&lt;/div&gt;	
&lt;/div&gt;
&lt;!-- Aman Nagesh --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/aman_nagesh.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Aman Nagesh&lt;/div&gt;
	&lt;div class=&#34;leader-info&#34;&gt; amannag@&lt;/div&gt;	
&lt;/div&gt;
&lt;!-- Jaiden Schraut --&gt;
&lt;div class=&#34;leader-box&#34;&gt;
    &lt;div class=&#34;leader-portrait&#34;&gt;
	&lt;img src=&#34;portraits/jaiden_schraut.jpg&#34;&gt;
    &lt;/div&gt;
    &lt;div class=&#34;leader-name&#34;&gt;Jaiden Schraut&lt;/div&gt;
	&lt;div class=&#34;leader-info&#34;&gt; jaidenxs@&lt;/div&gt;	
&lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;
&lt;a href=&#34;https://MSAIL.github.io/alumni/&#34;&gt;Here are some of our graduated members.&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
